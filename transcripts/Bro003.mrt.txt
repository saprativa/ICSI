Channel one. Test. Hello. Channel three. Test. Uh-oh. So you think we're going now, yes? O_K, good. Alright Going again Uh - So we're gonna go around as before, and uh do - do our digits. Uh transcript one three one one dash one three three zero. three two three four seven six five five three one six two four one six seven seven eight nine zero nine four zero zero three zero one five eight one seven three five three two six eight zero three six two four three zero seven four five zero six nine four seven four eight five seven nine six one five O_ seven eight O_ two zero nine six zero four zero zero one two I am reading transcript one three nine one one four one zero six seven eight nine O_ six nine eight zero one three one nine one six two three seven three four five six eight four seven nine two zero seven five O_ three six nine six zero nine three one zero zero three one two O_ three zero nine seven five two six seven nine eight three six seven zero six O_K I've got transcript one two seven one one two nine zero. one seven one O_ two eight one two zero seven one three four five zero nine six zero eight zero seven three eight six eight one three nine three four zero three nine four O_ four two zero one two zero three eight two three two four two eight one six five three five eight three O_ eight six five nine one zero seven six seven six six two eight seven nine O_ O_ five seven eight O_ four one four one Transcript number one two nine one dash one three one zero. two three nine zero two three eight one five two four seven four six seven five six seven zero nine four O_ one O_ O_ zero seven two six zero one six zero four six four five two three four zero seven two six four four zero seven four one eight eight eight eight five six three nine eight nine O_ zero zero three nine one Transcript number one one seven one dash one one nine zero. eight three four three five nine four three nine O_ three zero nine six nine one one two O_ O_ three five five six O_ eight six three zero four zero one one seven eight eight two eight nine eight five five O_ zero zero two one four three six seven O_ two four seven nine five nine zero five seven three five six seven six nine four six five seven O_K, this is Barry Chen and I am reading transcript one three five one dash one three seven zero, um four five O_ seven eight nine two eight seven one four eight nine eight five nine three six nine seven one five nine O_ four seven three O_- one O_ O_ zero one two zero six two four three four six seven five four four six seven eight nine nine eight nine O_ O_ one one two four seven three four nine two five three five zero zero eight four eight Transcript one three seven one one three nine O_. five six seven O_ O_ five nine two six three three O_ five five zero four nine one six two three O_ O_ two one nine two four six six seven two O_ eight five O_ three eight six nine O_ eight eight seven four zero one zero eight three two four five four five five six nine two three Uh - Yeah, you don't actually n- need to say the name. That'll probably be bleeped out. O_K. So. That's if these are anonymized, but Oh. O_K. Yeah - uh - I mean - not that there's anything defamatory about uh - eight five seven or or anything, but Uh, anyway. O_K. Uh - so here's what I have for - I - I was just jotting down things I think th- w- that we should do today. Uh - This is what I have for an agenda so far Um, We should talk a little bit about the plans for the uh - the field trip next week. Uh - a number of us are doing a field trip to uh Uh - O_G_I And uh - mostly uh- First though about the logistics for it. Then maybe later on in the meeting we should talk about what we actually you know, might accomplish. Uh - O_K. Uh, in and kind of go around - see what people have been doing - talk about that, a r- progress report. Um, Essentially. Um - And then uh - Another topic I had was that uh - uh - Uh - Dave here had uh said uh "Give me something to do." And I - I have - I have uh - failed so far in doing that. And so maybe we can discuss that a little bit. If we find some holes in some things that - that - someone could use some help with, he's - he's volunteering to help. I've got to move a bunch of furniture. O_K, always count on a serious comment from that corner. So, um, uh, and uh, then uh, talk a little bit about - about disks and resource - resource issues that - that's starting to get worked out. And then, anything else anybody has that isn't in that list? Uh - I was just wondering, does this mean the battery's dying and I should change it? Uh I think that means the battery's O_ K. - d - do you Let me see. Oh O_K, so th- Yeah, that's good. Yeah. You're alright? Cuz it's full. Yeah. Yeah. Alright. Yeah. It looks full of electrons. O_K. Plenty of electrons left there. O_K, so, um, uh. O_K, so, uh, I wanted to start this with this mundane thing. Um - Uh - I - I - it was - it was kind of my bright idea to have us take a plane that leaves at seven twenty in the morning. Um. Oh, yeah, that's right. Uh this is uh - The reason I did it uh was because otherwise for those of us who have to come back the same day it is really not much of a - of a visit. Uh - So um the issue is how - how - how would we ever accomplish that? Uh - what - what - what part of town do you live in? Um, I live in, um, the corner of campus. The, um, southeast corner. O_K. O_K, so would it be easier - those of you who are not, you know, used to this area, it can be very tricky to get to the airport at - at uh, you know, six thirty. Um. So. Would it be easier for you if you came here and I drove you? Yeah? Yeah, perhaps, yeah. Yeah, yeah, O_K. Yeah. Mm-hmm. Yeah. Sure. O_K, so if - if everybody can get here at six. At six. Yeah, I'm afraid we need to do that to get there on time. Yeah, so. Six, O_K. Oh boy. Anyway, so. Will that be enough time? Yeah. Yeah, so I'll just pull up in front at six and just be out front. And, uh, and yeah, that'll be plenty of time. It'll take - it - it - it won't be bad traffic that time of day and - and uh I guess once you get past the bridge that that would be the worst. Going to Oakland. Yeah , Oakland. Oakland. Yeah. Bridge Once you get past the turnoff to the Bay Bridge. oh, the turnoff to the bridge Yeah. Won't even do that. I mean, just go down Martin Luther King. Yeah. Yeah. O_K. Mm-hmm. And then Martin Luther King to nine-eighty to eight-eighty, and it's - it'd take us, Yeah. tops uh thirty minutes to get there. Oh, I - So that leaves us fifty minutes before the plane - it'll just - yeah. So Great, O_K so that'll It's - I mean, it's still not going to be really easy but - well Particularly for - for uh - for Barry and me, we're not - we're not staying overnight so we don't need to bring anything particularly except for uh - a pad of paper and - So, and, uh you, two O_K. have to bring a little bit but uh - you know, don't - don't bring a footlocker and we'll be O_K So. s- W- you're staying overnight. I figured you wouldn't need a great big suitcase, yeah. That's sort of So just - Oh yeah. Yeah. one night. So. Anyway. So, s- six A_M, in front. O_K. Six A_M in front. O_K. Uh, I'll be here. Uh - I'll - I'll - I'll - I'll give you my phone number, If I'm not here for a few m- after a few minutes then Wake you up. Nah, I'll be fine. I just, uh - it - for me it just means getting up a half an hour earlier than I usually do. Not - not - not a lot, so O_K. Wednesday. O_K, that was the real real important stuff. Um, I - I - I figured maybe wait on the potential goals for the meeting uh - until we talk about wh- what's been going on. So, uh, what's been going on? Why don't we start - start over here. Um. Well, preparation of the French test data actually. O_K. So, it means that um, well, it is, uh, a digit French database of microphone speech, downsampled to eight kilohertz and I've added noise to one part, with the - actually the Aurora-two noises. And, @@ so this is a training part. And then the remaining part, I use for testing and - with other kind of noises. So we can - So this is almost ready. I'm preparing the - the H_T_K baseline for this task. And, yeah. O_K Uh, So the H_T_K base lines - so this is using mel cepstra and so on, or - ? Yeah. Yeah. O_K. And again, I guess the p- the plan is, uh, to uh - then given this - What's the plan again? The plan with these data? With - So - So - Does i- Just remind me of what - what you were going to do with the - what - what - what - what's - y- You just described what you've been doing. Yeah. So if you could remind me of what you're going to be doing. Oh, this is - yeah, yeah. Uh, yeah. @@ Tell him about the cube. Well. The cube? I should tell him about the cube? Yeah. Yeah. Oh! Cube. Yeah. Uh we - actually we want to, mmm, Fill in the cube. Uh, uh, analyze three dimensions, the feature dimension, the training data dimension, and the test data dimension. Um. Well, what we want to do is first we have number for each uh task. So we have the um, T_I-digit task, the Italian task, the French task and the Finnish task. Yeah? So we have numbers with uh - systems - I mean - I mean neural networks trained on the task data. And then to have systems with neural networks trained on, uh, data from the same language, if possible, with, well, using a more generic database, which is phonetically - phonetically balanced, and. Um. Yeah. So. So- so we had talked - I guess we had talked at one point about maybe, the language I_D corpus? Is that a possibility for that? Ye- uh - Yeah, but, uh these corpus, w- w- there is a CallHome and a CallFriend also, The CallFriend is for language ind- identification. Well, anyway, these corpus are all telephone speech. So, um. This could be a - a problem for - Why? Because uh, uh, the - the SpeechDat databases are not telephone speech. They are downsampled to eight kilohertz but - but they are not uh with telephone bandwidth. Yeah. That's really funny isn't it? I mean cuz th- this whole thing is for developing new standards for the telephone. Telephone. Yeah. Yeah, but the - the idea is to compute the feature before the - before sending them to the - Well, you don't - do not send speech, you send features, computed on th- the - the device, or - Well. Mm-hmm. Yeah, I know, but the reason - Oh I see, so your point is that it's - it's - So you - it's uh - the features are computed locally, and so they aren't necessarily telephone bandwidth, uh Yeah. Yeah. or telephone Did you happen to find out anything about the O_G_I multilingual database? distortions. Yeah, that's wh- that's wh- that's what I meant. Yeah, it's - I said - @@ , there's - there's - there's an O_G_I language I_D, not the - not the, uh - the CallFriend is a - is a, uh, L_D_C w- thing, right? Yea- Yeah, there are also two other databases. One they call the multi-language database, and another one is a twenty-two language, something like that. But it's also telephone speech. Oh, they are? O_K. Uh. Well, nnn. But I'm not sure - So - I mean, we'r- e- e- The bandwidth shouldn't be such an issue right? Because e- e- this is downsampled and - and filtered, right? So it's just the fact that it's not telephone. And there are so many other differences between these different databases. I mean some of this stuff's recorded in the car, and some of it's - I mean there's - there's many different acoustic differences. So I'm not sure Yeah. if - . I mean, unless we're going to include a bunch of car recordings in the - in the training database, I'm not sure if it's - completely rules it out if our - if we - if our major goal is to have phonetic context Mmm. and you figure that there's gonna be a mismatch in acoustic conditions does it make it much worse f- to sort of add another mismatch, if you will. Uh, i- i- I - I guess the question is how important is it to - for us to get multiple languages uh, in there. Yeah, but - Mm-hmm. Um. Yeah. Well, actually, for the moment if we w- do not want to use these phone databases, we - we already have uh - English, Spanish and French uh, with microphone speech. Mm-hmm. Yeah. So. So that's what you're thinking of using is sort of the multi- the equivalent of the multiple? Well. Yeah, for the multilingual part we were thinking of using these three databases. And for the difference in phonetic context that you - ? Well, this - Uh, actually, these three databases are um generic databases. Provide that. Yeah. So w- f- for - for uh Italian, which is close to Spanish, French and, i- i- uh, T_I-digits we have both uh, digits training data and also more general training data. So. Mmm. Well, we also have this Broadcast News that we were talking about taking off the disk, which is - is microphone data for - for English. Yeah. Yeah, perhaps - yeah, there is also TIMIT. We could use TIMIT. Yeah. Right. Um. Yeah, so there's plenty of stuff around. O_K, so anyway, th- the basic plan is to, uh, test this cube. @@ Yeah. Yes. To fill i- fill it in, yeah. To fill in the cube. Uh. O_K. Yeah, and perhaps, um - We were thinking that perhaps the cross-language issue is not, uh, so big of a issue. Well, w- w- we - perhaps we should not focus too much on that cross-language stuff. I mean, uh, training - training a net on a language and testing a- for another language. Uh-huh. Mmm. Perhaps the most important is to have neural networks trained on the target languages. But, uh, with a general database - general databases. But that's - u- So that th- Well, the - the guy who has to develop an application with one language can use the net trained o- on that language, or a generic net, but not trained on a - Uh, depen- it depen- it depends how you mean "using the net". So, if you're talking about for producing these discriminative features that we're talking about you can't do that. Mmm. Because - because the - what they're asking for is - is a feature set. Right? And so, uh, we're the ones who have been weird by - by doing this training. Yeah. But if we say, "No, you have to have a different feature set for each language," I think this is ver- gonna be very bad. Oh. You think so. Mmm. So - Oh yeah. Yeah. I mean, Oh. That's - in principle, I mean conceptually, it's sort of like they want a re- @@ Mmm. well, they want a replacement for mel cepstra. So, we say "O_K, this is the year two thousand, we've got something much better than mel cepstra. It's, you know, gobbledy-gook." O_K? And so we give them these gobbledy-gook features but these gobbledy-gook features are supposed to be good for any language. Hmm. Cuz you don't know who's gonna call, and you know, I mean so it's - it's - it's, uh, uh - how do you know what language it is? Somebody picks up the phone. So thi- this is their image. Well, I chh - Someone picks up the phone, right? And - and he - he picks up the ph- Yeah, but the - the application is - there is a target language for the application. So, if a - Yeah. y- y- y- Well. Well. But, no but, y- you - you pick up the phone, Yeah? you talk on the phone, and it sends features out. O_K, so the phone doesn't know what a - what - what your language is. Yeah, if - Yeah. If it's th- in the phone, but - well, it - that - that could be th- at the server's side, and, well. But that's the image that they have. It could be, but that's the image they have, right? Mmm, yeah. So that's - that's - I mean, one could argue all over the place about how things really will be in ten years. But the particular image that the cellular industry has right now is that it's distributed speech recognition, where the, uh, uh, probabilistic part, and - and s- semantics and so forth are all on the servers, and you compute features of the - uh, on the phone. So that's - that's what we're involved in. We might - might or might not agree that that's the way it will be in ten years, but that's - that's - that's what they're asking for. So - so I think that - th- th- it is an important issue whether it works cross-language. Now, it's the O_G_I, uh, folks' perspective right now that probably that's not the biggest deal. And that the biggest deal is the, um envir- acoustic-environment mismatch. And they may very well be right, but I - I was hoping we could just do a test and determine if that was true. If that's true, we don't need to worry so much. Maybe - maybe we have a couple languages in the training set and that gives us enough breadth uh, uh, that - that - that the rest doesn't matter. Um, the other thing is, uh, this notion of training to uh - which I - I guess they're starting to look at up there, training to something more like articulatory features. Uh, and if you have something that's just good for distinguishing different articulatory features that should just be good across, you know, a wide range of languages. Mmm. Yeah. Yeah. Uh, but - Yeah, so I don't th- I know - unfortunately I don't - I see what you're comi- where you're coming from, I think, but I don't think we can ignore it. So we - we really have to do test with a real cross-language. I mean, tr- for instance training on English and testing on Italian, or - Or we can train - or else, uh, can we train a net on, uh, a range of languages and - Test on an unseen. which can include the test - the test @@ the target language, or - Yeah, so, um, there's - there's, uh - This is complex. So, ultimately, uh, as I was saying, I think it doesn't fit within their image that you switch nets based on language. Yeah. Now, can you include, uh, the - the target language? Um, from a purist's standpoint it'd be nice not to because then you can say when - because surely someone is going to say at some point, "O_K, so you put in the German and the Finnish. Uh, now, what do you do, uh, when somebody has Portuguese?" you know? Mmm. Um, and - Uh, however, you aren't - it isn't actually a constraint in this evaluation. So I would say if it looks like there's a big difference to put it in, then we'd make note of it, and then we probably put in the other, because we have so many other problems in trying to get things to work well here that - Mmm? that, you know, it's not so bad as long as we - we note it and say, "Look, we did do this". And so, ideally, what you'd wanna do is you'd wanna Uh. run it with and without the target language and the training set for a wide range of languages. Yeah. Yeah, perhaps. Yeah. Yeah. And that way you can say, "Well," Yeah. you know, "we're gonna build it for what we think are the most common ones", but if that - somebody uses it with a different language, you know, "here's what's you're l- here's what's likely to happen." Yeah, cuz the truth is, is that it's - it's not like there are - I mean, al- although there are thousands of languages, uh, from uh, uh, the point of view of cellular companies, there aren't. There's - you know, there's fifty or something, you know? So, Right. uh, an- and they aren't - you know, with the exception of Finnish, which I guess it's pretty different from most - most things. uh, it's - it's, uh - most of them are like at least some of the others. And so, our guess that Spanish is like Italian, and - and so on. I guess Finnish is a - is - is a little bit like Hungarian, supposedly, right? Or is - I think - well, I kn- oh, well I know that I don't know anything about Finnish. H- uh, H- I mean, I'm not a linguist, but I guess Hungarian and Finnish and one of the - one of the languages from the former Soviet Union are in this sort of same family. But they're just these, you know, Hmm. uh - countries that are pretty far apart from one another, have - Hmm. Mmm. I guess, people rode in on horses and brought their - O_K. The - Yeah. Your turn. Oh, my turn. Oh, O_K. Um, Let's see, I - I spent the last week, uh, looking over Stephane's shoulder. And - and understanding some of the data. I re-installed, um, um, H_T_K, the free version, so, um, everybody's now using three point O_, which is the same version that, uh, O_G_I is using. Oh, good. Yeah. So, without - without any licensing big deals, or anything like that. And, um, so we've been talking about this - this, uh, cube thing, and it's beginning more and more looking like the, uh, the Borge cube thing. It's really gargantuan. Um, but I- So are - are you going to be assimilated? I'm - Am I - Resistance is futile. Exactly. Um, yeah, so I- I've been looking at, uh, uh, TIMIT stuff. Um, the - the stuff that we've been working on with TIMIT, trying to get a, um - a labels file so we can, uh, train up a - train up a net on TIMIT and test, um, the difference between this net trained on TIMIT and a net trained on digits alone. Mm-hmm. Um, and seeing if - if it hurts or helps. And again, when y- just to clarify, when you're talking about training up a net, you're talking about training up a net for a tandem Anyway. Yeah, yeah. Um. approach? And - and the inputs are P_L_P and delta and that sort of thing, or - ? Mm-hmm. Well, the inputs are one dimension of the cube, which, um, we've talked about it being, uh, P_L_P, um, M_F_C_Cs, um, J_- J_RASTA, J_RASTA-L_D_A - Hmm. Yeah, but your initial things you're making one choice there, right? Which is P_L_P, or something? Yeah. Yeah, right. Um, I - I haven't - I haven't decided on - on the initial thing. Probably - probably something like P_L_P. Yeah. Yeah. Hmm. Um, so - so you take P_L_P and you - you, uh, do it - uh, you - you, uh, use H_T_K with it with the transformed features using a neural net that's trained. And the training could either be from Digits itself or from TIMIT. And that's the - and, and th- and then the testing would be these other things which - which - which might be foreign language. Right. Right. I see. Right. I - I - I get in the picture about the cube. O_K. Yeah. Maybe - O_K. O_K. Um, I mean, those listening to this will not have a picture either, so, um, I guess I'm - I'm not any worse off. Uh-huh. But but at some point - somebody should just show me the cube. It sounds s- I - I get - I think I get the general idea of it, yeah. Yeah, yeah, b- May- So, when you said that you were getting the labels for TIMIT, Mm-hmm. um, are y- what do you mean by that? Oh, I'm just - I'm just, uh, transforming them from the, um, the standard TIMIT transcriptions into - into a nice long huge P_file to do training. Mmm. Were the digits, um, hand-labeled for phones? Um, the - the digits - Or were they - those labels automatically derived? Oh yeah, those were - those were automatically derived by - by Dan Mmm. using, um, embedded - embedded training and alignment. Ah, but which Dan? Uh, Ellis. Right? O_K. O_K. Yeah. So. I was just wondering because that test you're t- I - I think you're doing this test because you want to determine whether or not, Uh-huh. uh, having s- general speech performs as well as having specific speech. That's right. Well, especially when you go over the different languages again, because you'd - the different languages have different words for the different digits, so it's - Mm-hmm. And I was - yeah, so I was just wondering if the fact that TIMIT - you're using the hand-labeled stuff from TIMIT might be - confuse the results that you get. I - I think it would, but - but on the other hand it might be better. Right, but if it's better, it may be better because it was hand-labeled. Oh yeah, but still @@ probably use it. Yeah. I mean, you know, I - I - I guess I'm sounding cavalier, but I mean, I think O_K. the point is you have, uh, a bunch of labels and - and they're han- hand uh - hand-marked. Uh, I guess, actually, TIMIT was not entirely hand-marked. It was automatically first, and then hand - hand-corrected. Oh, O_K. But - but, um, uh, it - it, um, it might be a better source. So, i- it's - you're right. It would be another interesting scientific question to ask, "Is it because it's a broad source or because it was, you know, carefully?" uh. Mm-hmm. And that's something you could ask, but given limited time, I think the main thing is if it's a better thing for going across languages on this training tandem system, then it's probably - Yeah. Right. What about the differences in the phone sets? Uh, between languages? No, between TIMIT and the - the digits. Oh, um, right. Well, there's a mapping from the sixty-one phonemes in TIMIT to - Sixty-one. to fifty-six, the ICSI fifty-six. Oh, O_K. I see. And then the digits phonemes, um, there's about twenty- twenty-two or twenty-four of them? Is that right? Yep. Out of that fifty-six? Oh, O_K. Out of that fifty-six. Yeah. So, it's - it's definitely broader, yeah. But, actually, the issue of phoneti- phon- uh phone- phoneme mappings will arise when we will do severa- use several languages because Yeah. you - Well, some phonemes are not, uh, in every languages, and - So we plan to develop a subset of the phonemes, uh, that includes, uh, all the phonemes of our training languages, and Mm-hmm. Mm-hmm. use a network with kind of one hundred outputs or something like that. You mean a superset, sort of. Uh, yeah, superset, yeah. Yeah. Yeah. Yeah. I th- I looks the SAMPA- SAMPA phone. Mmm. Yeah. SAMPA phone? For English - uh American English, and the - the - the language who have more phone are the English. Mmm. Of the - these language. But n- for example, in Spain, the Spanish have several phone that d- doesn't appear in the E- English and we thought to complete. But for that, it needs - we must r- h- do a lot of work because we need to generate new tran- transcription for the database that we have. Mm-hmm. Mm-hmm. Other than the language, is there a reason not to use the TIMIT phone set? Cuz it's larger? As opposed to the ICSI phone set? Oh, you mean why map the sixty-one to the fifty-six? Yeah. I don't know. I have - Um, I forget if that happened starting with you, or was it - o- or if it was Eric, afterwards who did that. But I think, basically, there were several of the phones that were just hardly ever there. Yeah, and I think some of them, they were making distinctions between silence at the end and silence at the beginning, when really they're both silence. Oh. I th- I think it was things like that that got it mapped down to fifty-six. O_K. Yeah, especially in a system like ours, which is a discriminative system. You know, you're really asking this net to learn. @@ It's - it's kind of hard. So. Yeah. Yeah. There's not much difference, really. And the ones that are gone, I think are - I think there was - they also in TIMIT had like a glottal stop, which Mm-hmm. was basically a short period of silence, and so. Well, we have that now, too, right? I don't know. Yeah. i- It's actually pretty common that a lot of the recognition systems people use have things like - like, say thirty-nine, So. phone symbols, right? Uh, and then they get the variety by - by bringing in the context, the phonetic context. Uh. So we actually have an unusually large number in - in what we tend to use here. Um. So, a- a- actually - maybe - now you've got me sort of intrigued. What - there's - Can you describe what - what's on the cube? I mean - Yeah, w- I th- I think that's a good idea to - to talk about the whole cube and maybe we could sections in the cube for people to work on. Yeah, yeah. Yeah. Yeah. Yeah. Yeah. Um, O_K. O_K, so even - even though the meeting recorder doesn't - doesn't, uh - and since you're not running a video camera we won't get this, but if you use a board it'll help us anyway. Uh, do you wanna do it? Uh, point out one of the limitations of this medium, but you've got the wireless on, right? Yeah, so you can walk around. O_K. O_K. Yeah, I have the wireless. O_K. Can y- can you walk around too? No. O_K, well, um, s- Uh, he can't, actually, but - He's tethered. basically, the - the cube will have three dimensions. The first dimension is the - the features that we're going to use. And the second dimension, um, is the training corpus. And that's the training on the discriminant neural net. Um and the last dimension Yeah and again - Yeah. So the - the training for H_T_K is always - that's always set up for the individual test, right? That there's some training data and some test data. So that's different than this. Yeah. happens to be - Right, right. This is - this is for - for A_N_N only. And, yeah, the training for the H_T_K models is always, uh, fixed for whatever language you're testing on. Right. And then, there's the testing corpus. So, then I think it's probably instructive to go and - and - and show you the features that we were talking about. Um, so, let's see. Help me out with - With what? @@ P_L_P. P_L_P. P_L_P? O_K. M_S_G. M_S_G. Uh, J_RASTA. J_RASTA. And J_RASTA-L_D_A. J_RASTA-L_D_A. Um, multi-band. Multi-band. So there would be multi-band before, um - before our network, I mean. Yeah, just the multi-band features, right? And - Yeah. Yeah. Uh-huh. Ah. Ah. So, something like, uh, s- T_C_T within bands and - Well. And then multi-band after networks. Meaning that we would have, uh, neural networks, uh, discriminant neural networks for each band. Uh, yeah. And using the - the outputs of these networks or the linear outputs or something like that. Uh, yeah. What about mel cepstrum? Oh, um - Or is that - you don't include that because it's part of the base or something ? Well, y- you do have a baseline Yeah databases. Yeah. system that's m- that's mel cepstra, right? So. Mm-hmm. But, uh, well, not for the - the A_N_N. I mean - O_K. So, yeah, we could - we could add M_F_C_C also. We could add - Probably should. I mean at least - at least conceptually, you know, it doesn't meant you actually have to do it, but conceptually it makes sense as a - as a base line. Yeah . Yeah. It'd be an interesting test just to have - just to do M_F_C_C with the neural net and everything else the same. Compare that with just M_- M_F_C_C without the - the net. Without the - Yeah. Yeah. Mm-hmm. I think - I think Dan did some of that. Oh. Um, in his previous Aurora experiments. And with the net it's - it's wonderful. Without the net it's just baseline. Um, I think O_G_I folks have been doing that, too. D- Because I think that for a bunch of their experiments they used, uh, mel cepstra, actually. Yeah. Yeah. Um, of course that's there and this is here and so on. O_K? O_K. Um, for the training corpus - corpus, um, we have, um, the - the d- digits from the various languages. Um, English Spanish um, French What else do we have? And the Finnish. Finnish. And Italian. Uh, no, Italian no. Italian no. I- Italian yes. Italian? Where did th- where did that come from? Digits? Oh. Oh. Italian. Italian. Is that - Was that distributed with Aurora, or - ? Where did that - ? One L_ or two L_'s? The newer one. Yeah. So English, uh, Finnish and Italian are Aurora. And Spanish and French is something that we can use in addition to Aurora. Uh, well. Yeah, so Carmen brought the Spanish, and Stephane brought the French. O_K. And, um, oh yeah, and - Is it French French or Belgian French? There's a - It's, uh, French French. French French. Like Mexican Spain and Spain. Yeah. Or Swiss. Swiss-German. I think that is more important, Mexican Spain. Because more people - Yeah. Yeah. Yeah, probably so. Yeah. Yeah, Herve always insists that Belgian is - i- is absolutely pure French, has nothing to do with - but he says those - those - those Parisians talk funny. Yeah, yeah, yeah. They have an accent. Yeah they - they do, yeah. Yeah. But then he likes Belgian fries too, so. O_K. And then we have, uh, um, broader - broader corpus, um, like TIMIT. TIMIT so far, right? And Spanish too. Spanish - Oh, Spanish stories? Albayzin is the name. What about T_I-digits? Um, T_I-digits - uh all these Aurora f- d- data p- Uh-huh. data is from - is derived from T_I-digits. Oh. Oh O_K. Um, basically, they - they corrupted it with, uh, different kinds of noises at different S_N_R levels. Ah. I see. Yeah. y- And I think Stephane was saying there's - there's some broader s- material in the French also? Yeah, we cou- we could use - Yeah. O_K. The French data. Spanish stories? No. No. Sp- Not Spanish stories? No. Spanish - No. Spanish something. O_K. Albayz- Yeah. Did the Aurora people actually corrupt it themselves, or just specify the signal and the signal-to-noise ratio? They - they corrupted it, um, themselves, but they also included the - the noise files for us, right? O_K. Yeah. Or - so we can go ahead and corrupt other things. I'm just curious, Carmen - I mean, I couldn't tell if you were joking or - i- Is it - is it Mexican Spanish, or is it - No no no no. No no no no. Spanish from Spain. Oh, no, no. It's - it's Spanish from Spain, Spanish. Yeah, O_K. From Spain. Alright. Spanish from Spain. Yeah, we're really covered there now. O_K. And the French from France. O_K. Yeah, the - No, the French is f- yeah, from, uh, Paris, also. Oh, from Paris, O_K. Yeah. O_K. And TIMIT's from lots of different places. Yeah. Yeah. From T_I. From - i- It's from Texas. So may- maybe it's - From the deep South. So- s- so it's not really from the U_S either. Is that - ? O_K. O_K. And, um, with- within the training corporas um, we're, uh, thinking about, um, training with noise. So, incorporating the same kinds of noises that, um, Aurora is in- incorporating in their, um - in their training corpus. Um, I don't think we- we're given the, uh - the unseen noise conditions, though, right? I think what they were saying was that, um, for this next test there's gonna be some of the cases where they have the same type of noise as you were given before hand and some cases where you're not. Like - Mm-hmm. Mm-hmm. O_K. So, presumably, that'll be part of the topic of analysis of the - the test results, is how well you do when it's matching noise and how well you do where it's not. Right. I think that's right. So, I guess we can't train on - on the - the unseen noise conditions. Well, not if it's not seen, yeah. Right. If - Not if it's unseen. Yeah. O_K. I mean, i- i- i- i- it does seem to me that a lot of times when you train with something that's at least a little bit noisy it can - it can help you out in other kinds of noise even if it's not matching just because there's some more variance that you've built into things. But, but, uh, uh, exactly how well it will work will Mm-hmm. depend on how near it is to what you had ahead of time. So. Mm-hmm. O_K, so that's your training corpus, and then your testing corpus - ? Um, the testing corporas are, um, just, um, the same ones as Aurora testing. And, that includes, um, the English Finnish. Spa- um, Italian. Finnish. Uh, we'r- we're gonna get German, right? Ge- Well, so, yeah, the final test, on a guess, is supposed to be German and Danish, right? At the final test will have German. Uh, yeah. Right. The s- yeah, the Spanish, perhaps, Spanish. Oh yeah, we can - we can test on s- Spanish. we will have. Yeah. But the - the Aurora Spanish, I mean. Oh yeah. Mm-hmm. Oh, there's a - there's Spanish testing in the Aurora? Uh, not yet, but, uh, yeah, Yeah, it's preparing. uh, e- pre- they are preparing it, and, well, according to Hynek it will be - we will have this at the end of November, or - They are preparing. Um. O_K, so, uh, something like seven things in each, uh - each column. Yeah - So that's, uh, three hundred and forty-three, uh, different systems that are going to be developed. There's three of you. Uh, so that's hundred and - hundred and fourteen each. What? Yeah. One hundred each, about. What a- what about noise conditions? w- Don't we need to put in the column for noise conditions? Are you just trying to be difficult? No, I just don't understand. Well, th- uh, when - when I put these testings on there, I'm assumi- I'm just kidding. Yeah. There- there's three - three tests. Um, type-A_, type-B_, and type-C_. And they're all - they're all gonna be test- tested, um, with one training of the H_T_K system. Um, there's a script that tests all three different types of noise conditions. Test-A_ is like a matched noise. Test-B_ is a - is a slightly mismatched. And test-C_ is a, um, mismatched channel. And do we do all our training on clean data? Um, no, no, we're - we're gonna be, um, training on the noise files that we do have. Also, we can clean that. No. So, um - Yeah, so I guess the question is how long does it take to do a - a training? I mean, it's not totally crazy t- I mean, these are - a lot of these are built-in things and we know - we have programs that compute P_L_P, we have M_S_G, we have J_RA- you know, a lot of these things will just kind of happen, won't take uh a huge amount of development, it's just trying it out. So, we actually can do quite a few experiments. But how - how long does it take, do we think, for one of these Mm-hmm. trainings? That's a good question. What about combinations of things? Oh yeah, that's right. I mean, cuz, so, for instance, I think the major advantage of M_S_G - Yeah, good point. Oh! Och! A major advantage of M_S_G, I see, th- that we've seen in the past is combined with P_L_P. Yeah. Um. Now, this is turning into a four-dimensional cube? Or you just add it to the features. No. Here. Well, you just select multiple things on the one dimension. Just - Oh, yeah. O_K. Yeah, so, I mean, you don't wanna, uh - Let's see, seven choose two would - - be, uh, twenty-one different combinations. Um. It's not a complete set of combinations, though, right? Probably - What? It's not a complete set of combinations, though, right? No. Yeah, I hope not. Yeah, there's - That would be - Uh, yeah, so P_L_P and M_S_G I think we definitely wanna try cuz we've had a lot of good experience with putting those together. Mm-hmm. Um. Yeah. When you do that, you're increasing the size of the inputs to the net. Do you have to reduce the hidden layer, or something? Well, so - I mean, so i- it doesn't increase the number of trainings. No, no, I'm - I'm just wondering about number of parameters in the net. Do you have to worry about keeping that the same, or - ? But - Uh, I don't think so. There's a computation limit, though, isn't there? Yeah, I mean, it's just more compu- Excuse me? Isn't there like a limit on the computation load, or d- latency, or something like that for Aurora task? Oh yeah, we haven't talked about any of that at all, have we? No. Yeah, so, there's not really a limit. What it is is that there's - there's, uh - it's just penalty, you know? That - that if you're using, uh, a megabyte, then they'll say that's very nice, but, of course, it will never go on a cheap cell phone. O_K. Um. And, u- uh, I think the computation isn't so much of a problem. I think it's more the memory. Uh, and, expensive cell phones, exa- expensive hand-helds, and so forth, are gonna have lots of memory. So it's just that, uh, these people see the - the cheap cell phones as being still the biggest market, so. Mm-hmm. Um. But, yeah, I was just realizing that, actually, it doesn't explode out, um - It's not really two to the seventh. But it's - but - but - i- i- it doesn't really explode out the number of trainings cuz these were all trained individually. Right? So, uh, if you have all of these nets trained some place, then, uh, you can combine their outputs and do the K_L transformation and so forth and - Mm-hmm. and, uh - So, what it - it blows out is the number of uh testings. And, you know - and the number of times you do that last part. But that last part, I think, is so - has gotta be pretty quick, so. Uh. Right? I mean, it's just running the data through - Oh. Well, you gotta do the K_L transformation, but - But wh- what about a net that's trained on multiple languages, though? Eight - y- Is that just separate nets for each language then combined, or is that actually one net trained on? Necessary to put in . Uh, probably one net. Well. Good question. Uh. One would think one net, So. but we've - I don't think we've tested that. Right? So, in the broader training corpus we can - we can use, uh, the three, or, a combination of - of two - two languages. Database three. In one net. Yeah. Mm-hmm. Yeah, so, I guess the first thing is if w- if we know how much a - how long a - a training takes, if we can train up all these - these combinations, uh, then we can start working on testing of them individually, and in combination. Right? Mm-hmm. Because the putting them in combination, I think, is not as much computationally as the r- training of the nets in the first place. Yeah. Right? So y- you do have to compute the K_L transformation. Uh, which is a little bit, but it's not too much. It's not too much, no. Yeah. So it's - But - Yeah. But there is the testing also, which implies training, uh, the H_T_K models and, well, The - the model - the H_T_K model. Uh, right. it's - yeah. But it's - it's - it's not so long. It @@ - Yeah. Right. So if you do have lots of combinations, it's - How long does it take for an, uh, H_T_K training? It's around six hours, I think. For training and testing, yeah. It depends on the - More than six hours. For the Italian, yes. More. Maybe one day. One day? For H_T_K? Really? Yeah. Well. Running on what? Uh, M_ - M_F_C_C. No, I'm sorry, ru- running on what machine? Uh, Ravioli. Uh, I don't know what Ravioli is. Is it - is it an Ultra-five, or is it a - ? mmm Um. I don't know. I don't know. Who is that? I don't know. I don't know. I don't know what a Ravioli is. I don't know. We can check really quickly, I guess. Yeah, I- I think it's- it's- it's not so long because, well, the T_I-digits test data is about, uh how many hours? Uh, th- uh, thirty hours of speech, I think, something like that. It's a few hours. Hmm. Yeah. And it p- Well. Right, so, I mean, clearly, there - there's no way we can even begin to do an- any significant amount here unless we use multiple machines. It's six hours. Right? So - so - w- we - I mean there's plenty of machines here and they're n- they're often not in - in a great - great deal of use. So, I mean, I think it's - it's key that - that the - that you look at, uh, you know, what machines are fast, what machines are used a lot - Uh, are we still using P_make? Is that - ? Oh, I don't know how w- how we would P_make this, though. Um. Well, you have a - I mean, once you get the basic thing set up, you have just all the - uh, a- all these combinations, right? Yeah. Mm-hmm. Um. It's - it's - let's say it's six hours or eight hours, or something for the training of H_T_K. How long is it for training of - of, uh, the neural net? The neural net? Um. I would say two days. Depends on the corpuses, right? It depends. Yeah. It s- also depends on the net. Depends on the corpus. Yeah. How big is the net? For Albayzin I trained on neural network, uh, was, um, one day also. Uh, but on what machine? On a SPERT board. Uh. I - I think the neural net SPERT. Yes. Y- you did a - you did it on a SPERT board. O_K, again, we do have a bunch of SPERT boards. Yeah. And I think there - there - there's - I think you folks are probably go- the ones using them right now. Is it faster to do it on the SPERT, or - ? Uh, don't know. Used to be. It's - it's still a little faster on the SPERTs. Yeah, yeah. Is it? Ad- Adam - Adam did some testing. Or either Adam or - or Dan did some testing and they found that the SPERT board's still - still faster. And Mm-hmm. Mm-hmm. the benefits is that, you know, you run out of SPERT and then you can do other things on your - your computer, and you don't - Mm-hmm. Yeah. So you could be - we have quite a few SPERT boards. You could set up, uh, you know, ten different jobs, or something, to run on SPERT - different SPERT boards and - and have ten other jobs running on different computers. So, it's got to take that sort of thing, or - or we're not going to get through any significant number of these. Yeah. Um. So this is - Yeah, I mean, I kind of like this because what it - No - uh, no, what I like about it is we - we - we do have a problem that we have very limited time. O_K. You know, so, with very limited time, we actually have really quite a - quite a bit of computational resource available if you, you know, get a look across the institute and how little things are being used. And uh, on the other hand, almost anything that really i- you know, is - is new, where we're saying, Yeah. "Well, let's look at, like we were talking before about, uh, uh, voiced-unvoiced-silence detection features and all those sort -" that's - I think it's a great thing to go to. But if it's new, then we have this development and - and - and learning process t- to - to go through on top of - just the - the - all the - all the work. So, I - I - I don't see how we'd do it. So what I like about this is you basically have listed all the things that we already know how to do. And - and all the kinds of data that we, at this point, already have. Yeah. And, uh, you're just saying let's look at the outer product of all of these things and see if we can calculate them. a- a- Am I - am I interpreting this correctly? Is this sort of what - what you're thinking of doing in the short term? Mmm. O_K. Yeah. So - so then I think it's just the - the missing piece is that you need to, uh, you know - you know, talk to - talk to, uh, Chuck, talk to, uh, Adam, uh, sort out about, uh, what's the best way to really, you know, attack this as a - as a - as a mass problem in terms of using many machines. Uh, and uh, then, you know, set it up in terms of scripts and so forth, and - uh, in - in kind o- some kind of structured way. Uh. Um, and, you know, when we go to, uh, O_G_I next week, uh, we can then present to them, you know, what it is that we're doing. And, uh, we can pull things out of this list that we think they are doing sufficiently, that, you know, we're not - we won't be contributing that much. Mmm. Mm-hmm. Um. And, uh - Then, uh, like, we're there. Yeah? How big are the nets you're using? Um, for the - for nets trained on digits, um, we have been using, uh, four hundred order hidden units. And, um, for the broader class nets we're - we're going to increase that because the, um, the digits nets only correspond to about twenty phonemes. Uh-huh. So. Broader class? Um, the broader - broader training corpus nets like TIMIT. Um, w- we're gonna - Oh, it's not actually broader class, it's actually finer class, but you mean - Right. y- You mean Right. Yeah. more classes. More classes. Right, right. More classes. That's what I mean. Mm-hmm. Yeah. Yeah. Yeah. And. Yeah. Carmen, did you - do you have something else to add? We - you haven't talked too much, and - D- I begin to work with the Italian database to - nnn, to - with the f- front-end and with the H_T_K program and the @@ . And I trained eh, with the Spanish two neural network with P_L_P and with LogRASTA P_L_P. I don't know exactly what is better if - if LogRASTA or J_RASTA. Well, um, J_RASTA has the potential to do better, but it doesn't always. It's - i- i- J_RASTA is more complicated. It's - it's, uh - instead of doing RASTA with a log, you're doing RASTA with a log-like function that varies depending on a J_ parameter, uh, which is supposed to be sensitive to the amount of noise there is. So, it's sort of like the right transformation to do the filtering in, is dependent on how much noise there is. Hm-hmm. And so in J_RASTA you attempt to do that. It's a little complicated because once you do that, you end up in some funny domain and you end up having to do a transformation afterwards, which requires some tables. And, uh, so it's - it's - it's a little messier, uh, there's more ways that it can go wrong, uh, but if - if - if you're careful with it, it can do better. So, it's - So. Hm-hmm. It's a bit - I'll do better. Um, and I think to - to - to recognize the Italian digits with the neural netw- Spanish neural network, and also to train another neural network with the Spanish digits, the database of Spanish digits. And I working that. Yeah. But prepa- to prepare the - the database are difficult. Was for me, n- it was a difficult work last week with the labels because the - the program with the label obtained that I have, the Albayzin, is different w- to the label to train the neural network. And that is another work that we must to do, to - to change . I - I didn't understand. Uh, for example Albayzin database was labeled automatically with H_T_K. It's not hand - it's not labels by hand. Labels. I'm sorry, I'm sorry. The labels. I'm sorry. Oh, "l- labeled ". I'm sorry, I have a p- I had a problem with the pronunciation. The labels. Yeah, O_K. So, O_K, so let's start over. So, TI- TIMI- TIMIT's hand-labeled, and - and you're saying about the Spanish? Oh, also that - Yes. The Spanish labels? That was in different format, that the format for the em - the program to train the neural network. Oh, I see. I necessary to convert. And someti- well - It's - it's - Yeah. So you're just having a problem converting the labels. Yeah, but n- yes, because they have one program, Feacalc, but no, l- LabeCut, but don't - doesn't, eh, include the H_T_K format to convert. Mm-hmm. And, Hmm. I don't know what. I ask - e- even I ask to Dan Ellis what I can do that, and h- they - he say me that h- he does- doesn't any - any s- any form to - to do that. And at the end, I think that with LabeCut I can transfer to ASCII format, and H_T_K is an ASCII format. And I m- do another, uh, one program to put ASCII format of H_T_K to ase- ay- ac- ASCII format to Mm-hmm. Exceed and they used LabCut to - to pass . O_K, yeah. Actually that was complicated, but well, I know how we can did that - do that. So you- Sure. So it's just usual kind of uh - sometimes say housekeeping, right? To get these - get these things sorted out. Yeah. So it seems like there's - there's some peculiarities of the, uh - of each of these dimensions that are getting sorted out. And then, um, if - if you work on getting the, uh, assembly lines together, and then the - the pieces sort of get ready to go into the assembly line and gradually can start, you know, start turning the crank, more or less. And, uh, uh, we have a lot more computational capability here than they do at O_G_I, so I think that i- if - What's - what's great about this is it sets it up in a very systematic way, so that, uh, once these - all of these, you know, mundane but real problems get sorted out, we can just start turning the crank and - and Mm-hmm. push all of us through, and then finally figure out what's best. Yeah. Um, I - I was thinking two things. Uh, the first thing was, um - we - we actually had thought of this as sort of like, um - not - not in stages, but more along the - the time axis. Just kind of like one stream at a time, je-je-je-je-je check out the results and - and go that way. Mm-hmm. Oh, yeah, yeah, sure. No, I'm just saying, I'm just thinking of it like loops, right? And so, y- y- y- if you had three nested loops, that you have a choice for this, a choice for this, and a choice for that, right? And you're going through them all. That - that's what I meant. Uh-huh. Yeah. Mm-hmm. Right, right. And, uh, the thing is that once you get a better handle on how much you can realistically do, uh, um, concurrently on different machines, different SPERTs, and so forth, uh, and you see how long it takes on what machine and so forth, you can stand back from it and say, "O_K, if we look at all these combinations we're talking about, and combinations of combinations, and so forth," you'll probably find you can't do it all. Mm-hmm. O_K. O_K, so then at that point, uh, we should sort out which ones do we throw away. Which of the combinations across - you know, what are the most likely ones, and - Mm-hmm. And, uh, I still think we could do a lot of them. I mean, it wouldn't surprise me if we could do a hundred of them or something. But, probably when you include all the combinations, you're actually talking about a thousand of them or something, and that's probably more than we can do. Uh, but a hundred is a lot. And - and, uh, um - O_K. Yeah. Yeah, and the - the second thing was about scratch space. And I think you sent an email about, um, e- scratch space for - for people to work on. And I know that, uh, Stephane's working from an N_T machine, so his - his home directory exists somewhere else. His - his stuff is somewhere else, yeah. Yeah, I mean, my point I - I want to - Yeah, thanks for bring it back to that. My - th- I want to clarify my point about that - that - that Chuck repeated in his note. Um. We're - over the next year or two, we're gonna be upgrading the networks in this place, but right now they're still all te- pretty much all ten megabit lines. Mm-hmm. And we have reached the - this - the machines are getting faster and faster. So, it actually has reached the point where it's a significant drag on the time for something to move the data from one place to another. Mm-hmm. So, you - you don't w- especially in something with repetitive computation where you're going over it multiple times, you do - don't want to have the - the data that you're working on distant from where it's being - where the computation's being done if you can help it. Mm-hmm. Uh. Now, we are getting more disk for the central file server, which, since it's not a computational server, would seem to be a contradiction to what I just said. But the idea is that, uh, suppose you're working with, uh, this big bunch of multi- multilingual databases. Um, you put them all in the central ser- at the cen- central file server. Mm-hmm. Then, when you're working with something and accessing it many times, you copy the piece of it that you're working with over to some place that's close to where the computation is and then do all the work there. And then that way you - you won't have the - the network - you won't be clogging the network for yourself and others. That's the idea. Mmm. So, uh, it's gonna take us - It may be too late for this, uh, p- precise crunch we're in now, but, uh, we're, uh - It's gonna take us a couple weeks at least to get the, uh, uh, the amount of disk we're gonna be getting. We're actually gonna get, uh, I think four more, uh, thirty-six gigabyte drives and, uh, put them on another - another disk rack. We ran out of space on the disk rack that we had, so we're getting another disk rack and four more drives to share between, uh - primarily between this project and the Meetings - Meetings Project. Um. But, uh, we've put another - I guess there's another eighteen gigabytes that's - that's in there now to help us with the immediate crunch. But, uh, are you saying - So I don't know where you're - Stephane, where you're doing your computations. If - i- so, you're on an N_T machine, so you're using some external machine to - Yeah, it, uh - Well, to - It's Nutmeg and Mustard, I think, the - Do you know these yet? I don't know what kind. Nuh-uh. Yeah, O_K. Uh, are these - are these, uh, computational servers, or something? I'm - I've been kind of out of it. Yeah, I think, yeah. I think so. Mmm. Unfortunately, these days my idea of running comput- of computa- doing computation is running a spread sheet. So. Mmm. Uh, haven't been - haven't been doing much computing personally, so. Um. Yeah, so those are computational servers. So I guess the other question is what disk there i- space there is there on the computational servers. Right. Yeah, I'm not sure what's available on - is it - you said Nutmeg and what was the other one? Mustard. Mustard. O_K. Huh. Yeah, Well, you're the - you're the disk czar now. So Right, right. Well , I'll check on that. Yeah. Yeah, so basically, uh, Chuck will be the one who will be sorting out what disk needs to be where, and so on, and I'll be the one who says, "O_K, spend the money." So. Which, I mean, n- these days, uh, if you're talking about scratch space, it doesn't increase the, uh, need for backup, and, uh, I think it's not that big a d- and the - the disks themselves are not that expensive. Right now it's - What you can do, when you're on that machine, is, uh, just go to the slash-scratch directory, and do a D_F minus K_, and it'll tell you if there's space available. Yeah. Uh, and if there is then, uh - But wasn't it, uh - I think Dave was saying that he preferred that people didn't put stuff in slash-scratch. It's more putting in d- s- X_A or X_B or, right? Well, there's different - there, um, there's - Right. So there's the slash-X_-whatever disks, and then there's slash-scratch. And both of those two kinds are not backed up. And if it's called "slash-scratch", it means it's probably an internal disk to the machine. Um. And so that's the kind of thing where, like if - um, O_K, if you don't have an N_T, but you have a - a - a Unix workstation, and they attach an external disk, it'll be called "slash-X_-something" uh, if it's not backed up and it'll be "slash-D_-something" if it is backed up. And if it's inside the machine on the desk, it's called "slash-scratch". But the problem is, if you ever get a new machine, they take your machine away. It's easy to unhook the external disks, put them back on the new machine, but then your slash-scratch is gone. So, you don't wanna put anything in slash-scratch that you wanna keep around for a long period of time. But if it's a copy of, say, some data that's on a server, you can put it on slash-scratch because, um, first of all it's not backed up, and second it doesn't matter if that machine disappears and you get a new machine because you just recopy it to slash-scratch. So tha- that's why I was saying you could check slash-scratch on those - on - on, um, Mustard and - and Nutmeg to see if - if there's space that you could use there. I see. You could also use slash-X_-whatever disks on Mustard and Nutmeg. Yeah, yeah. Um. Yeah, and we do have - I mean, yeah, so - so you - yeah, it's better to have things local if you're gonna run over them lots of times so you don't have to go to the network. Right, so es- so especially if you're - right, if you're - if you're taking some piece of the training corpus, which usually resides in where Chuck is putting it all on the - on the, uh, file server, uh, then, yeah, it's fine if it's not backed up because if it g- g- gets wiped out or something, y- I mean it is backed up on the other disk. So, yeah, O_K. Mm-hmm. Yeah, so, one of the things that I need to - I've started looking at - Uh, is this the appropriate time to talk about the disk space stuff? Sure. I've started looking at, um, disk space. Dan - David, um, put a new, um, drive onto Abbott, that's an X_ disk, which means it's not backed up. So, um, I've been going through and copying data that is, you know, some kind of corpus stuff usually, that - that we've got on a C_D-ROM or something, onto that new disk to free up space on other disks. And, um, so far, um, I've copied a couple of Carmen's, um, databases over there. We haven't deleted them off of the slash-D_C disk that they're on right now in Abbott, um, uh, but we - I would like to go through - sit down with you about some of these other ones and see if we can move them onto, um, this new disk also. Yeah, O_K. There's - there's a lot more space there, and it'll free up more space for doing the experiments and things. So, anything that - that you don't need backed up, we can put on this new disk. Um, but if it's experiments and you're creating files and things that you're gonna need, you probably wanna have those on a disk that's backed up, just in case something goes wrong. So. Um So far I've - I've copied a couple of things, but I haven't deleted anything off of the old disk to make room yet. Um, and I haven't looked at the - any of the Aurora stuff, except for the Spanish. So I - I guess I'll need to get together with you and see what data we can move onto the new disk. Yeah, O_K. Um, yeah, I - I just - an- another question occurred to me is - is what were you folks planning to do about normalization? Uh. Um. Well, we were thinking about using this systematically for all the experiments. Um. This being - ? So, but - Uh. So that this could be another dimension, but we think perhaps we can use the - the best, uh, um, uh, normalization scheme as O_G_I is using, so, with parameters that they use there, or - and - Yeah, I think that's a good idea. I mean it's i- i- we - we seem to have enough dimensions as it is. So probably if we u- u- Yeah, yeah, yeah. sort of take their - probably the on-line - line normalization because then it - it's - if we do anything else, we're gonna end up having to do on-line normalization too, so we may as well just do on-line normalization. Mm-hmm. So. Um. So that it's plausible for the final thing. Good. Um. So, I guess, yeah, th- the other topic - I - maybe we're already there, or almost there, is goals for the - for next week's meeting. Uh. i- i- i- it seems to me that we wanna do is flush out what you put on the board here. Uh. You know, maybe, have it be somewhat visual, a little bit. Uh, so w- we can say what we're doing, yeah. O_K. Like a s- like a slide? O_K. And, um, also, if you have sorted out, um, this information about how long i- roughly how long it takes to do on what and, you know, what we can - how many of these trainings, uh, uh, and testings and so forth that we can realistically do, uh, then one of the big goals of going there next week would be to - to actually settle on which of them we're gonna do. And, uh, when we come back we can charge in and do it. Um. Anything else that - I- a- a- Actually - started out this - this field trip started off with - with, uh, Stephane talking to Hynek, so you may have - you may have had other goals, uh, for going up, and any- anything else you can think of would be - we should think about accomplishing? I mean, I'm just saying this because maybe there's things we need to do in preparation. Oh , I think basically, this is - this is, uh, yeah. O_K. O_K. Uh. Alright. And uh - and the other - the - the last topic I had here was, um, uh d- Dave's fine offer to - to, uh, do something on this. I mean he's doing - - he's working on other things, but to - to do something on this project. So the question is, "Where - where could we, uh, uh, most use Dave's help?" Um, yeah, I was thinking perhaps if, um, additionally to all these experiments, which is not really research, well I mean it's, uh, running programs and, um, Yeah. trying to have a closer look at the - perhaps the, um, speech, uh, noise detection or, uh, voiced-sound-unvoiced-sound detection and - Which could be important in - i- for noise - noise - I think that would be a - I think that's a big - big deal. Because the - you know, the thing that Sunil was talking about, uh, with the labels, uh, labeling the database when it got to the noisy stuff? The - That - that really throws things off. You know, having the noise all of a sudden, your - your, um, speech detector, I mean the - the, um - What was it? What was happening with his thing ? He was running through these models very quickly. He was getting lots of, uh, Mmm. uh insertions, is what it was, in his recognitions. The only problem - I mean, maybe that's the right thing - the only problem I have with it is exactly the same reason why you thought it'd be a good thing to do. Um, I - I think that - Let's fall back to that. But I think the first responsibility is sort of to figure out if there's something that, uh, an - an additional - Uh, that's a good thing you - remove the mike. Go ahead, good. Uh, uh. What an additional clever person could help with when we're really in a crunch for time. Right? Cuz Dave's gonna be around for a long time, right? He's - he's gonna be here for years. Yeah. Yeah. And so, um, over years, if he's - if he's interested in, you know, voiced-unvoiced-silence, he could do a lot. But if there - if in fact there's something else that he could be doing, that would help us when we're - we're sort of uh strapped for time - We have - we - we've, you know, only, uh, another - another month or two to - you know, with the holidays in the middle of it, um, to - to get a lot done. If we can think of something - some piece of this that's going to be - The very fact that it is sort of just work, and i- and it's running programs and so forth, is exactly why it's possible that it - some piece of could be handed to someone to do, because it's not - Uh, yeah, so that - that's the question. And we don't have to solve it right this s- second, but if we could think of some - some piece that's - that's well defined, that he could help with, he's expressing a will- willingness to do that. What about training up a, um, Uh. a multilingual net? Yeah. Yes, maybe to, mmm, put together the - the label - the labels between TIMIT and Spanish or something like that. Yeah, so defining the superset, and, uh, joining the data and - Yes. Yeah. Mmm. Yeah. Uh. Yeah, that's something that needs to be done in any event. Yeah. So what we were just saying is that - that, um - I was arguing for, if possible, coming up with something that - that really was development and wasn't research because we - we're - we have a time crunch. And so, uh, if there's something that would - would save some time that someone else could do on some other piece, then we should think of that first. See the thing with voiced-unvoiced-silence is I really think that - that it's - to do - to do a - a - a - a poor job is - is pretty quick, uh, or, you know, a so-so job. You can - you can - you can throw in a couple fea- we know what - what kinds of features help with it. Hmm. You can throw something in. You can do pretty well. But I remember, in fact, when you were working on that, and you worked on for few months, as I recall, and you got to, say ninety-three percent, and Mm-hmm. getting to ninety-four really really hard. Another year. Yeah, yeah. So, um - And th- th- the other tricky thing is, since we are, uh, even though we're not - we don't have a strict prohibition on memory size, and - and computational complexity, uh, clearly there's some limitation to it. So if we have to - if we say we have to have a pitch detector, say, if we - if we're trying to incorporate pitch information, or at least some kind of harmonic - harmonicity, or something, this is another whole thing, take a while to develop. Anyway, it's a very very interesting topic. I mean, one - I think one of the - a lot of people would say, and I think Dan would also, uh, that one of the things wrong with current speech recognition is that we - we really do throw away all the harmonicity information. Uh, we try to get spectral envelopes. Reason for doing that is that most of the information about the phonetic identity is in the spectral envelopes are not in the harmonic detail. But the harmonic detail does tell you something. Like the fact that there is harmonic detail is - is real important. So. Um. So, uh. So I think - Yeah. So - wh- that - so the - the other suggestion that just came up was, well what about having him work on the, uh, multilingual super f- superset kind of thing. Uh, coming up with that and then, you know, training it - training a net on that, say, um, from - from, uh - from TIMIT or something. Is that - or uh, for multiple databases. What - what would you - what would you think it would - wh- what would this task consist of? Yeah, it would consist in, uh, well, um, creating the - the superset, and, uh, modifying the lab- labels for matching the superset. Uh. Uh, creating a superset from looking at the multiple languages, and then creating i- m- changing labels on TIMIT? Well, creating the mappings, actually. Yeah. Or on - or on multiple language - multiple languages? No. Yeah, yeah, with the @@ three languages, and - The multiple language. Maybe for the other language because TIMIT have more phone. Yeah. Uh. So you'd have to create a mapping from each language to the superset. Yeah. Mm-hmm. From each language to the superset, yeah. Mmm. Yeah. There's, um - Carmen was talking about this SAMPA thing, and it's, um, it's an effort by linguists to come up with, um, a machine readable I_P_A, um, sort of thing, right? And, um, they - they have a web site that Stephane was showing us that has, um - Yeah. has all the English phonemes and their SAMPA correspondent, um, phoneme, and then, um, they have Spanish, they have German, they have all - all sorts of languages, um, mapping - mapping to the SAMPA phonemes, which - Yeah, the tr- the transcription, though, for Albayzin is n- the transcription are of SAMPA the same, uh, how you say, symbol that SAMPA appear. SAMPA? What does "SAMPA" mean? Mm-hmm. Hmm. But I don't know if TIMIT o- how is TIMIT. So, I mean - What - I'm sorry. Go ahead. I was gonna say, does that mean I_P_A is not really international? No, it's - it's saying - y- It uses special diacritics and stuff, which you can't do with ASCII characters. Yeah. can't print on ASCII. Oh, I see. So the SAMPA's just mapping those. Got it. What, uh - Has O_G_I done anything about this issue? Do they have - Do they have any kind of superset that they already have? I don't think so. Well, they - they - they're going actually the - the other way, defining uh, phoneme clusters, apparently. Well. Aha. That's right. Uh, and that's an interesting way to go too. So they just throw the speech from all different languages together, then cluster it into sixty or fifty or whatever clusters? I think they've not done it, uh, doing, uh, multiple language yet, but what they did is to training, uh, English nets with all the phonemes, and then training it in English nets with, uh, kind of seventeen, I think it was - seventeen, uh, broad classes. Automatically derived - Mm-hmm. Yeah. Automatically derived broad classes, or - ? Uh-huh. Yeah, I think so. Uh, and, yeah. And the result was that apparently, when testing on cross-language it was better. I think so. But Hynek didn't add - didn't have all the results when he showed me that, so, well. But - So that does make an interesting question, though. Is there's some way that we should tie into that with this. Um. Right? I mean, if - if in fact that is a better thing to do, should we leverage that, rather than doing, um, our own. Right? So, if i- if - if they s- I mean, we have - i- we have the - the trainings with our own categories. And now we're saying, "Well, how do we handle cross-language?" And one way is to come up with a superset, but they are als- they're trying coming up with clustered, and do we think there's something wrong with that? I think that there's something wrong or - Well, because - O_K. What w- Well, for the moment we are testing on digits, and e- i- perhaps u- using broad phoneme classes, it's - it's O_K for um, uh classifying the digits, but as soon as you will have more words, well, words can differ with only a single phoneme, and - which could be the same, uh, class. Well. I see. So. Right. Although, you are not using this for the - You're using this for the feature generation, though, not the - So, I'm afraid - Yeah, but you will ask the net to put one for th- th- the phoneme class and - s- Yeah. So. Yeah. So you're saying that there may not be enough information coming out of the net to help you discriminate the words? Well. Yeah, yeah. Mmm. Fact, most confusions are within the phone - phone classes, right? I think, uh, Larry was saying like obstruents are only confused with other obstruents, et cetera, et cetera. Yeah. Hmm. Yeah. Yeah. Yeah, this is another p- yeah, another point. Yeah. So - so, maybe we could look at articulatory type stuff, right? But that's what I thought they were gonna - Did they not do that, or - ? I don't think so. Well, they were talking about, perhaps, but they d- I d- w- Yeah. So - They're talking about it, but that's sort of a question whether they did because that's - that's the other route to go. Instead of this, you know - Mm-hmm. Superclass. Instead of the - the - the - the superclass thing, which is to take - So suppose y- you don't really mark arti- To really mark articulatory features, you really wanna look at the acoustics and - and see where everything is, and we're not gonna do that. So, uh, the second class way of doing it is to look at the, uh, phones that are labeled and translate them into acoustic - uh, uh - articulatory, uh, uh, features. So it won't really be right. You won't really have these overlapping things and so forth, but - So the targets of the net - are these - ? Articulatory feature. Articulatory features. Right. But that implies that you can have more than one on at a time? That's right. Ah. O_K. You either do that or you have multiple nets. I see. Um. And, um I don't know if our software - this - if the qu- versions of the Quicknet that we're using allows for that. Do you know? Allows for - ? Multiple targets being one? Oh, um, we have gotten soft targets to - to work. O_K. So that - that'll work, yeah. Yeah. O_K. So, um, that's another thing that could be done - is that we could - we could, uh, just translate - instead of translating to a superset, Um. just translate to articulatory features, some set of articulatory features and train with that. Now the fact - even though it's a smaller number, it's still fine because you have the - the, uh, combinations. So, in fact, it has every, you know - it had - has - has every distinction in it that you would have the other way. Yeah. But it should go across languages better. We could do an interesting cheating experiment with that too. We could - I don't know, if you had uh the phone labels, you could replace them by their articulatory features and then feed in a vector with those uh, things turned on based on what they're supposed to be for each phone to see if it - if you get a big win. Do you know what I'm saying? No. So, um, I mean, if your net is gonna be outputting, uh, a vector of - basically of - well, it's gonna have probabilities, but let's say that they were ones and zeros, then y- and you know for each, um, I don't know if you know this for your testing data, but if you know for your test data, you know, what the string of phones is and - and you have them aligned, then you can just - instead of going through the net, just create the vector for each phone and feed that in to see if that data helps. Eh, eh, what made me think about this is, I was talking with Hynek and he said that there was a guy at A_T_and_T who spent eighteen months working on a single feature. And because they had done some cheating experiments - This was the guy that we were just talking a- that we saw on campus. So, this was Larry Saul who did this - did this. He used sonorants. Oh, O_K. Right, O_K, right. Was what he was doing. Yeah. And they - they had done a cheating experiment or something, right? and determined that - He - he di- he didn't mention that part. But. Well, Hynek said that - that, I guess before they had him work on this, they had done some experiment where if they could get that one feature right, I see. O_K. it dramatically improved the result. So I was thinking, you know - it made me think about this, that if - it'd be an interesting experiment just to see, you know, if you did get all of those right. Should be. Because if you get all of them in there, that defines all of the phones. So that's - that's equivalent to saying that you've got - Right. got all the phones right. So, if that doesn't help, there's - Yeah. Although, yeah, it would be - make an interesting cheating experiment because we are using it in this funny way, where we're converting it into features. Yeah. And then you also don't know what error they've got on the H_T_K side. You know? Yeah. It sort of gives you your - the best you could hope for, kind of. Mmm. Mmm, I see. The soft training of the nets still requires the vector to sum to one, though, right? To sum up to one. So you can't really feed it, like, two articulatory features that are on at the same time with ones cuz it'll kind of normalize them down to one half or something like that, for instance. But perhaps you have the choice of the final nonl- uh, nonlinearity, yeah. Right. Nonlinearity? Um, it's sig- No, it's actually sigmoid-X_ Is it always softmax or - ? Yeah. for the - So if you choose sigmoid it's o- it's O_K? You, um - I think - I think apparently, the, uh - Did we just run out of disk, or - ? Why don't you just choose linear? What's that? Right? Linear outputs? Linear outputs? Isn't that what you'll want? Um. If you're gonna do a K_L Transform on it. Right, right. Right, but during the training, we would train on sigmoid-X_ and then Oh, you - Yeah? at the end just chop off the final nonlinearity. Hmm. So, we're - we're - we're off the air, or - ? Um, about to be About to be off the air. 