Eight, eight? Three. O_K, we're going. This is three. Yep. Yep. Test. Hmm. Let's see. Move it bit. Test? Test? O_K, I guess it's alright. So, let's see. Yeah, Barry's not here and Dave's not here. Um, I can say about - just q- just quickly to get through it, that Dave and I submitted this A_S_R_U. This is for A_S_R_U. Yeah. So. Um. Yeah, it's - it's interesting. I mean, basically we're dealing with rever- reverberation, and, um, when we deal with pure reverberation, the technique he's using works really, really well. Uh, and when they had the reverberation here, uh, we'll measure the signal-to-noise ratio and it's, uh, about nine D_B. So, um, Hmm. a fair amount of - You mean, from the actual, uh, recordings? k- Yeah. It's nine D_B? Yeah. Um - And actually it brought up a question which may be relevant to the Aurora stuff too. Um, I know that when you figured out the filters that we're using for the Mel scale, there was some experimentation that went on at - at, uh - at O_G_I. Um, but one of the differences that we found between the two systems that we were using, the - the Aurora H_T_K system baseline system and the system that we were - the - the uh, other system we were using, the uh, the S_R_I system, was that the S_R_I system had maybe a, um, hundred hertz high-pass. Yep. And the, uh, Aurora H_T_K, it was like twenty. S- sixty-four. Uh. S- sixty-four. Sixty-four? Uh. Yeah, if you're using the baseline. Is that the ba- band center? No, the edge. The edge is really, uh, sixty-four? For some reason, uh, Yeah . @@ So the, uh, center would be somewhere around Dave thought it was twenty, but. like hundred and - hundred and - hundred - hundred and - maybe - it's like - fi- hundred hertz. But do you know, for instance, h- how far down it would be at twenty hertz? What the - how much rejection would there be at twenty hertz, let's say? At twenty hertz. Yeah, any idea what the curve looks like? Twenty hertz frequency - Oh, it's - it's zero at twenty hertz, right? The filter? Yea- actually, the left edge of the first filter is at sixty-four. So - Sixt- s- sixty-four. So anything less than sixty-four is zero. Mmm. It's actually set to zero? Yeah. What kind of filter is that? Yeah. Is this - oh, from the - from - It - This is the filter bank in the frequency domain that starts at sixty-four. Yeah. Oh, so you, uh - so you really set it to zero, the F_F_T? Yeah, yeah. So it's - it's a weight on the ball spectrum. Triangular weighting. Right. O_K. Um - O_K. So that's - that's a little different than Dave thought, I think. But - but, um, still, it's possible that we're getting in some more noise. So I wonder, is it - @@ Was there - their experimentation with, uh, say, throwing away that filter or something? And, uh - Uh, throwing away the first? Yeah. Um, yeah, we - we've tried including the full - full bank. Right? From zero to four K_. And that's always worse than using sixty-four hertz. Mm-hmm. Right, but the question is, whether sixty-four hertz is - is, uh, too, uh, low. Yeah, I mean, make it a hundred or so? Yeah. I t- I think I've tried a hundred and it was more or less the same, or slightly worse. On what test set? On the same, uh, SpeechDat-Car, Aurora. Um, it was on the SpeechDat-Car. Yeah. So I tried a hundred to four K_. Yeah. Um, So it was - and on - and on the, um, um, T_I-digits also? No, no, no. I think I just tried it on SpeechDat-Car. Mmm. That'd be something to look at sometime because what, um, eh, he was looking at was performance in this room. Mm-hmm. Would that be more like - Well, you'd think that'd be more like SpeechDat-Car, I guess, in terms of the noise. The SpeechDat-Car is more, uh, sort of roughly stationary, a lot of it. And - Yeah. and T_I-digits maybe is not so much as - Yeah. Mm-hmm. Yeah. Mm-hmm. O_K. Well, maybe it's not a big deal. But, um - Anyway, that was just something we wondered about. But, um, uh, certainly a lot of the noise, uh, is, uh, below a hundred hertz. Uh, the Yeah. signal-to-noise ratio, you know, looks a fair amount better if you - if you high-pass filter it from this room. But, um - but it's still pretty noisy. Even - even for a hundred hertz up, it's - it's still fairly noisy. The signal-to-noise ratio is - is - Mm-hmm. is actually still pretty bad. So, um, I mean, the main - the - the - Hmm. So that's on th- that's on the f- the far field ones though, right? Yeah. Yeah, that's on the far field. Yeah, the near field's pretty good. So wha- what is, uh - what's causing that? Well, we got a - a video projector in here, uh, and, uh - which we keep on during every - every session we record, which, you know, I - I - Yeah. w- we were aware of but - but we thought it wasn't a bad thing. I mean, that's a Uh-huh. Yeah. nice noise source. Uh, and there's also the, uh - uh, air conditioning. Hmm. Which, uh, you know, is a pretty low frequency kind of thing. But - but, uh - Mm-hmm. So, those are - those are major components, I think, I see. uh, for the stationary kind of stuff. Mmm. Um, but, um, it, uh - I guess, I - maybe I said this last week too but it - it - it really became apparent to us that we need to - to take account of noise. And, uh, so I think when - when he gets done with his prelim study I think one of the next things we'd want to do is to take this, uh - uh, noise, uh, processing stuff and - and, uh - uh, synthesize some speech from it. And then - When are his prelims? Um, I think in about, um, a little less than two weeks. Oh. Wow. Yeah. Yeah. So. Uh, it might even be sooner. Uh, let's see, this is the sixteenth, seventeenth? Yeah, I don't know if he's before - It might even be in a week. A week, week and a half. So, I- Huh. I - I guessed that they were gonna do it some time during the semester but they'll do it any time, huh? They seem to be - Well, the semester actually is starting up. Is it already? Yeah, the semester's late - late August they start here. Yikes. So they do it right at the beginning of the semester. Yeah. Yeah. So, uh - Yep. I mean, that - that was sort of one - I mean, the overall results seemed to be first place in - in - in the case of either, um, artificial reverberation or a modest sized training set. Uh, either way, uh, i- uh, it helped a lot. And - But if you had a - a really big training set, a recognizer, uh, system that was capable of taking advantage of a really large training set - I thought that - One thing with the H_T_K is that is has the - as we're using - the configuration we're using is w- s- is - being bound by the terms of Aurora, we have all those parameters just set as they are. So even if we had a hundred times as much data, we wouldn't go out to, you know, ten or t- or a hundred times as many Gaussians or anything. So, um, it's kind of hard to take advantage of - of - of big chunks of data. Mmm, yeah. Mm-hmm. Uh, whereas the other one does sort of expand as you have more training data. It does it automatically, actually. And so, um, uh, that one really benefited from the larger set. And it was also a diverse set with different noises and so forth. Uh, so, um, that, uh - that seemed to be - So, if you have that - that better recognizer that can - that can build up more parameters, and if you, um, have the natural room, which in this case has a p- a pretty bad signal-to-noise ratio, then in that case, um, the right thing to do is just do - u- use speaker adaptation. And - and not bother with - with this acoustic, uh, processing. But I think that that would not be true if we did some explicit noise-processing as well as, uh, Mm-hmm. the convolutional kind of things we were doing. So. That's sort of what we found. Hmm. I, um - uh, started working on the uh - Mississippi State recognizer. Oh, O_K. So, I got in touch with Joe and - and, uh, from your email and things like that. And, uh, they added me to the list - uh, the mailing list. And he gave me all of the O_K, great. pointers and everything that I needed. And so I downloaded the, um - There were two things, uh, that they had to download. One was the, uh, I guess the software. And another wad - was a, um, sort of like a sample - a sample run. So I downloaded the software and compiled all of that. And it Eight. compiled fine. No problems. Oh, eh, great. And, um, I grabbed the sample stuff but I haven't, uh, That sample was released only yesterday or the day before, right? compiled it. No - Well, I haven't grabbed that one yet. So there's two. Oh, there is another short sample set - o- o- sample. O_K. There was another short one, yeah. And so I haven't grabbed the latest one that he just, uh, put out yet. Oh, O_K. F- Yeah, O_K. So. Um, but, the software seemed to compile fine and everything, so. And, um, So. Is there any word yet about the issues about, um, adjustments for different feature sets or anything? No, I - I d- You asked me to write to him and I think I forgot to ask him about that. Yeah. Or if I did ask him, he didn't reply. I - I don't remember yet . Uh, I'll - I'll d- I'll double check that and ask him again. Yeah. Yeah, it's like that - that could r- turn out to be an important issue for us. Yeah. Hmm. Mmm. Yeah. Yeah. Cuz they have it - Maybe I'll send it to the list. Yeah. Cuz they have, uh, already frozen those in i- insertion penalties and all those stuff is what - I feel. Because they have this document Uh-huh. explaining the recognizer. And they have these tables with, uh, various language model weights, insertion penalties. u- O_K, I haven't seen that one yet. Uh, it's th- it's there on that web. And, uh, on that, I mean, they have run some experiments using various So. O_K. insertion penalties and all those - And so they've picked - Yeah, I think they pi- p- yeah, they picked the values from - the values. Oh, O_K. O_K. For r- w- what test set? Uh, p- the one that they have reported is a NIST evaluation, Wall Street Journal. But that has nothing to do with what we're testing on, right? You know. No. So they're, like - um - Mm-hmm. So they are actually trying to, uh, fix that - those values using the clean, uh, training part of the Wall Street Journal. Which is - I mean, the Aurora. Aurora has a clean subset. I mean, they want to train it and then this - they're going to run some evaluations. Right. So they're set- they're setting it based on that? Yeah. O_K. So now, we may come back to the situation where we may be looking for a modification of the features to account for the fact that we can't modify these parameters. But, um, Yeah. Yeah. uh - but it's still worth, I think, just - since - you know, just chatting with Joe about the issue. Yeah, O_K. Do you think that's something I should just send to him or do you think I should send it to this - there's an - a m- a mailing list. Um - Well, it's not a secret. I mean, we're, you know, certainly willing to talk about it with everybody, but I think - I think that, um - um, it's probably best to start talking with him just to - O_K. Uh @@ you know, it's a dialogue between two of you about what - you know, what does he think about this and what - what - you know - what could be done about it. Um, Yeah. O_K. if you get ten people in - involved in it there'll be a lot of perspectives based on, you know, how - Yeah. you know. Uh - But, I mean, I think it all should come up eventually, but if - if - Right. O_K. if there is any, uh, uh, way to move in - a way that would - that would, you know, be more open to different kinds of features. But if - if, uh - if there isn't, and it's just kind of shut down and - and then also there's probably not worthwhile bringing it into a larger forum where - where political issues will come in. Yeah. O_K. Oh. @@ So this is now - it's - it's compiled under Solaris? Yeah. Yeah, O_K. Because he - there was some mail r- saying that it's - may not be stable for Linux and all those. Yep. Yeah. Yeah, i- that was a particular version. SUSI yeah. Yeah, SUSI or whatever it was but we don't have that. So. Yeah, yeah. Yeah, O_K. O_K, that's fine. Yeah. Should be O_K. Yeah, it compiled fine actually. No - no errors. Nothing. So. That's good. Uh, this is slightly off topic but, uh, I noticed, just glancing at the, uh, Hopkins workshop, uh, web site that, uh, um - one of the thing- I don't know - Well, we'll see how much they accomplish, but one of the things that they were trying to do in the graphical models thing was to put together a - a, uh, tool kit for doing, uh r- um, arbitrary graphical models for, uh, speech recognition. Hmm. So - And Jeff, uh - the two Jeffs were Who's the second Jeff? Uh - Oh, uh, do you know Geoff Zweig? No. Oh. Uh, he - he, uh - he was here for a couple years and he, uh - got his P_H_D. He - Oh, O_K. And he's, uh, been at I_B_M for the last couple years. Oh, O_K. So. Uh, so he did - he did his P_H_D on dynamic Bayes-nets, uh, Wow. That would be neat. for - for speech recognition. He had some continuity built into the model, presumably to handle some, um, inertia in the - in the production system, and, um - Hmm. So. Hmm. Um, I've been playing with, first, the, um, V_A_D. Um, so it's exactly the same approach, but the features that the V_A_D neural network use are, uh, M_F_C_C after noise compensation. Oh, I think I have the results. What was it using before? Before it was just @@ P_L_Ps. So. Yeah, it was actually - No. Not - I mean, it was just the noisy features I guess. Yeah, yeah, yeah, not compensated . Yeah, noisy - noisy features. Um - This is what we get after - This - So, actually, we, yeah, here the features are noise compensated and there is also the L_D_A filter. Um, and then it's a pretty small neural network which use, um, nine frames of - of six features from C_zero to C_fives, plus the first derivatives. And it has one hundred hidden units. Is that nine frames u- s- uh, centered around the current frame? Or - Yeah. Mm-hmm. S- so, I'm - I'm sorry, there's - there's - there's how many - how many inputs? So it's twelve times nine. Twelve times nine inputs, and a hundred, uh, hidden. Hidden and Two outputs. two outputs. Two outputs. O_K. So I guess about eleven thousand parameters, Mm-hmm. which - actually shouldn't be a problem, even in - in small phones. Yeah. So, I'm - I'm - s- so what is different between this and - It should be O_K. So the previous syst- It's based on the system that has a fifty-three point sixty-six percent improvement. and what you - It's the same system. The only thing that changed is the n- a p- eh - a es- the estimation of the silence probabilities. Ah. O_K. Which now is based on, uh, cleaned features. And, it's a l- it's a lot better. Wow. Yeah. Um - That's great. So it's - it's not bad, but the problem is still that the latency is too large. What's the latency? Because - um - the - the latency of the V_A_D is two hundred and twenty milliseconds. And, uh, the V_A_D is used uh, i- for on-line normalization, and it's used before the delta computation. So if you add these components it goes t- to a hundred and seventy, right? I - I'm confused. You started off with two-twenty and you ended up with one-seventy? With two an- two hundred and seventy. If - Yeah, if you add the c- delta comp- delta computation which is done afterwards. Two-seventy. Oh. Um - So it's two-twenty. I- the- is this - are these twenty-millisecond frames? Is that why? Is it after downsampling? or - The two-twenty is one hundred milliseconds for the um - No, it's forty milliseconds for t- for the, uh, uh, cleaning of the speech. Um - then there is, um, the neural network which use nine frames. So it adds forty milliseconds. a- O_K. Um, after that, um, you have the um, filtering of the silence probabilities. Which is a million filter it , and it creates a one hundred milliseconds delay. So, um - @@ Plus there is a delta at the input. Yeah, and there is the delta at the input which is, One hundred um - milliseconds for smoothing. So it's - Uh, median. @@ - It's like forty plus - forty - plus - Mmm. Forty - And then forty - This forty plus twenty, plus one hundred. forty p- @@ So it's two hundred actually. Uh - Yeah, there are twenty that comes from - There is ten that comes from the L_D_A filters also. Right? Oh, O_K. Uh, so it's two hundred and ten, yeah. If you are using - Uh - Plus the frame, so it's two-twenty. t- If you are using three frames - If you are phrasing f- using three frames, it is thirty here for delta. Yeah, I think it's - it's five frames, but. So five frames, that's twenty. O_K, so it's who un- two hundred and ten. Uh, p- Wait a minute. It's forty - forty for the - for the cleaning of the speech, forty for the I_N_ - A_N_N, a hundred for the smoothing. So. Forty cleaning. Yeah. Well, but at ten - , Twenty for the delta. At th- At the input. I mean, that's at the input to the net. Twenty for delta. Yeah. And there i- Delta at input to net? Yeah. Yeah. So it's like s- five, six cepstrum plus delta at nine - nine frames of - And then ten milliseconds for - Fi- There's an L_D_A filter. ten milliseconds for L_D_A filter, and t- and ten - another ten milliseconds you said for the frame? For the frame I guess. I computed two-twenty - Yeah, well, it's - I guess it's for the fr - the - O_K. And then there's delta besides that? So this is the features that are used by our network and then afterwards, you have to compute the delta on the, uh, main feature stream, which is O_K. um, delta and double-deltas, which is fifty milliseconds. Yeah. No, I mean, the - after the noise part, the forty - the - the other hundred and eighty - Well, I mean, hhh, Wait a minute. Some of this is, uh - is, uh - is in parallel, isn't it? I mean, the L_D_A - Oh, you have the L_D_A as part of the V_D_- uh, V_A_D? Or - The V_A_D use, uh, L_D_A filtered features also. Oh, it does? Mm-hmm. Ah. So in that case there isn't too much in parallel. Uh - No. There is, um, just downsampling, upsampling, and the L_D_A. Um, so the delta at the end is how much? It's - It's fifty. Fifty. Alright. So - But well, we could probably put the delta, um, before on-line normalization. It should not that make a big difference, because - What if you used a smaller window for the delta? Could that help a little bit? I mean, I guess there's a lot of things you could do to - Yeah. Yeah. Yeah, but, nnn - So- Yeah. So if you - if you put the delta before the, uh, ana- on-line - If - Yeah - uh - then - then it could go in parallel. And then y- then you don't have that additive - Mm-hmm. Cuz i- Yep. Yeah, cuz the time constant of the on-line normalization is pretty long compared to the O_K. delta window, so. It should not make - O_K. And you ought to be able to shove tw- , uh - sh- uh - pull off twenty milliseconds from somewhere else to get it under two hundred, right? I mean - Mm-hmm. Is two hundred the d- The hundred milla- mill- a hundred milliseconds for smoothing is sort of an arbitrary amount. It could be eighty and - and probably do @@ - Yeah, yeah. i- a hun- uh - Wh- what's the baseline you need to be under? Well, we don't know. They're still arguing about it. I mean, if it's two - if - if it's, uh - Two hundred? @@ Oh. if it's two-fifty, then we could keep the delta where it is if we shaved off twenty. If it's two hundred, if we shaved off twenty, we could - we could, uh, meet it by moving the delta back. So, how do you know that what you have is too much if they're still deciding? Uh, we don't, but it's just - I mean, the main thing is that since that we got burned last time, and - you know, by not worrying about it very much, we're just staying conscious of it. Uh-huh. Oh, O_K, I see. And so, th- I mean, if - if - if a week before we have to be done someone says, "Well, you have to have fifty milliseconds less than you have now", it would be pretty frantic around here. So - Ah, O_K. Uh - But still, that's - that's a pretty big, uh, win. And it doesn't seem like you're - in terms of your delay, you're, uh, that - He added a bit on, I guess, because before we were - we were - had - were able to have the noise, Hmm. uh, stuff, uh, and the L_V_A be in parallel. And now he's - he's requiring it to be done first. Well, but- I think the main thing, maybe, is the cleaning of the speech, which takes forty milliseconds or so. And - Right. Well, so you say - let's say ten milliseconds - seconds for the L_D_A. and - but - the L_D_A is, well, pretty short right now. Yeah. Well, ten. And then forty for the other. Yeah, the L_D_A - L_D_A - we don't know, is, like - is it very crucial for the features, right? No. I just - This is the first try. I mean, I - maybe the L_D_A's not very useful then. Yeah. S- s- h- Right, so you could start pulling back, but - Yeah, l- But I think you have - I mean, you have twenty for delta computation which y- now you're sort of doing twice, right? But yo- w- were you doing that before? Mmm. On the - in the - Mm-hmm. Well, in the proposal, um, the input of the V_A_D network were Just - just three frames, I think. Yeah, just the static, no delta. Uh, static features. Right. So, what you have now is fort- uh, forty for the - the noise, twenty for the delta, and ten for the L_D_A. That's seventy milliseconds @@ of stuff which was formerly in parallel, right? So I think, Mm-hmm. you know, that's - that's the difference as far as the timing, right? Yeah. Um, and you could experiment with cutting various pieces of these back a bit, but - I mean, we're s- we're not - we're not in terrible shape. Yeah, that's what it seems like to me. It's pretty good. Mm-hmm. Yeah. It's - it's not like it's adding up to four hundred milliseconds or something. Where - where is this - where is this fifty-seven point O_ two in - in comparison to the last evaluation? Well, it's - I think it's better than anything, uh, anybody got. Yeah. Oh, is that right? The best was fifty-four Yeah. point five. Point s- Oh. Yeah. Uh- And our system was forty-nine, but with the neural network. Wow. So this is almost ten percent. With the f- with the neural net. Yeah, and r- and - Yeah, so this is - this is like the first proposal. The proposal- one. It was forty-four, actually. It would- Yeah. Yeah. And we still don't have the neural net in. So - so it's - You know. So it's - Wow. We're - we're doing better. I mean, we're getting This is - this is really good. better recognition. I mean, I'm sure other people working on this are not sitting still either, but - but - Yeah. but, uh - Uh, I mean, the important thing is that we learn how to do this better, and, you know. So. Um, Yeah. So, our, um - Yeah, you can see the kind of - kind of numbers that we're having, say, on SpeechDat-Car which is a hard task, cuz it's really, um - I think it's just sort of - sort of reasonable numbers, starting to be. Mm-hmm. I mean, it's still terri- Yeah, even for a well-matched case it's sixty percent error rate reduction, which is - Yeah. Yeah. Probably half. Good! Um, Yeah. So actually, this is in between what we had with the previous V_A_D and what Sunil did with an I_D_L V_A_D. Which gave sixty-two percent improvement, right? Yeah, it's almost that. It's almost an average somewhere around - Yeah. So - Yeah. What was that? Say that last part again? So, if you use, like, an I_D_L V_A_D, o- o- uh, for dropping the frames, Or the best we can get. the best that we can get - i- That means that we estimate the silence probability on the clean version of the utterances. Then you can go up to sixty-two percent error rate reduction, globally. Mmm. Mmm - Yeah. So that would be even - That wouldn't change this number down here to sixty-two? Yeah. Yeah. So you - you were get- If you add a g- good v- very good V_A_D, Yeah. that works as well as a V_A_D working on clean speech, Yeah. then you wou- you would go - So that's sort of the best you could hope for. Mm-hmm. I see. Probably. Yeah. So fi- si- fifty-three is what you were getting with the old V_A_D. Yeah. And, uh - and sixty-two with the - the, you know, quote, unquote, cheating V_A_D. And fifty-seven is what you got with the real V_A_D. Mm-hmm. That's great. Uh, yeah, the next thing is, I started to play - Well, I don't want to worry too much about the delay, no. Maybe it's better to wait O_K. for the decision Yeah. from the committee. Uh, but I started to play with the, um, uh, tandem neural network. Mmm I just did the configuration that's very similar to what we did for the February proposal. And - Um. So. There is a f- a first feature stream that use uh straight M_F_C_C features. Mm-hmm. Well, these features actually. And the other stream is the output of a neural network, using as input, also, these, um, cleaned M_F_C_C. Um - I don't have the comp- Mmm? Those are th- those are th- what is going into the tandem net? Those two? So there is just this feature stream, the fifteen M_F_C_C plus delta and double-delta. No. Yeah? Um, so it's - makes forty-five features that are used as input to the H_T_K. And then, there is - there are more inputs that comes from the tandem M_L_P. Oh, oh. O_K. I see. Yeah, h- he likes to use them both, cuz then it has one part that's discriminative, one part that's not. Uh- huh. Yeah. Um - Right. O_K. So, um, uh, yeah. Right now it seems that - i- I just tested on SpeechDat-Car while the experiment are running on your - on T_I-digits. Well, it improves on the well-matched and the mismatched conditions, but it get worse on the highly mismatched. Um, Compared to these numbers? Compared to these numbers, yeah. Um, like, on the well-match and medium mismatch, the gain is around five percent relative, y- but it goes down a lot more, like fifteen percent on the H_M case. You're just using the full ninety features? @@ The - Y- you have ninety features? i- I have, um - From the networks, it's twenty-eight. So - And from the other side it's forty-five. So it's - you have seventy-three features, So, d- i- It's forty-five. Yeah. Yeah. and you're just feeding them like that. Mm-hmm. There isn't any K_L_T or anything? There's a K_L_T after the neural network, as - as before. That's how you get down to twenty-eight? Yeah. Why twenty-eight? I don't know. Uh. It's - Oh. i- i- i- It's because it's what we did for the first proposal. We tested, Ah. uh, trying to go down and Yeah. It's a multiple of seven. Yeah. Yeah. Yeah. So - Um. I wanted to do something very similar to the proposal as a first - I see. Yeah. Yeah. That makes sense. first try. But we have to - for sure, we have to go down, because the limit is now sixty features. So, Yeah. uh, we have to find a way to decrease the number of features. Um - So, it seems funny that - I don't know, maybe I don't u- quite understand everything, but that adding features - I guess - I guess if you're keeping the back-end fixed. Maybe that's it. Because it seems like just adding information shouldn't give worse results. But I guess if you're keeping the number of Gaussians fixed in the recognizer, then - Well, yeah. But, I mean, just in general, adding information - Mmm. Suppose the information you added, well , was a really terrible feature and all it brought in was noise. Yeah. Right? So - so, um - Or - or suppose it wasn't completely terrible, but it was completely equivalent to another one feature that you had, except it was noisier. Uh-huh. Right? In that case you wouldn't necessarily expect it to be better at all. Oh, yeah, I wasn't necessarily saying it should be better. I'm just surprised that you're getting fifteen percent relative worse Uh-huh. But it's worse. on the wel- On the highly mismatch. Yeah. On the highly mismatched condition. Yeah, I - So, "highly mismatched condition" means that in fact your training is a bad estimate of your test. Uh-huh. So having - having, uh, a g- a l- a greater number of features, if they aren't maybe the right features that you use, certainly can e- can easily, uh, make things worse. I mean, you're right. If you have - if you have, uh, lots and lots of data, and you have - and your - your - your training is representative of your test, then getting more sources of information should just help. But - but it's - It doesn't necessarily work that way. Huh. Mm-hmm. So I wonder, um, Well, what's your - what's your thought about what to do next with it? Um, I don't know. I'm surprised, because I expected the neural net to help more when there is more mismatch, as it was the case for the - So, was the training set same as the p- the February proposal? Mm-hmm. @@ Yeah, it's the same training set, so it's TIMIT with O_K. the T_I-digits', uh, noises, Mm-hmm. uh, added. Um - Well, we might - uh, we might have to experiment with, uh better training sets. Again. But, I - The other thing is, I mean, before you found that was the best configuration, but you might have to retest those things now that we have different - Mm-hmm. The rest of it is different, right? So, um, uh, For instance, what's the effect of just putting the neural net on without the o- other - other path? Mm-hmm. Yeah. I mean, you know what the straight features do. That gives you this. Mm-hmm. You know what it does in combination. You don't necessarily know what - What if you did the - Would it make sense to do the K_L_T on the full set of combined features? Instead of just on the - Yeah. I g- I guess. Um. The reason I did it this ways is that in February, it - we - we tested different things like that, so, having two K_L_T, having just a K_L_T for a network, or having a global K_L_T. Oh, I see. And - So you tried the global K_L_T before and it didn't really - Well - Yeah. And, uh, th- Yeah. The differences between these configurations were not huge, but - I see. it was marginally better with this configuration. Uh-huh. Uh-huh. But, yeah, that's obviously another thing to try, Um. since things are - things are different. And I guess if the - Mm-hmm. Mm-hmm. These are all - so all of these seventy-three features are going into, um, the, uh - the H_M_M. Yeah. And is - are - i- i- are - are any deltas being computed of tha- of them? Of the straight features, yeah. So. n- Not of the - But n- th- the, um, tandem features are u- Are not. used as they are. So, yeah, maybe we can add some context from these features also as - Could. i- Dan did in - in his last work. Yeah, but the other thing I was thinking was, um - Uh, now I lost track of what I was thinking. But. What is the - You said there was a limit of sixty features or something? Mm-hmm. What's the relation between that limit and the, um, forty-eight - Oh, I know what I was gonna say. uh, forty eight hundred bits per second? Um, not - no relation. The f- the forty-eight No relation. So I - I - I don't understand, because i- hundred bits is for transmission of some features. I mean, if you're only using h- And generally, i- it - s- allows you to transmit like, fifteen, uh, cepstrum. The issue was that, um, this is supposed to be a standard that's then gonna be fed to somebody's recognizer somewhere which might be, you know, it - it might be a concern how many parameters are use - u- used and so forth. And so, uh, they felt they wanted to set a limit. So they chose sixty. Some people wanted to use hundreds of parameters and - and that bothered some other people. u- And so Uh-huh. they just chose that. I - I - I think it's kind of r- arbitrary too. But - but that's - that's kind of what was chosen. I - I remembered what I was going to say. What I was going to say is that, um, maybe - maybe with the noise removal, uh, these things are now more correlated. So you have two sets of things that are kind of uncorrelated, uh, within themselves, but they're pretty correlated with one another. Mm-hmm. And, um, they're being fed into these, uh, variants, only Gaussians and so forth, and - and, uh, Mm-hmm. so maybe it would be a better idea now than it was before to, uh, have, uh, one K_L_T over everything, Mm-hmm. to de-correlate it. Yeah, I see. Maybe. You know . What are the S_N_Rs in the training set, TIMIT? It's, uh, ranging from zero to clean? Mm-hmm. Yeah. From zero to clean. Yeah. So we found this - this, uh - this Macrophone data, and so forth, that we were using for these other experiments, to be pretty good. So that's - i- after you explore these other alternatives, that might be another way to start looking, is - is just improving the training set. Mm-hmm. Mm-hmm. I mean, we were getting, uh, lots better recognition using that, than - Of course, you do have the problem that, um, u- i- we are not able to increase the number of Gaussians, uh, or anything to, uh, uh, to match anything. So we're only improving the training of our feature set, but that's still probably something. So you're saying, add the Macrophone data to the training of the neural net? The tandem net? Yeah, that's the only place that we can train. We can't train the other stuff with anything other than the standard amount, so. Yeah. Right. Um, um - What - what was it trained on again? The one that you used? It's TIMIT with noise. Uh-huh. So, yeah, it's rather a small - Yeah. How big is the net, by the way? Um, Uh, it's, uh, five hundred hidden units. And - And again, you did experiments back then where you made it bigger and it - and that was - that was sort of the threshold point. Much less than that, it was worse, and Yeah. Yeah. much more than that, it wasn't much better. Hmm. So is it - is it though the performance, Yeah. @@ ? big relation in the high ma- high mismatch has something to do with the, uh, cleaning up that you - that is done on the TIMIT after adding noise? So - it's - i- All the noises are from the T_I-digits, right? Yeah. So you - i- Um - Well, it- it's like the high mismatch of the SpeechDat-Car They - k- uh - after cleaning up, maybe having more noise than the - the training set of TIMIT after clean - s- after you do the noise clean-up. Mmm. I mean, earlier you never had any compensation, you just trained it straight away. Mm-hmm. So it had like all these different conditions of S_N_Rs, Mm-hmm. actually in their training set of neural net. Mm-hmm. But after cleaning up you have now a different set of S_N_Rs, right? For the training of the neural net. Yeah. Mm-hmm. And - is it something to do with the mismatch that - that's created after the cleaning up, like the high mismatch - You mean the - the most noisy occurrences on SpeechDat-Car might be Mm-hmm. a lot more noisy than - Of - that - I mean, the S_N_R after the noise compensation of the SpeechDat-Car. Oh, so - Right. So the training - the - the neural net is being trained with noise compensated Maybe. @@ Yeah. Yeah, yeah. stuff. Which makes sense, Yeah. but, uh, you're saying - Yeah, the noisier ones are still going to be, Yeah. even after our noise compensation, are still gonna be pretty noisy. Mm-hmm. Yeah, so now the after-noise compensation the neural net is seeing a different set of S_N_Rs than that was originally there in the training set. Of TIMIT. Because in the TIMIT it was zero to some clean. Right. So the net saw all the S_N_R @@ Yes. conditions. Now after cleaning up it's a different set of S_N_R. Right. Right. And that S_N_R may not be, like, com- covering the whole set of S_N_Rs that you're getting in the SpeechDat-Car. Right, but the SpeechDat-Car data that you're seeing is also reduced in noise by the noise compensation. Yeah, yeah, yeah, yeah, it is. But, I'm saying, there could be some - Yeah. So. Mm-hmm. some issues of - Yeah. Well, if the initial range of S_N_R is different, we - the problem was already there before. And - Yeah. Because - Mmm - Yeah, I mean, it depends on whether you believe that the noise compensation is equally reducing the noise on the test set and the training set. Uh - Hmm. On the test set, yeah. @@ Right? I mean, you're saying there's a mismatch in noise Hmm. Mm-hmm. that wasn't there before, but if they were both the same before, then if they were both reduic- reduced equally, Mm-hmm. then, there would not be a mismatch. So, I mean, this may be - Heaven forbid, this noise compensation process may be imperfect, but. Uh, so maybe it's treating some things differently. Well, I - Yeah, uh - I don't know. I - I just - that could be seen from the T_I-digits, uh, testing condition because, um, the noises are from the T_I-digits, right? Noise - Yeah. So - So cleaning up the T_I-digits and if the performance goes down in the T_I-digits mismatch - high mismatch like this - Clean training, yeah. on a clean training, or zero D_B testing. Yeah, we'll - so we'll see. Uh. Maybe. Yeah. Then it's something to do. Mm-hmm. Yeah. I mean, one of the things about - I mean, the Macrophone data, um, I think, you know, it was recorded over many different telephones. Mm-hmm. And, um, so, there's lots of different kinds of acoustic conditions. I mean, it's not artificially added noise or anything. So it's not the same. I don't think there's anybody recording over a car from a car, but - I think it's - it's varied enough that if - if doing this adjustments, uh, and playing around with it doesn't, uh, make it better, the most - uh, it seems like the most obvious thing to do is to improve the training set. Um - I mean, what we were - uh - the condition - It - it gave us an enormous amount of improvement in what we were doing with Meeting Recorder digits, even though there, again, these m- Macrophone digits were very, very different from, uh, what we were going on here. I mean, we weren't talking over a telephone here. But it was just - I think just having a - a nice variation in acoustic conditions was just a good thing. Mm-hmm. Mmm. Yep. Yeah, actually to s- eh, what I observed in the H_M case is that the number of deletion dramatically increases. It - it doubles. Number of deletions. When I added the num- the neural network it doubles the number of deletions. Yeah, so I don't you know how to interpret that, but, mmm - Yeah. Me either. t- And - and did - an- other numbers stay the same? Insertion substitutions stay the same? They p- stayed the same, they - maybe they are a little bit Roughly? uh, lower. Uh-huh. They are a little bit better. Yeah. But - Mm-hmm. Did they increase the number of deletions even for the cases that got better? Say, for the - I mean, it - So it's only the highly mismatched? No, it doesn't. No. And it - Remind me again, the "highly mismatched" means that the - Clean training and - Uh, sorry? It's clean training - Well, close microphone training and Close mike training - distant microphone, um, high speed, I think. Well - The most noisy cases are the distant microphone for testing. Right. So - Well, maybe the noise subtraction is subtracting off speech. Wh- Separating . Yeah. But - Yeah. I mean, but without the neural network it's - well, it's better. It's just when we add the neural networks. The feature are the same except that - Yeah, right. Uh, that's right, that's right. Um - Well that - that says that, you know, the, um - the models in - in, uh, the recognizer are really paying attention to the neural net features. Yeah. Mm-hmm. Uh. But, yeah, actually - the TIMIT noises are sort of a range of noises and they're not so much the stationary driving kind of noises, right? It's - it's pretty different. Isn't it? Uh, there is a car noise. So there are f- just four noises. Um, uh, "Car", I think, "Babble." "Babble", "Subway", right? and - "Street" or "Airport" or something. and - "Street" isn't - " Train station", yeah. Or "Train station". Yeah. So - it's mostly - Well, "Car" is stationary, Mm-hmm. "Babble", it's a stationary background plus some voices, Mm-hmm. some speech over it. And the other two are rather stationary also. Well, I - I think that if you run it - Actually, you - maybe you remember this. When you - in - in the old experiments when you ran with the neural net only, and didn't have this side path, um, uh, with the - the pure features as well, Mm-hmm. did it make things better to have the neural net? Was it about the same? Uh, w- i- It was - b- a little bit worse. Than - ? Than just the features, yeah. So, until you put the second path in with the pure features, the neural net wasn't helping at all. Mm-hmm. Well, that's interesting. It was helping, uh, if the features are b- were bad, I mean. Yeah. Just plain P_L_Ps or M_F_C_Cs. Yeah. But as soon as we added L_D_A on-line normalization, and all these things, then - They were doing similar enough things. Well, I still think it would be k- sort of interesting to see what would happen if you just had the neural net without the side thing. And - and the thing I - I have in mind is, Yeah, mm-hmm. uh, maybe you'll see that the results are not just a little bit worse. Maybe that they're a lot worse. You know? And, um - But if on the ha- other hand, uh, it's, say, somewhere in between what you're seeing now and - and - and, uh, what you'd have with just the pure features, then maybe there is some problem of a - of a, uh, combination of these things, or correlation between them somehow. Mm-hmm. If it really is that the net is hurting you at the moment, then I think the issue is to focus on - on, uh, improving the - the net. Yeah, mm-hmm. Um. So what's the overall effe- I mean, you haven't done all the experiments but you said it was i- somewhat better, say, five percent better, for the first two conditions, and fifteen percent worse for the other one? But it's - but of course that one's weighted lower, so I wonder what the net effect is. Y- yeah, oh. Yeah. I d- I - I think it's - it was one or two percent. That's not that bad, but it was l- like two percent relative worse on SpeechDat-Car. I have to - to check that. Well, I have - I will. Well, it will - overall it will be still better even if it is fifteen percent worse, because the fifteen percent worse is given like f- w- twenty-five - Right. point two five eight. Mm-hmm. Hmm. Right. So the - so the worst it could be, if the others were exactly the same, is four, Is it like - Yeah, so it's four. and - and, uh, in fact since the others are somewhat better - Is i- So either it'll get cancelled out, or you'll get, like, almost the same. Yeah, it was - it was slightly worse. Um, Uh. Slightly bad. Yeah. Yeah, it should be pretty close to cancelled out. Yeah. Mm-hmm. You know, I've been wondering about something. In the, um - a lot of the, um - the Hub-five systems, um, recently have been using L_D_A. and - and they, um - They run L_D_A on the features right before they train the models. So there's the - the L_D_A is - is right there before the H_M_Ms. Yeah. So, you guys are using L_D_A but it seems like it's pretty far back in the process. Uh, this L_D_A is different from the L_D_A that you are talking about. The L_D_A that you - saying is, like, you take a block of features, like nine frames or something, Yeah. Uh-huh. and then do an L_D_A on it, and then reduce the dimensionality to something like twenty-four or something like that. And then feed it to H_M_M. Yeah, you c- you c- you can. I mean, it's - you know, you're just basically i- Yeah, so this is like a two d- two dimensional tile . You're shifting the feature space. Yeah. So this is a two dimensional tile . And the L_D_A that we are f- applying is only in time, not in frequency - high cost frequency. So it's like - more like a filtering in time, rather than Ah. O_K. doing a r- So what i- what about, um - i- u- what i- w- I mean, I don't know if this is a good idea or not, but what if you put - ran the other kind of L_D_A, uh, on your features right before they go into Uh, it - the H_M_M? m- Mm-hmm. No, actually, I think - i- Well. What do we do with the A_N_N is - is something like that except that it's not linear. But Yeah. it's - it's like a nonlinear discriminant analysis. But. Right, it's the - It's - Right. The - So - Yeah, so it's sort of like - The tandem stuff is kind of like i- Yeah. It's - nonlinear L_D_A. I g- Yeah. Yeah. Yeah. Uh. But I mean, w- but the other features that you have, um, th- the non- tandem ones, Mm-hmm. Yeah, I know. That - that - Yeah. Well, in the proposal, they were transformed u- using P_C_A, but - Uh-huh. Yeah, it might be that L_D_A The a- the argument i- is kind of i- in - and it's not like we really know, but the argument anyway is that, um, could be better. uh, we always have the prob- I mean, discriminative things are good. L_D_A, neural nets, they're good. Yeah. Uh, they're good because you - you - you learn to distinguish between these categories that you want to be good at distinguishing between. And P_C_A doesn't do that. It - P_A_C- P_C_A - low-order P_C_A throws away pieces that are uh, maybe not - not gonna be helpful just because they're small, basically. Right. But, uh, the problem is, training sets aren't perfect and testing sets are different. So you f- you - you face the potential problem with discriminative stuff, be it L_D_A or neural nets, that you are training to discriminate between categories in one space but what you're really gonna be g- getting is - is something else. Uh-huh. And so, uh, Stephane's idea was, uh, let's feed, uh, both this discriminatively trained thing and something that's not. So you have a good set of features that everybody's worked really hard to make, Yeah. and then, uh, you - you discriminately train it, but you also take the path that - that doesn't have that, and putting those in together. Uh-huh. And that - that seem- So it's kind of like a combination of the - uh, what, uh, Dan has been calling, you know, a feature - uh, you know, a feature combination versus posterior combination or something. It's - it's, you know, you have the posterior combination but then you get the features from that and use them as a feature combination with these - these other things. And that seemed, at least in the last one, as he was just saying, he - he - when he only did discriminative stuff, Yeah. i- it actually was - was - it didn't help at all in this particular case. There was enough of a difference, I guess, between the testing and training. But by having them both there - The fact is some of the time, the discriminative stuff is gonna help you. Mm-hmm. And some of the time it's going to hurt you, and by combining two information sources if, you know - if - if - Right. So you wouldn't necessarily then want to do L_D_A on the non-tandem features because That i- i- now you're doing something to them that - I think that's counter to that idea. Now, again, it's - we're just trying these different things. We don't really know what's gonna work best. But Yeah, right. if that's the hypothesis, at least it would be counter to that hypothesis to do that. Right. Um, and in principle you would think that the neural net would do better at the discriminant part than L_D_A. Right. Yeah. Well - y- Though, maybe not. Yeah. Exactly. I mean, we, uh - we were getting ready to do the tandem, uh, stuff for the Hub- five system, and, um, Andreas and I talked about it, and the idea w- the thought was, "Well, uh, yeah, that i- you know - th- the neural net should be better, but we should at least have uh, a number, you know, to show that we did try the L_D_A in place of the neural net, so that we can Right. you know, show a clear path. You know, that you have it without it, then you have the L_D_A, then you have the neural net, and you can see, theoretically. So. I was just wondering - I - I - Well, I think that's a good idea. Yeah. Did - did you do that or - tha- that's a - Um. No. That's what - that's what we're gonna do next as soon as I finish this other thing. So. Yeah. Yeah. No, well, that's a good idea. I - I - i- Yeah. We just want to show. I mean, it - everybody believes it, but you know, we just - Oh, no it's a g- No, no, but it might not - not even be true. I mean, it's - it's - it's - it's - it's a great idea. I mean, Yeah. one of the things that always disturbed me, uh, in the - the resurgence of neural nets that happened in the eighties was that, um, a lot of people - Because neural nets were pretty easy to - to use - Yeah. a lot of people were just using them for all sorts of things without, uh, looking at all into the linear, uh - uh, versions of them. And, Mm-hmm. Yeah. uh, people were doing recurrent nets but not looking at I_I_R filters, and - You know, I mean, uh, so I think, yeah, it's definitely a good idea to try it. Yeah, and everybody's putting that on their systems now, and so, I- that's what made me wonder about Well, they've been putting them in their systems off and on for ten years, but - but - but, uh, this, but. Yeah, what I mean is it's - it's like in the Hub-five evaluations, you know, and you read the system descriptions and And now they all have that. everybody's got, you know, L_D_A on their features. And so. Uh. I see. Yeah. It's the transformation they're estimating on - Well, they are trained on the same data as the final H_M_M are. Yeah, so it's different. Yeah, exactly. Cuz they don't have these, you know, mismatches that - that you guys have. So that's why I was wondering if maybe it's not even a good idea. I don't know. Mm-hmm. Mm-hmm. I - I don't know enough about it, but - Um. Mm-hmm. I mean, part of why - I - I think part of why you were getting into the K_L_T - Y- you were describing to me at one point that you wanted to see if, uh, you know, getting good orthogonal features was - and combining the - the different temporal ranges - was the key thing that was happening or whether it was this discriminant thing, right? So you were just trying - I think you r- I mean, this is - it doesn't have the L_D_A aspect but th- as far as the orthogonalizing transformation, you were trying that at one point, right? Mm-hmm. Mm-hmm. I think you were. Yeah. Does something. It doesn't work as well. Yeah. Yeah. So, yeah, I've been exploring a parallel V_A_D without neural network with, like, less latency using S_N_R and energy, um, after the cleaning up. So what I'd been trying was, um, uh - After the b- after the noise compensation, n- I was trying t- to f- find a f- feature based on the ratio of the energies, that is, cl- after clean and before clean. So that if - if they are, like, pretty c- close to one, which means it's speech. And if it is n- if it is close to zero, which is - So it's like a scale @@ probability value. So I was trying, uh, with full band and multiple bands, m- ps- uh - separating them to different frequency bands and deriving separate decisions on each bands, and trying to combine them. Uh, the advantage being like it doesn't have the latency of the neural net if it - if it can g- And it gave me like, uh, one point - Mm-hmm. One - more than one percent relative improvement. So, from fifty-three point six it went to fifty f- four point eight. So it's, like, only slightly more than a percent improvement, just like - Mm-hmm. Which means that it's - it's doing a slightly better job than the previous V_A_D, Mm-hmm. uh, at a l- lower delay. Mm-hmm. Um, so, um - so - u- But - i- d- I'm sorry, does it still have the median filter stuff? It still has the median filter. So - So it still has most of the delay, it just doesn't - Yeah, so d- with the delay, that's gone is the input, which is the sixty millisecond. The forty plus twenty. At the input of the neural net you have this, uh, f- nine frames of context plus the delta. Well, w- i- Mm-hmm. Oh, plus the delta, right. O_K. Yeah. So that delay, plus the L_D_A. Mm-hmm. Uh, so the delay is only the forty millisecond of the noise cleaning, plus the hundred millisecond smoothing at the output. Mm-hmm. Mm-hmm. Um. So. Yeah. So the - the - di- the biggest - The problem f- for me was to find a consistent threshold that works well across the different databases, because I t- I try to make it work on tr- SpeechDat-Car and it fails on T_I-digits, or if I try to make it work on that it's just the Italian or something, it doesn't work on the Finnish. Mm-hmm. Mm-hmm. So, um. So there are - there was, like, some problem in balancing the deletions and insertions when I try different thresholds. So - Mm-hmm. The - I'm still trying to make it better by using some other features from the - after the p- clean up - maybe, some, uh, correlation - auto-correlation or some s- additional features of - to mainly the improvement of the VAD . I've been trying. Now this - this - this, uh, "before and after clean", it sounds like you think that's a good feature. That - that, it - you th- think that the, uh - the - i- it appears to be a good feature, right? Mm-hmm. Yeah. What about using it in the neural net? Yeah, eventually we could - could just Yeah, so - Yeah, so that's the - Yeah. So we've been thinking about putting it into the neural net also. Yeah. Because they did - that itself - Then you don't have to worry about the thresholds and - There's a threshold and - Yeah. Yeah. So that - that's, uh - Yeah. but just - Yeah. So if we - if we can live with the latency or cut the latencies elsewhere, then - then that would be a, Yeah. Yeah. uh, good thing. Um, anybody - has anybody - you guys or - or Naren , uh, somebody, tried the, uh, um, second th- second stream thing? Uh. Oh, I just - I just h- put the second stream in place and, uh ran one experiment, but just like - just to know that everything is fine. Uh-huh. So it was like, uh, forty-five cepstrum plus twenty-three mel - log mel. Yeah. And - and , just, like, it gave me the baseline performance of the Aurora, which is like zero improvement. Yeah. Yeah. So I just tried it on Italian just to know that everything is - But I - I didn't export anything out of it because it was, like, a weird feature set. Yeah. So. Yeah. Well, what I think, you know, would be more what you'd want to do is - is - is, uh, put it into another neural net. Yeah, yeah, yeah, yeah. Mm-hmm. Right? And then - But, yeah, we're - we're not quite there yet. So we have to figure out the neural nets, I guess. Yeah. The uh, other thing I was wondering was, um, if the neural net, um, has any - because of the different noise con- unseen noise conditions for the neural net, where, like, you train it on those four noise conditions, Mm-hmm. while you are feeding it with, like, a- additional - some four plus some - f- few more conditions which it hasn't seen, actually, from the - f- f- while testing. Um - Yeah, yeah. Right. instead of just h- having c- uh, those cleaned up t- cepstrum, sh- should we feed some additional information, like - The - the - We have the V_A_D flag. I mean, should we f- feed the V_A_D flag, also, at the input so that it - it has some additional discriminating information at the input? Hmm- hmm! Um - Wh- uh, the - the V_A_D what? We have the V_A_D information also available at the back-end. Uh-huh. So if it is something the neural net is not able to discriminate the classes - Yeah. I mean - Because most of it is sil- I mean, we have dropped some silence f- We have dropped so- silence frames? No, we haven't dropped silence frames still. Mm-hmm. Uh, still not. Yeah. Yeah. So - the b- b- biggest classification would be the speech and silence. Th- So, by having an additional, uh, feature which says "this is speech and this is nonspeech ", I mean, it certainly helps in some unseen noise conditions for the neural net. What - Do y- do you have that feature available for the test data? Well, I mean, we have - we are transferring the V_A_D to the back-end - feature to the back-end. Because we are dropping it at the back-end after everything - all the features are computed. So - Oh, oh, I see. I see. so the neural - so that is coming from a separate neural net or some V_A_D. O_K. O_K. Which is - which is certainly giving a So you're saying, feed that, also, into @@ to - Yeah. So it- it's an - additional discriminating information. the neural net. Yeah. Yeah. Right. You could feed it into the neural net. The other thing you could do So that - is just, um, p- modify the, uh, output probabilities of the - of the, uh, uh, um, neural net, tandem neural net, based on the fact that you have a silence probability. Mm-hmm. Right? Mm-hmm. So you have an independent estimator of what the silence probability is, and you could multiply the two things, and renormalize. Uh, I mean, you'd have to do the Yeah. nonlinearity part and deal with that. Uh, I mean, go backwards from what the nonlinearity would, you know - would be. But - but, uh - Through - t- to the soft max . Yeah, so - maybe, yeah, when - But in principle wouldn't it be better to feed it in? And let the net do that? Well, u- Not sure. I mean, let's put it this way. I mean, y- you - you have this complicated system with thousands and thousand parameters Hmm. Yeah. and you can tell it, uh, " Learn this thing. " Or you can say, "It's silence! Go away! " I mean, I mean, i- Doesn't - ? I think - I think the second one sounds a lot more direct. Uh. What - what if you - Right. So, what if you then, uh - since you know this, what if you only use the neural net on the speech portions? Well, uh, That's what - Well, I guess that's the same. Uh, that's similar. Yeah, I mean, y- you'd have to actually run it continuously, but it's - @@ - But I mean - I mean, train the net only on - Well, no, you want to train on - on the nonspeech also, because that's part of what you're learning in it, to - to - to generate, that it's - it has to distinguish between. Speech. But I mean, if you're gonna - if you're going to multiply the output of the net by this other decision, uh, would - then you don't care about whether the net makes that distinction, right? Well, yeah. But this other thing isn't perfect. Ah. So that you bring in some information from the net itself. Right, O_K. That's a good point. Yeah. Now the only thing that - that bothers me about all this is that I - I - I - The - the fact - i- i- It's sort of bothersome that you're getting more deletions. Yeah. But - So I might maybe look at, is it due to the fact that um, the probability of the silence at the output of the network, is, uh, Is too high. too - too high or - If it's the case, then multiplying it again by - Yeah. So maybe - So - It may not be - it - i- by something? Mm-hmm. Yeah, it - it may be too - it's too high in a sense, like, everything is more like a, um, Yeah. flat probability. Yeah. So, like, it's not really doing any distinction between speech and nonspeech - or, I mean, different - among classes. Oh-eee-hhh. Uh, yeah. Yeah. Mm-hmm. Be interesting to look at the - Yeah, for the - I wonder if you could do this. But if you look at the, um, highly mism- high mismat- the output of the net on the high mismatch case and just look at, you know, the distribution versus the - the other ones, do you - do you see more peaks or something? Yeah. Yeah, like the entropy of the - the output, or - Yeah. Yeah, for instance. It - it seems that the V_A_D network doesn't - Well, But I - bu- it doesn't drop, uh, too many frames because the dele- the number of deletion is reasonable. But it's just when we add the tandem, the final M_L_P, and then - Yeah. Now the only problem is you don't want to ta- I guess wait for the output of the V_A_D u- before you can put something into the other system, cuz that'll shoot up the latency a lot, right? Am I missing something here? Mm-hmm. But - Yeah. Right. Yeah. So that's maybe a problem with what I was just saying. But - but - I- I guess - But if you were gonna put it in as a feature it means you already have it by the time you get to the tandem net, right? Um, well. We - w- we don't have it, actually, because it's - it has a high rate energy - the V_A_D has a - No. Ah. Yeah. O_K. It's kind of done in - I mean, some of the things are, not in parallel, but certainly, it would be in parallel with the - with a tandem net. Right. In time. So maybe, if that doesn't work, um - But it would be interesting to see if that was the problem, anyway. And - and - and then I guess another alternative would be to take the feature that you're feeding into the V_A_D, Mm-hmm. and feeding it into the other one as well. Mm-hmm. And then maybe it would just learn - learn it better. Um - But that's - Yeah, that's an interesting thing to try to see, if what's going on is that in the highly mismatched condition, it's, um, causing deletions by having this silence probability up - up too high, Mm-hmm. at some point where the V_A_D is saying it's actually speech. Yeah. So, m- Which is probably true. Cuz - Well, the V_A_- if the V_A_D said - since the V_A_D is - is - is right a lot, uh - Yeah. Hmm. Anyway. Might be. Mm-hmm. Yeah. Well, we just started working with it. But these are - these are some good ideas I think. Mm-hmm. Yeah, and the other thing - Well, there are other issues maybe for the tandem, like, uh, well, do we want to, w- uh n- Do we want to work on the targets? Or, like, instead of using phonemes, using more context dependent units? For the tandem net you mean? Hmm. Well, I'm - Yeah. I'm thinking, also, a w- about Dan's work where he - he trained a network, not on phoneme targets but on the H_M_M state targets. And - it was giving s- slightly better results. Problem is, if you are going to run this on different m- Yeah. test sets, including large vocabulary, Yeah. um, Uh - Mmm. I was just thinking maybe about, I think - like, generalized diphones, and - come up with a - a reasonable, not too large, set of context dependent units, and - and - Yeah. And then anyway we would have to reduce this with the K_L_T. So. But - I don't know. Yeah. Yeah. Well, maybe. Mm-hmm. But I d- I d- it - it - i- it's all worth looking at, but it sounds to me like, uh, looking at the relationship between this and the - speech noise stuff is - is - Mm-hmm. is probably a key thing. That and the correlation between stuff. So if, uh - if the, uh, high mismatch case had been more like the, uh, the other two cases in terms of giving you just a better performance, Mm-hmm. how would this number have changed? Oh, it would be - Yeah. Around five percent better, I guess. If - y- Like sixty? if - i- Well, we don't know what's it's gonna be the T_I-digits yet. He hasn't got the results back yet. Yeah. If you extrapolate the SpeechDat-Car well-matched and medium-mismatch, Uh-huh. Yeah. it's around, yeah, maybe five. So this would be sixty-two? Sixty- two. Sixty-two, yeah. Yeah. Somewhere around sixty, must be. Which is - Right? Yeah. Well, it's around five percent, because it's - s- Right? If everything is five percent. Yeah. Yeah. Mm-hmm. All the other ones were five percent, the - I d- I d- I just have the SpeechDat-Car right now, so - Yeah. Yeah. It's running - it shou- we should have the results today during the afternoon, but - Hmm. Well. Hmm. Well - Um - So I won't be here for - When - When do you leave? Uh, I'm leaving next Wednesday. May or may not be in in the morning. I leave in the afternoon. Um, so I - But you're - are you - you're not gonna be around this afternoon? Yeah. Oh, well. I'm talking about next week. I'm leaving - leaving next Wednesday. Oh. This afternoon - uh - Oh, right, for the Meeting meeting? Yeah, that's just cuz of something on campus. Uh-huh. Ah, O_K, O_K. Yeah. But, um, yeah, so next week I won't, and the week after I won't, cuz I'll be in Finland. And the week after that I won't. By that time you'll be - Uh, you'll both be gone from here. So there'll be no - definitely no meeting on - on September sixth. Uh, and - What's September sixth? Uh, that's during Eurospeech. Oh, oh, right. O_K. So, uh, Sunil will be in Oregon. Uh, Stephane and I will be in Denmark. Uh - Right? So it'll be a few weeks, really, before we have a meeting of the same cast of characters. Um, but, uh - I guess, just - I mean, you guys should probably meet. And maybe Barry - Barry will be around. And - and then uh, uh, we'll start up again with Dave and - Dave and Barry and Stephane and us on the, uh, twentieth. No. Thirteenth? About a month? So, uh, you're gonna be gone for the next three weeks or something? I'm gone for two and a half weeks starting - starting next Wed- late next Wednesday. So that's - you won't be at the next three of these meetings. Is that right? Uh, I won't - it's probably four because of - is it three? Let's see, twenty-third, thirtieth, sixth. That's right, next three. And the - the third one won't - probably won't be a meeting, cuz - cuz, uh, Su- Sunil, Stephane, and I will all not be here. Oh, right. Right. Um - Mmm. So it's just, uh, the next two where there will be - there, you know, may as well be meetings, but I just won't be at them. O_K. And then starting up on the thirteenth, uh, we'll have meetings again but we'll have to do without Sunil here somehow. So. When do you go back? Thirty-first, August. Yeah. Yeah. So. Cool . When is the evaluation? November, or something? Yeah, it was supposed to be November fifteenth. Has anybody heard anything different? I don't know. The meeting in - is the five and six of December. p- s- It's like - Yeah, it's tentatively all full . Yeah. So - Mm-hmm. Uh, that's a proposed date, I guess. Yeah, um - so the evaluation should be on a week before or - Yeah. Yep. But, no, this is good progress. So. Uh - O_K. Guess we're done. Should we do digits? Digits? Yep. Transcript L_ dash three five two. Five seven six four, five six seven zero, four six nine three. Six eight five zero, nine one three nine, four six four eight. Three four four two, seven, one eight two. One eight seven four, nine nine eight four, five eight nine seven. One eight three nine, zero one four five, three six two nine. Five four three, six two, six six seven three. Seven one five one, six zero seven two, five nine four two. Nine eight eight, eight one, nine eight one eight. Transcript L_ dash three five three. Seven nine one, one two six, five four two. Eight seven three, nine eight four, nine six four six. Three five, seven four, two two, five nine, six one. Five nine, nine seven, nine eight, five one, eight two. Seven five four five, six six five three, zero one one two. Nine nine zero seven, three, nine two six. Zero one nine, three nine eight, zero three five zero. Two eight six, two zero two, one eight one. Transcript L_ dash three five four. Two nine six, eight six three, seven six zero five. Seven one five six, one three seven zero, four two five six. Nine five three seven, zero, two one eight. One eight six three, nine eight seven one, one zero two nine. Three five three three, four nine three nine, zero three one five. Four zero eight six, four eight, nine five zero, three. Eight zero two eight, nine, seven nine one. Eight nine, nine one, three five, one eight, zero four. Transcr- transcript L_ dash three five zero. Eight four four, two three two, two six one seven. One two, eight three, one nine, nine one, one three. Four five, two four, five nine, six two, three three. Three, eight four six, five five, two zero two, five. Four, six nine three, one three, three six four, six. Two, eight four six, four one, four four six, four. Two nine nine four, three two eight seven, eight seven four two. Four two eight, two zero, seven four eight zero. O_K. It's a wrap. 