@@ I think for two years we were two months, uh, away from being done. And what was that, Morgan? What project? Uh, the, uh, TORRENT chip. Oh. Yeah. We were two - we were - Yeah. Uh, uh, we went through it - Jim and I went through old emails at one point and - and for two years there was this thing saying, yeah, we're - we're two months away from being done. It was very - very believable schedules, too. I mean, we went through and - with the schedules - and we - Yeah. Oh, yeah. It was very true. It was true for two years. So, should we just do the same kind of deal where we go around and do, uh, status report kind of things? O_K. And I guess when Sunil gets here he can do his last or something. Mm-hmm. So. Yeah. So we probably should wait for him to come before we do his. O_K. That's a good idea. O_K. Yeah. Yeah. Any objection? Do y- O_K, M- All in favor Do you want to start, Morgan? Do you have anything, or - ? Uh, I don't do anything. I - No, I mean, I - I'm involved in discussions with - with people about what they're doing, but I think they're - since they're here, they can talk about it themselves. O_K. So should I go so that, uh, Yeah. Why don't you go ahead, Barry? you're gonna talk about Aurora stuff, per se? O_K. Um. O_K. Well, this past week I've just been, uh, getting down and dirty into writing my - my proposal. So, um - Mmm. I just finished a section on, uh - on talking about these intermediate categories that I want to classify, um, as a - as a middle step. And, um, I hope to - hope to get this, um - a full rough draft done by, uh, Monday so I can give it to Morgan. When is your, uh, meeting? Um, my meeting with, uh - ? Oh, oh, you mean the - the quals. Uh, the quals are happening in July twenty-fifth. Yeah. The quals. Yeah. Oh. Soon. Yeah. D_-Day. Uh-huh. Yeah. Uh-huh. So, is the idea you're going to do this paper and then you pass it out to everybody ahead of time and - ? Right, right. So, y- you write up a proposal, and give it to people ahead of time, and you have a short presentation. And, um, and then, um - then everybody asks you questions. Hmm. Yeah. I remember now. Yep. So, um. Y- s- Have you d- ? I was just gonna ask, do you want to say any - a little bit about it, or - ? Oh. Uh, a little bit about - ? Mmm. Wh- what you're - what you're gonna - You said - you were talking about the, uh, particular features that you were looking at, or - Oh, the - the - Right. Well, I was, um, I think one of the perplexing problems is, um, for a while I was thinking that I had to come up with a complete set of intermediate features - in- intermediate categories to - to classify right away. But what I'm thinking now is, I would start with - with a reasonable set. Something - something like, um, um - like, uh, re- regular phonetic features, just to - just to start off that way. And do some phone recognition. Um, build a system that, uh, classifies these, um - these feat- uh, these intermediate categories using, uh, multi-band techniques. Combine them and do phon- phoneme recognition. Look at - then I would look at the errors produced in the phoneme recognition and say, O_K, well, I could probably reduce the errors if I included this extra feature or this extra intermediate category. That would - that would reduce certain confusions over other confusions. And then - and then reiterate. Um, build the intermediate classifiers. Uh, do phoneme recognition. Look at the errors. And then postulate new - or remove, um, intermediate categories. And then do it again. So you're gonna use TIMIT? Um, for that - for that part of the - the process, yeah, I would use TIMIT. Mm-hmm. And, um, then - after - after, uh, um, doing TIMIT. Right? Um, that's - Mm-hmm. that's, um - that's just the ph- the phone recognition task. Yeah. Uh, I wanted to take a look at, um, things that I could model within word. So, I would mov- I would then shift the focus to, um, something like Schw- Switchboard, uh, where I'd - I would be able to, um - to model, um, intermediate categories that span across phonemes, not just within the phonemes, themselves, Mm-hmm. um, and then do the same process there, um, on - on a large vocabulary task like Switchboard. Uh, and for that - for that part I would - I'd use the S_R_I recognizer since it's already set up for - for Switchboard. And I'd run some - some sort of tandem-style processing with, uh, my intermediate classifiers. Oh. So that's why you were interested in getting your own features into the S_R_I files. Yeah. That's why I - I was asking about that. Yeah. Yeah. Yeah. Um, and I guess that's - that's it. Any - any questions? Sounds good. So you just have a few more weeks, huh? Um, yeah. A few more. It's about a month from now? It's a - it's a month and - and a week. Yeah. Yeah. So, uh, you want to go next, Dave? And we'll do - Oh. O_K, sure. So, um, last week I finally got results from the S_R_I system about this mean subtraction approach. And, um, we - we got an improvement, uh, in word error rate, training on the T_I-digits data set and testing on Meeting Recorder digits of, um, six percent to four point five percent, um, on the n- on the far-mike data using P_Z_M F_, but, um, the near-mike performance worsened, um, from one point two percent to two point four percent. And, um, wh- why would that be, um, considering that we actually got an improvement in near-mike performance using H_T_K? And so, uh, with some input from, uh, Andreas, I have a theory in two parts. Um, first of all H_T_K - sorry, S_R- the S_R_I system is doing channel adaptation, and so H_T_K wasn't. Um, so this, um - This mean subtraction approach will do a kind of channel normalization and so that might have given the H_T_K use of it a boost that wouldn't have been applied in the S_R_I case. And also, um, the - Andreas pointed out the S_R_I system is using more parameters. It's got finer-grained acoustic models. So those finer-grained acoustic models could be more sensitive to the artifacts in the re-synthesized audio. Um. And me and Barry were listening to the re-synthesized audio and sometimes it seems like you get of a bit of an echo of speech in the background. And so that seems like it could be difficult for training, cuz you could have different phones lined up with a different foreground phone, um, depending on the timing of the echo. So, um, I'm gonna try training on a larger data set, and then, eh, the system will have seen more examples o- of these artifacts and hopefully will be more robust to them. So I'm planning to use the Macrophone set of, um, read speech, and, um - Hmm. I had another thought just now, which is, uh, remember we were talking before about - we were talking in our meeting about, uh, this stuff that - some of the other stuff that Avendano did, where they were, um, getting rid of low-energy sections? Um, uh, if you - if you did a high-pass filtering, as Hirsch did in late eighties to reduce some of the effects of reverberation, uh, uh, Avendano and Hermansky were arguing that, uh, perhaps one of the reasons for that working was ma- may not have even been the filtering so much but the fact that when you filter a - an all-positive power spectrum you get some negative values, and you gotta figure out what to do with them if you're gonna continue treating this as a power spectrum. So, what - what Hirsch did was, uh, set them to zero - set the negative values to zero. So if you imagine a - a waveform that's all positive, which is the time trajectory of energy, um, and, uh, shifting it downwards, and then getting rid of the negative parts, that's essentially throwing away the low-energy things. And it's the low-energy parts of the speech where the reverberation is most audible. You know, you have the reverberation from higher-energy things showing up in - So in this case you have some artificially imposed reverberation-like thing. I mean, you're getting rid of some of the other effects of reverberation, but because you have these non-causal windows, you're getting these funny things coming in, uh, at n- And, um, what if you did - ? I mean, there's nothing to say that the - the processing for this re-synthesis has to be restricted to trying to get it back to the original, according to some equation. I mean, you also Uh-huh. could, uh, just try to make it nicer. Mm-hmm. And one of the things you could do is, you could do some sort of V_A_D-like thing and you actually could take very low-energy sections and set them to some - some, uh, very low or - or near zero value. Uh-huh. I mean, uh, I'm just saying if in fact it turns out that - that these echoes that you're hearing are, uh - or pre-echoes, whichever they are - are - are, uh, part of what's causing the problem, you actually could get rid of them. Uh-huh. Be pretty simple. O_K. I mean, you do it in a pretty conservative way so that if you made a mistake you were more likely to keep in an echo than to throw out speech. Hmm. Um, what is the reverberation time like there ? In thi- in this room? Uh - On, uh, the - the one what - the s- in the speech that you are - you are using like? Y- Yeah. I - I - I - I don't know. So, it's this room. It's, uh - Oh, this room? O_K. It's - it's this room. So - so it's - these are just microphone - this micro- close microphone and a distant microphone, he's doing these different tests on. Oh. Uh, we should do a measurement in here. I g- think we never have. I think it's - I would guess, uh, Hmm! point seven, point eight seconds f- uh, R_T_sixty - something like that? Mm-hmm. But it's - you know, it's this room. So. O_K. Mm-hmm. Uh. But the other thing is, he's putting in - w- I was using the word "reverberation" in two ways. He's also putting in, uh, a - he's taking out some reverberation, but he's putting in something, because he has averages over multiple windows stretching out to twelve seconds, which are then being subtracted from the speech. And since, you know, what you subtract, sometimes you'll be - you'll be subtracting from some larger number and sometimes you won't. And - Mm-hmm. Mm-hmm. So you can end up with some components in it that are affected by things that are seconds away. Uh, and if it's a low energy compo- portion, you might actually hear some funny things. Yeah. O- o- one thing, um, I noticed is that, um, the mean subtraction seems to make the P_Z_M signals louder after they've been re-synthesized. So I was wondering, is it possible that one reason it helped with the Aurora baseline system is just as a kind of gain control? Cuz some of the P_Z_M signals sound pretty quiet if you don't amplify them. Mm-hmm. I don't see why - why your signal is louder after processing, because yo- Yeah. I don't know why -y, uh, either. Yeah. I don't think just multiplying the signal by two would have any effect. Mm-hmm. Oh, O_K. Yeah. I mean, I think if you really have louder signals, Well, well - what you mean is that you have better signal-to-noise ratio. So if what you're doing is improving the signal-to-noise ratio, then it would be better. Mm-hmm. But just it being bigger if - with the same signal-to-noise ratio - It w- i- i- it wouldn't affect things. O_K. Yeah. No. Well, the system is - use the absolute energy, so it's a little bit dependent on - on the signal level. But, not so much, I guess. Well, yeah. But it's trained and tested on the same thing. So if the - if the - if you change Mmm. Mm-hmm. in both training and test, the absolute level by a factor of two, it will n- have no effect. Yeah. Did you add this data to the training set, for the Aurora? Uh - Or you just tested on this? Um. Did I w- what? Sorry? Well, Morgan was just saying that, uh, as long as you do it in both training and testing, it shouldn't have any effect. But I - Yeah. I was sort of under the impression that you just tested with this data. You didn't train it also. I - I b- I - Right. I trained on clean T_I-digits. I - I did the mean subtraction on clean T_I-digits. But I didn't - I'm not sure if it made the clean ti- T_I-digits any louder. Oh, I see. I only remember noticing it made the, um, P_Z_M signal louder. O_K. Well, I don't understand then. Yeah. Huh. I don't know. If it's - if it's - like, if it's trying to find a - a reverberation filter, it could be that this reverberation filter is making things quieter. And then if you take it out - that taking it out makes things louder. I mean. Uh, no. I mean, uh, there's - there's nothing inherent about Nuh-huh. removing - if you're really removing, uh, r- uh, then I don't see how that would make it louder. So it might be just some - The mean. O_K. Yeah, I see. Yeah. O_K. So I should maybe listen to that stuff again. Yeah. It might just be some artifact of the processing that - that, uh, if you're - Uh, yeah. I don't know. Oh. O_K. I wonder if there could be something like, uh - Eh- for s- for the P_Z_M data, uh, you know, if occasionally, uh, somebody hits the table or something, you could get a spike. Uh. I'm just wondering if there's something about the, um - you know, doing the mean normalization where, uh, it - it could cause you to have better signal-to-noise ratio. Um. Well, you know, there is this. Wait a minute. It - it - i- maybe - i- If, um - Subtracting the - the mean log spectrum is - is - is like dividing by the spectrum. So, depending what you divide by, if your - if s- your estimate is off and sometimes you're - you're - you're getting a small number, Mm-hmm. you could make it bigger. Mm-hmm. So, it's - it's just a - a question of - there's - It - it could be that there's some normalization that's missing, or something to make it - Mm-hmm. Uh, y- you'd think it shouldn't be larger, but maybe in practice it is. Hmm. That's something to think about. I don't know. I had a question about the system - the S_R_I system. So, you trained it on T_I-digits? But except this, it's exactly the same system as the one that was tested before and that was trained on Macrophone. Right? So on T_I-digits it gives you one point two percent error rate and on Macrophone it's still O_ point eight. Uh, but is it exactly the same system? Uh. I think so. If you're talking about the Macrophone results that Andreas had about, um, a week and a half ago, I think it's the same system. Hmm. Mm-hmm. So you use V_T_L- uh, vocal tract length normalization and, um, Mm-hmm. like M_L_L_R transformations also, and - I'm sorry, was his point eight percent, er, a - a result on testing on Macrophone or - or training? That's - all that stuff. It was training on Macrophone and testing - yeah, on - on meeting digits. Oh. So that was done already. So we were - Uh, and it's point eight? O_K. Mm-hmm. O_K. Yeah. I - I've just been text- testing the new Aurora front-end with - well, Aurora system actually - so front-end and H_T_K, um, acoustic models on the meeting digits and it's a little bit better than the previous system. We have - I have two point seven percent error rate. And before with the system that was proposed, it's what? It was three point nine. So. We are getting better. Oh, that's a lot better. So, what - w- ? With the - with the H_T_K back-end? What we have for Aurora? And - Yeah. Two point seven. I know in the meeting, like - Right. On the meeting we have two point seven. Oh. That's with the new I_I_R filters? Uh. Yeah, yeah. So, yeah, we have the new L_D_A filters, and - O_K. I think, maybe - I didn't look, but one thing that makes a difference is this D_C offset compensation. Uh, eh - Do y- did you have a look at - at the meet- uh, meeting digits, if they have a D_C component, or - ? I - I didn't. No. Oh. Hmm. No. The D_C component could be negligible. I mean, if you are recording it through a mike. I mean, any - all of the mikes have the D_C removal - some capacitor sitting right in that bias it . Yeah. But this - uh, uh, uh, no. Because, uh, there's a sample and hold in the A_to_D. And these period- these typically do have a D_C offset. Oh, O_K. And - and they can be surprisingly large. It depends on the electronics. Oh, so it is the digital - O_K. It's the A_to_D that introduces the D_C in. Yeah. The microphone isn't gonna pass any D_C. But - but, Yeah. Yeah. Yeah. O_K. typi- you know, unless - Actually, there are instrumentation mikes that - that do pass - go down to D_C. But - but, Mm-hmm. uh, no, it's the electronics. And they - and - Mm-hmm. then there's amplification afterwards. And you can get, I think it was - I think it was in the Wall Street Journal data that - that - I can't remember, one of the DARPA things. There was this big D_C- D_C offset we didn't - we didn't know about for a while, while we were Mm-hmm. messing with it. And we were getting these terrible results. And then we were talking to somebody and they said, "Oh, yeah. Didn't you know? Everybody knows that. There's all this D_C offset in th-" So, yes. You can have D_C offset in the data. Yeah. Oh, O_K. O_K. So was that - was that everything, Dave? Oh. And I also, um, did some experiments about normalizing the phase. Um. So I c- I came up with a web page that people can take a look at. And, um, the interesting thing that I tried was, um, Adam and Morgan had this idea, um, since my original attempts to, um, take the mean of the phase spectra over time and normalize using that, by subtracting that off, didn't work. Um, so, well, that we thought that might be due to, um, problems with, um, the arithmetic of phases. They - they add in this modulo two pi way and, um, there's reason to believe that that approach of taking the mean of the phase spectrum wasn't really mathematically correct. So, what I did instead is I took the mean of the F_F_T spectrum without taking the log or anything, and then I took the phase of that, and I subtracted that phase off to normalize. But that, um, didn't work either. See, we have a different interpretation of this. He says it doesn't work. I said, I think it works magnificently, but just not for the task we intended. Uh, it gets rid of the speech. What does it leave? Uh, gets rid of the speech. Uh, it leaves - you know, it leaves the junk. I mean, I - I think it's - it's tremendous. Oh, wow. You see, all he has to do is go back and reverse what he did before, and he's really got something. Well, could you take what was left over and then subtract that? Ex- exactly. Yeah, you got it. Yeah. Yeah. Oh, it's - So, it's - it's a general rule. Just listen very carefully to what I say and do the opposite. Including what I just said. And, yeah, that's everything. All set? Do you want to go, Stephane? Um. Yeah. Maybe, concerning these d- still, these meeting digits. I'm more interested in trying to figure out what's still the difference between the S_R_I system and the Aurora system. And - Um. Yeah. So, I think I will maybe train, like, gender-dependent models, because this is also one big difference between the two systems. Um, the other differences were the fact that maybe the acoustic models of the S_R_I are more - S_R_I system are more complex. But, uh, Chuck, you did some experiments with this and It didn't seem to help in the H_T_K system. it was hard t- to - to have some exper- some improvement with this. Um. Well, it sounds like they also have - he - he's saying they have all these, uh, Mm-hmm. uh, different kinds of adaptation. You know, they have channel adaptation. They have speaker adaptation. Yeah. Right. Yeah. Yeah. Yeah. Yeah. Well, there's also the normalization. Like they do, um - I'm not sure how they would do it when they're working with the digits, but, like, in the Switchboard data, there's, um - The vocal tr- conversation-side normalization for the non-C_zero components, and then utterance normalization for the C_zero components. Yeah. Yeah. This is another difference. Their normalization works like on - on the utterance levels. Mm-hmm. But we have to do it - We have a system that does it on-line. So, it might be - Right. it might be better with - it might be worse if Yeah. the channel is constant, or - And the acoustic models are like -k triphone models or - or is it the whole word? Nnn. S_R_I - it's - it's tr- Yeah. I guess it's triphones. S_R_I. Yeah. It's triphone. I think it's probably more than that. I mean, so they - they have - I - I thin- think they use these, uh, Huh. uh, genone things. So there's - there's these kind of, uh, uh, pooled models and - and they can go out to all sorts of dependencies. Oh. It's like the tied state. So. Mm-hmm. They have tied states and I think - I - I - I don't real- I'm talk- I'm just guessing here. But I think - I think they - they don't just have triphones. I think they have a range of - of, uh, dependencies. O_K. Mm-hmm. Mm-hmm. Mm-hmm. Hmm. And - Yeah. Well. Um. Well, the first thing I - that I want to do is just maybe these gender things. Uh. And maybe see with Andreas if - Well, I - I don't know how much it helps, what's the model. So - so the n- stuff on the numbers you got, the two point seven, is that using the same training data that the S_R_I system used and got one point two? That's right. So it's the clean T_I-digits training set. So exact same training data? Right. Mm-hmm. O_K. I guess you used the clean training set. Right. For - with the S_R_I system - Mm-hmm. Well. You know, the - the Aurora baseline is set up with these, um - this version of the clean training set that's been filtered with this G_-seven-one-two filter, and, um, to train the S_R_I system on digits S_- Andreas used the original T_I-digits, um, under U_ doctor-speech data T_I-digits, which don't have this filter. But I don't think there's any other difference. Mm-hmm. Mm-hmm. Yeah. So is that - ? Uh, are - are these results comparable? So you - you were getting with the, uh, Aurora baseline something like two point four percent on clean T_I-digits, when, uh, training the S_R_I system with clean T_R digits - T_I-digits. Right? And - Yeah. Um. Uh-huh. And, so, is your two point seven comparable, where you're, uh, uh, using, uh, the submitted system? Yeah. I think so. Yeah. O_K. So it's about the same, maybe a little worse. Mm-hmm. W- w- it was one - one point two Ye- with the S_R_I system, I - I'm sorry. Yeah. The complete S_R_I system is one point two. Yeah. You - you were H_T_K. Right? O_K. That's right. So - O_K, so the comparable number then, uh Mm-hmm. for what you were talking about then, since it was H_T_K, would be the um, two point f- It was four point something. Right? D- d- The H_T_K system with, uh, b- Oh, right, right, right, right. M_F_C_C features - Do you mean the b- ? The baseline Aurora-two system, trained on T_I-digits, tested on Meeting Recorder near, I think we saw in it today, and it was about six point six percent. Oh. Right. Right, right, right. O_K. Alright. So - So - Yeah. The only difference is the features, right now, between this and - He's doing some different things. Yes. O_K, good. So they are helping. That's good to hear. Yeah. Mm-hmm. They are helping. Yeah. Um. Yeah. And another thing I - I maybe would like to do is to just test the S_R_I system that's trained on Macrophone - test it on, Yeah. uh, the noisy T_I-digits, cuz I'm still wondering where this improvement comes from. When you train on Macrophone, it seems better on meeting digits. But I wonder if it's just because maybe Macrophone is acoustically closer to the meeting digits than - than T_I-digit is, which is - T_I-digits are very clean recorded digits and - Mm-hmm. Uh, f- s- You know, it would also be interesting to see, uh - to do the regular Aurora test, um, but use the S_R_I system instead of H_T_K. That's - Yeah. That's what I wanted, just, uh - Yeah. So, just using the S_R_I system, test it on - and test it on Aurora T_I-digits. Right. Why not the full Aurora, uh, test? Um. Yeah. There is this problem of multilinguality yet. So we don't - Mm-hmm. You'd have to train the S_R_I system with - with all the different languages. i- i- We would have to train on - Right. Yeah. Yeah. That's what I mean. So, like, comple- It'd be a lot of work. That's the only thing. Yeah. It's - Mmm. Well, I mean, uh, Mmm. uh, I guess the work would be into getting the - the files in the right formats, or something. Right? I mean - Mm-hmm. Because when you train up the Aurora system, you're, uh - you're also training on all the That's right. Yeah. data. I mean, it's - Yeah. I see. Oh, so, O_K. Right. I see what you mean. That's true, but I think that also when we've had these meetings week after week, oftentimes people have not done the full arrange of things because - on - on whatever it is they're trying, because it's a lot of work, even just with the H_T_K. Mm-hmm. Mm-hmm. So, it's - it's a good idea, but it seems like it makes sense to do some pruning Mm-hmm. first with a - a test or two that makes sense for you, and then take the likely candidates and go further. Yeah. Yeah. Mm-hmm. Yeah. But, just testing on T_I-digits would already give us some information about what's going on. And - mm-hmm. Uh, yeah. O_K. Uh, the next thing is this - this V_A_D problem that, um, um - So, I'm just talking about the - the curves that I - I sent - I sent you - so, whi- that shows that when the S_N_R decrease, uh, the current V_A_D approach doesn't drop much frames for some particular noises, uh, which might be then noises that are closer to speech, uh, acoustically. I- i- Just to clarify something for me. I- Mm-hmm. They were supp- Supposedly, in the next evaluation, they're going to be supplying us with boundaries. So does any of this matter? I mean, other than our interest in it. Uh - Uh - Well. First of all, the boundaries might be, uh - like we would have t- two hundred milliseconds or - before and after speech. Uh. So removing more than that might still make a difference in the results. And - Do we - ? I mean, is there some reason that we think that's the case? No. Because we don't - didn't Yeah. looked that much at that. But, still, I think it's an interesting problem. And - Oh, yeah. Um. Yeah. But maybe we'll get some insight on that when - when, uh, the gang gets back from Crete. Because there's lots of interesting problems, of course. And then the thing is if - if they really are going to have some means of giving us fairly tight, Mm-hmm. Yeah, yeah. Mm-hmm. uh, boundaries, then that won't be so much the issue. Mm-hmm. Um But I don't know . Because w- we were wondering whether that V_A_D is going to be, like, a realistic one or is it going to be some manual segmentation. And then, like, if - if that V_A_D is going to be a realistic one, then we can actually use their markers to shift the point around, I mean, the way we want to find a - I mean, rather than keeping the twenty frames, we can actually move the marker to a point which we find more suitable for us. Mm-hmm. But if that is going to be something like a manual, uh, segmenter, then we can't use that information anymore, because that's not going to be the one that is used in the final evaluation. Right. Mm-hmm. Right. So. We don't know what is the type of V_A_D which they're going to provide. Yeah. Yeah. And actually there's - Yeah. There's an - uh, I think it's still for - even for the evaluation, uh, it might still be interesting to work on this because the boundaries apparently that they would provide is just, um, starting of speech and end of speech uh, at the utterance level. And - Um. With some - some gap. I mean, with some pauses in the center, So - provided they meet that - whatever the hang-over time which they are talking. Yeah. But when you have like, uh, five or six frames, both - Yeah. Then the- they will just fill - fill it up. I mean, th - Yeah. it - it - with - Yeah. So - So if you could get at some of that, uh - although that'd be hard. But - but - Yeah. Yeah. It might be useful Yeah. for, like, noise estimation, and a lot of other things that we want to work on. But - Mmm. Right. O_K. Yeah. So I did - I just started to test putting together two V_A_D which was - was not much work actually. Um, I im- re-implemented a V_A_D that's very close to the, um, energy-based V_A_D that, uh, the other Aurora guys use. Um. So, which is just putting a threshold on the noise energy, Mm-hmm. and, detect- detecting the first group of four frames that have a energy that's above this threshold, and, uh, from this point, uh, tagging the frames there as speech. So it removes the first silent portion - portion of each utterance. And it really removes it, um, still o- on the noises where our M_L_P V_A_D doesn't work a lot. Mmm. Uh, and - Cuz I would have thought that having some kind of spectral information, uh - uh, you know, in the old days people would use energy and zero crossings, for instance - uh, would give you some better performance. Right? Cuz you might have low-energy fricatives or - or, uh stop consonants, or something like that. Mm-hmm. Uh. Yeah. So, your point is - will be to u- use whatever - Oh, that if you d- if you use purely energy and don't look at anything spectral, then you don't have a good way of distinguishing between low-energy speech components and nonspeech. Mm-hmm. And, um, just as a gross generalization, most nonsp- many nonspeech noises have a low-pass kind of characteristic, some sort of slope. And - and most, um, low-energy speech components that are unvoiced have a - Mm-hmm. a high-pass kind of characteristic - an upward slope. Yeah. So having some kind of a - uh, you know, at the beginning of a - of a - of an S_ sound for instance, just starting in, it might be pretty low-energy, but it will tend to have this high-frequency component. Whereas, Mm-hmm. a - a lot of rumble, and background noises, and so forth will be predominantly low-frequency. Uh, you know, by itself it's not enough to tell you, but it plus energy is sort of - Yeah. it plus energy plus timing information is sort of - Mm-hmm. I mean, if you look up in Rabiner and Schafer from like twenty-five years ago or something, that's sort of what they were using then. So it's - it's not a - Mm-hmm. Mm-hmm. Hmm. So, yeah. It - it might be that what I did is - so, removes like low, um, uh - low-energy, uh, speech frames. Because the way I do it is I just - I just combine the two decisions - so, the one from the M_L_P and the one from the energy-based - with the - with the and operator. So, I only keep the frames where the two agree that it's speech. So if the energy-based dropped - dropped low-energy speech, mmm, they - they are - they are lost. Mmm. But s- still, the way it's done right now it - it helps on - on the noises where - it seems to help on the noises where Mm-hmm. our V_A_D was not very good. Well, I guess - I mean, one could imagine combining them in different ways. But - but, I guess what you're saying is that the - the M_L_P-based one has the spectral information. Yeah. So. But - Yeah. But the way it's combined wi- is maybe done - Well, yeah . Well, you can imagine - The way I use a an- a "AND" operator is - So, it - I, uh - Is - ? The frames that are dropped by the energy-based system are - are, uh, dropped, even if the, um, M_L_P decides to keep them. Right. Right. And that might not be optimal, but - but - I mean, I guess in principle what you'd want to do is have a - But, yeah. Mm-hmm. No- uh, a probability estimated by each one and - and put them together. Yeah. Mmm. M- Yeah. Something that - that I've used in the past is, um - when just looking at the energy, is to look at the derivative. And you make your decision when the derivative is increasing for so many frames. Then you say that's beginning of speech. Uh-huh. But, I'm - I'm trying to remember if that requires that you keep some amount of speech in a buffer. I guess it depends on how you do it. But I mean, that's - that's been a useful thing. Yeah. Mm-hmm. Mm-hmm. Yeah. Well, every- everywhere has a delay associated with it. I mean, you still have to k- always keep a buffer, Mm-hmm. then only make a decision because you still need to smooth the decision further. Right. Right. So that's always there. Yeah. O_K. Well, actually if I don't - maybe don't want to work too much of - on it right now. I just wanted to - to see if it's - what I observed was the re- was caused by this - this V_A_D problem. And it seems to be the case. Mm-hmm. Um. Uh, the second thing is the - this spectral subtraction. Um. Um, which I've just started yesterday to launch a bunch of, uh, twenty-five experiments, uh, with different, uh, values for the parameters that are used. So, it's the Makhoul-type spectral subtraction which use an over-estimation factor. So, we substr- I subtract more, um, noise than the noise spectra that is estimated on the noise portion of the s- uh, the utterances. So I tried several, uh, over-estimation factors. And after subtraction, I also add a constant noise, and I also try different, uh, noise, uh, values and we'll see what happen. Hmm. O_K. Mm-hmm. Mm-hmm. But st- still when we look at the, um - Well, it depends on the parameters that you use, but for moderate over-estimation factors and moderate noise level that you add, you st- have a lot of musical noise. Um. On the other hand, when you subtract more and when you add more noise, you get rid of this musical noise but maybe you distort a lot of speech. So. Well. Mmm. Well, it - until now, it doesn't seem to help. But We'll see. So the next thing, maybe I - what I will try to - to do is just to try to smooth mmm, the, um - to smooth the d- the result of the subtraction, to get rid of the musical noise, using some kind of filter, or - Can smooth the S_N_R estimate, also. Yeah. Right. Mmm. Your filter is a function of S_N_R. Hmm? Yeah. So, to get something that's - would be closer to what you tried to do with Wiener filtering. And - Yeah. Mm-hmm. Yeah. Actually, it's, uh - Uh. I don't know, it's - go ahead. And it's - go ahead. It - Maybe you can - I think it's - That's it for me. O_K. So, uh - u- th- I've been playing with this Wiener filter, like. And there are - there were some bugs in the program, so I was p- initially trying to clear them up. Because one of the bug was - I was assuming that always the VAD - uh, the initial frames were silence. It always started in the silence state, but it wasn't for some utterances. So the - it wasn't estimating the noise initially, and then it never estimated, because I assumed that it was always Mm-hmm. silence. So this is on SpeechDat-Car Italian? Yeah. SpeechDat-Car Italian. So, in some cases s- there are also - Yeah. There're a few cases, actually, which I found later, that there are. o- Uh-huh. So that was one of the bugs that was there in estimating the noise. And, uh, so once it was cleared, uh, I ran a few experiments with different ways of smoothing the estimated clean speech and how t- estimated the noise and, eh, smoothing the S_N_R also. And so the - the trend seems to be like, uh, smoothing the current estimate of the clean speech for deriving the S_N_R, which is like deriving the Wiener filter, seems to be helping. Then updating it quite fast using a very small time constant. So we'll have, like, a few results where the - estimating the - the - More smoothing is helping. But still it's like - it's still comparable to the baseline. I haven't got anything beyond the baseline. But that's, like, not using any Wiener filter. And, uh, so I'm - I'm trying a few more experiments with different time constants for smoothing the noise spectrum, and smoothing the clean speech, and smoothing S_N_R. So there are three time constants that I have. So, I'm just playing around. So, one is fixed in the line, like Smoothing the clean speech is - is helping, so I'm not going to change it that much. But, the way I'm estimating the noise and the way I'm estimating the S_N_R, I'm just trying - trying a little bit. So, that h- And the other thing is, like, putting a floor on the, uh, S_N_R, because that - if some - In some cases the clean speech is, like - when it's estimated, it goes to very low values, so the S_N_R is, like, very low. And so that actually creates a lot of variance in the low-energy region of the speech. So, I'm thinking of, like, putting a floor also for the S_N_R so that it doesn't vary a lot in the low-energy regions. And, uh. So. The results are, like - So far I've been testing only with the baseline, which is - which doesn't have any L_D_A filtering and on-line normalization. I just want to separate the - the contributions out. So it's just VAD, plus the Wiener filter, plus the baseline system, which is, uh, just the spectral - I mean, the mel sp- mel, uh, frequency coefficients. Um. And the other thing that I tried was - but I just took of those, uh, Carlos filters, which Hynek had, to see whether it really h- helps or not. I mean, it was just a - a run to see whether it really degrades or it helps. And it's - it seems to be like it's not hurting a lot by just blindly picking up one filter which is nothing but a four hertz - a band-pass m- m- filter on the cubic root of the power spectrum. So, that was the filter that Hy- uh, Carlos had. And so - Yeah. Just - just to see whether it really - it's - it's - is it worth trying or not. So, it doesn't seems to be degrading a lot on that. So there must be something that I can - that can be done with that type of noise compensation also, which - I guess I would ask Carlos about that. I mean, how - how he derived those filters and - and where d- if he has any filters which are derived on O_G_I stories, added with some type of noise which - what we are using currently, or something like that. So maybe I'll - This is cubic root of power spectra? Yeah. Cubic root of power spectrum. So, if you have this band-pass filter, you probably get n- you get negative values. Right? Yeah. And I'm, like, floating it to z- zeros right now. O_K. So it has, like - the spectrogram has, like - Uh, it actually, uh, enhances the onset and offset of - I mean, the - the begin and the end of the speech. So it's - there seems to be, like, deep valleys in the begin and the end of, like, high-energy regions, because the filter has, like, a sort of Mexican-hat type structure. Mm-hmm. So, those are the regions where there are, like - when I look at the spectrogram, there are those deep valleys on the begin and the end of the speech. Mm-hmm. But the rest of it seems to be, like, pretty nice. Mm-hmm. So. That's something I observe using that filter. And - Yeah. There are a few - very - not a lot of - because the filter doesn't have a - really a deep negative portion, so that it's not really creating a lot of negative values in the cubic root. So, I'll - I'll s- may- continue with that for some w- I'll - I'll - Maybe I'll ask Carlos a little more about how to play with those filters, and - but while making this Wiener filter better. So. Yeah. That - that's it, Morgan. Uh, last week you were also talking about building up the subspace stuff? Yeah. I - I - I would actually m- m- didn't get enough time to work on the subspace last week. It was mostly about finding those bugs and th- you know, things, and I didn't work much on that. O_K. How about you, Carmen? Well, I am still working with, eh, V_T_S. And, one of the things that last week, eh, say here is that maybe the problem was with the diff because the signal have different level of energy. Hmm? And, maybe, talking with Stephane and with Sunil, we decide that maybe it was interesting to - to apply on-line normalization before applying V_T_S. But then we decided that that's - it doesn't work absolutely, because we modified also the noise. And - Well, thinking about that, we - we then - we decide that maybe is a good idea. We don't know. I don't hav- I don't - this is - I didn't do the experiment yet - to apply V_T_S in cepstral domain. The other thing is - So - so, in - i- i- and - Not - and C_zero would be a different - So you could do a different normalization for C_zero than for other things anyway. I mean, the other thing I was gonna suggest is that you could have two kinds of normalization with - with, uh, different time constants. So, uh, you could do some normalization s- uh, before the V_T_S, and then do some other normalization after. I don't know. But - but C_zero certainly acts differently than the others do, so that's - Uh. Mm-hmm. Well, we s- decide to m- to - to obtain the new expression if we work in the cepstral domain. And - Well. I am working in that now, but I'm not sure if that will be usefu- useful. I don't know. It's k- it's k- Uh-huh. It's quite a lot - It's a lot of work. Well, it's not too much, but this - it's work. And I want to know if - Uh-huh. Yeah. if we have some feeling that the result - I - I would like to know if - I don't have any feeling if this will work better than apply V_T_S aft- in cepstral domain will work better than apply in m- mel - in filter bank domain. I r- I'm not sure. I don't - I don't know absolutely nothing. Mm-hmm. Yeah. Well, you're - I think you're the first one here to work with V_T_S, so, uh, maybe we could call someone else up who has, ask them their opinion. Uh, I don't - I don't have a good feeling for it. Mm-hmm. Um. Pratibha. Actually, the V_T_S that you tested before was in the log domain and so the codebook is Yeah? e- e- kind of dependent on the level of the speech signal. And - So I expect it - If - if you have something that's independent of this, I expect it to - it - to, uh, be a better To have better - model of speech. And. Well. You - you wouldn't even need to switch to cepstra. Right? I mean, you can just sort of normalize the - No. We could normali- norm- I mean, remove the median. Yeah. Yeah. Mm-hmm. And then you have one number which is very dependent on the level cuz it is the level, and the other which isn't. Mm-hmm. Yeah. But here also we would have to be careful about removing the mean of speech and Ye- not of noise. Because it's like first doing general normalization and then Yea- noise removal, which is - Yeah. We - I was thinking to - to - to estimate the noise with the first frames and then apply the V_A_D, Mm-hmm. Mm-hmm. before the on-line normalization. We - we see - Mm-hmm. Well, I am thinking about that and working about that, but I don't have result this week. Yeah. Sure. I mean, one of the things we've talked about - maybe it might be star- time to start thinking about pretty soon, is as we look at the pros and cons of these different methods, how do they fit in with one another? Because we've talked about potentially doing some combination of a couple of them. Maybe - maybe pretty soon we'll have some sense of what their characteristics are, so we can see what Mm-hmm. should be combined. Mm-hmm. Is that it? O_K. Why don't we read some digits? O_K? Yep. Want to go ahead, Morgan? Sure. Transcript L_ dash two one two. Two, four eight eight, two nine, one zero zero, nine. Seven zero, six one, seven eight, two five, nine six. Eight five eight, seven seven nine, six one nine. Four eight four four, one, seven five two. Two four, six two, seven two, one eight, two one. Eight, five five two, three five, one two five, one. Five zero three eight, one, one seven seven. Six, nine seven four, nine six, three one one, five. Transcript L_ dash two one three. Two four, five four, four three, seven six, five seven. Three one eight, O_ two O_, nine nine seven. One, three one seven, three eight, eight three one, three. Zero seven one seven, two four one five, three nine eight five. Zero, nine eight nine, zero six, two one seven, three. Two nine, nine five, six nine, six six, nine nine. Eight five nine, nine four six, four six five one. Two four five, three seven five, nine nine three five. Transcript L_ dash two one four. Seven five three, nine five nine, one three five two. Five three five, one O_ one, O_ six seven. One seven zero, four three seven, one five five. Nine, two four six, eight one, O_ seven one, zero. One four seven, seven six, seven nine one five. Two one nine two, two zero nine five, six zero nine seven. Zero, two five zero, eight five, eight four zero, five. Three four t- two three, seven, four nine one. Transcript L_ dash two one five. Three five seven one, six five four five, nine nine one nine. Five five two, zero eight five, two two five. One three seven, nine eight eight, nine five eight. Zero five six, six five four, zero five eight nine. Three four five six, five, two six eight. Three zero one eight, two, eight six eight. Eight three, nine four, four zero, seven six, three three. Five zero seven, nine three two, zero three zero. Transcript L_ dash two one six. Zero two zero, one three, two nine five zero. Zero one three, eight zero, nine six two six. Zero two, seven four, four zero, seven five, nine one. Two four, seven two, two nine, three two, three three. Seven four five, one one, four eight one seven. Six, nine one eight, nine five, three nine six, four. Six six seven six, three, seven nine nine. Five one, seven zero, nine zero, seven eight, one one. Transcript L_ dash two one seven. Seven two three, nine five, four two one one. Nine, two three eight, five two, three six three, nine. Zero four seven, two one seven, one one nine. Three one five five, nine two six six, zero five eight four. Five nine eight eight, four eight three eight, six nine three four. Eight five one, one four nine, zero one seven. Zero three zero zero, six, five eight seven. Five three two, four two, five seven eight eight. Transcript L_ dash two one eight. Eight, eight nine six, O_ two, three six six, seven. Seven eight, O_ seven, one seven, two O_, three nine. O_ six four, three two two, eight five one. Five one five, five nine, nine nine six one. Four three seven, eight seven six, six eight five seven. O_ three six, nine two six, O_ two nine. Three eight one, five five, eight four three four. Four one five, five O_ one, five two eight. O_ K. 