O_K, we're on. O_K. So, I mean, everyone who's on the wireless check that they're on. Alright. C- we - I see. Yeah. Yeah. O_K, our agenda was quite short. Oh, could you close the door, maybe? Yeah. Sure. Two items, which was, uh, digits and possibly stuff on - on, uh, forced alignment, which Jane said that Liz and Andreas had in- information on, but they didn't, so. @@ Mm-hmm. I guess the only other thing, uh, for which I - We should do that second, because Liz might join us in time for that. O_K. Um. O_K, so there's digits, alignments, and, um, I guess the other thing, which I came unprepared for, uh, is, uh, to dis- s- s- see if there's anything anybody wants to discuss about the Saturday meeting. Right. So. Any - I mean, maybe not. Digits and alignments. But - Uh. Talk about aligning people's schedules. Yeah. Yeah. Mm-hmm. Yeah. I mean - Right. Yeah, I mean, it was - Yeah, it's forced alignment of people's schedules. Yeah. If we're very - Forced align. Yeah. Yeah. With - with - whatever it was, a month and a half or something ahead of time, the only time we could find in common - roughly in common, was on a Saturday. Yeah. Ugh. Yep. It's pretty sad. Yeah. Yeah. Have - Have we thought about having a conference call to include him in more of - in more of the meeting? I - I mean, I don't know, if we had the - if we had the telephone on the table - No. But, h- I mean, he probably has to go do something. Right? No, actually I - I have to - I have to shuttle kids from various places to various other places. I see. O_K. Yeah. So. And I don't have - and I don't, um, have a cell phone so I can't be having a conference call while driving. A cell phone? No. It's not good. That's not good. R- r- right. So we have to - we - Plus, it would make for interesting noise - background noise. @@ So we have to equip him with a - with a - with a head-mounted, uh, cell phone and - Yep. Uh - Ye- we- and we'd have to force you to read lots and lots of digits, so it could get real - real car noise. Oh, yeah. Oh, yeah. Take advantage. Yeah. And with the kids in the background. Yeah. I'll let - I'd let - I let, uh, my five-year-old have a try at the digits, eh. Yeah. So, anyway, I can talk about digits. Um, did everyone get the results or shall I go over them again? I mean that it was basically - the only thing that was even slightly surprising was that the lapel did so well. Um, and in retrospect that's not as surprising as maybe i- it shouldn't have been as surprising as I - as - as I felt it was. The lapel mike is a very high-quality microphone. And as Morgan pointed out, that there are actually some advantages to it in terms of breath noises and clothes rustling if no one else is talking. Exactly. Yeah. Mm-hmm. Um, so, uh - It's g- it - Well, it's - Yeah, sort of the bre- the breath noises and the mouth clicks and so forth like that, the lapel's gonna be better on. Or the cross-talk. Yeah. The lapel is typically worse on the - on clothes rustling, but if no one's rustling their clothes, Right. I mean, a lot of people are just sort of leaning over and reading the digits, so it's - it's a very it's - it's - different task than sort of the natural. So. Yeah. You don't move much during reading digits, I think. Yeah. Yeah. Right. Probably the fact that it picks up other people's speakers - other people's talking is an indication of that it - the fact it is a good microphone. Yeah. Right. Right. Right. So in the digits, in most - most cases, there weren't other people talking. So. So. D- do the lapel mikes have any directionality to them? There typically don't, no. Because I - I suppose you could make some that have sort of - that you have to orient towards your mouth, and then it would - They have a little bit, but they're not noise-cancelling. So, uh - They're - they're intended to be omni-directional. Right. Mm-hmm. And th- it's - and because you don't know how people are gonna put them on, you know. Right. So, also, Andreas, on that one the - the back part of it should be right against your head. And that will he- keep it from flopping aro- up and down as much. It is against my head. O_K. Yeah. Um. Yeah, we actually talked about this in the, uh, front-end meeting this morning, too. Uh-huh. Much the same thing, and - and it was - uh, I mean, there the point of interest to the group was primarily that, um, the, uh - the system that we had that was based on H_T_ K, that's used by, you know, all the participants in Aurora, Everybody. was so much worse than the - than the S_R_ I. And the interesting thing is that even though, yes, it's a digits task and that's a relatively small number of words and there's a bunch of digits that you train on, it's just not as good as having a - a l- very large amount of data and training up a - a - a nice good big H_M_M. Um, also you had the adaptation in the S_R_I system, which we didn't have in this. Um. So. Um. And we know - Di- did I send you some results without adaptation? No. Or if you did, I didn't include them, cuz it was - I- s- I think Stephane, uh, had seen them. So - Yeah, I think I did, actually. So there was a significant loss from not doing the adaptation. Yeah. Um. A - a - a couple percent or some- I mean - Well, I don't know it - Overall - Uh, I - I don't remember, but there was - there was a significant, um, loss or win from adaptation - with - with adaptation. And, um, that was the phone-loop adaptation. And then there was a very small - like point one percent on the natives - uh, win from doing, um, you know, adaptation to the recognition hypotheses. And I tried both means adaptation and means and variances, and the variances added another - or subtracted another point one percent. So, it's, um - that's the number there. Point six, I believe, is what you get with Right. both, uh, means and variance adaptation. But I think one thing is that, uh, I would presume - Hav- Have you ever t- Have you ever tried this exact same recognizer out on the actual T_I-digits test set? This exact same recognizer? No. It might be interesting to do that. Cuz my - my - cuz my sense, um - But - but, I have - I mean, people - people at S_R_I are actually working on digits. I bet it would do even slightly better. I could - and they are using a system that's, um - you know, h- is actually trained on digits, um, but h- h- otherwise uses the same, you know, decoder, the same, Mm-hmm. uh, training methods, and so forth, and I could ask them what they get on T_I-digits. Yeah, bu- although I'd be - I think it'd be interesting to just take this exact actual system so that these numbers were comparable and try it out on T_I-digits. Mm-hmm. Well, Adam knows how to run it, so you just Yeah. No problem. Yeah. Yeah. Yeah. Cuz our sense from the other - from the Aurora, uh, task is that - I mean, cuz we were getting sub one percent make a f- And try it with T_I-digits? Mm-hmm. Mm-hmm. numbers on T_I-digits also with the tandem thing. So, Mmm. one - so there were a number of things we noted from this. One is, yeah, the S_R_I system is a lot better than the H_T_K - Hmm. this, you know, very limited training H_T_K system. Mm-hmm. Uh, but the other is that, um, the digits recorded here in this room with these close mikes, i- uh, are actually a lot harder than the studio-recording T_I-digits. I think, you know, one reason for that, uh, might be that there's still - even though it's close-talking, there still is some noise and some room acoustics. Mm-hmm. Mm-hmm. And another might be that, uh, I'd - I would presume that in the studio, uh, uh, situation recording read speech that if somebody did something a little funny or n- pronounced something a little funny or made a little - that they didn't include it, they made them do it again. They didn't include it. Whereas, I took out the ones that I noticed that were blatant - that were correctable. Mmm. Yeah. So that, if someone just read the wrong digit, I corrected it. And then there was another one where Jose Yeah. couldn't tell whether - I couldn't tell whether he was saying zero or six. And I asked him and he couldn't tell either. Hmm. So I just cut it out. You know, so I just e- edited out the first, i- uh, word of the utterance. Yeah. Um, so there's a little bit of correction but it's definitely not as clean as T_I-digits. So my expectations is T_I-digits would, especially - I think T_I-digits is all American English. Right? So it would probably do even a little better still Mm-hmm. on the S_R_I system, but we could give it a try. Well. But remember, we're using a telephone bandwidth front-end here, uh, on this, uh - on this S_R_I system, so, um, I was - I thought that maybe that's actually a good thing because it - it gets rid of some of the - uh, the noises, um, you know, in the - the - below and above the - Mm-hmm. um, the, you know, speech bandwidth and, Mm-hmm. um, I suspect that to get sort of the last bit out of these higher-quality recordings you would have to in fact, uh, use models that, uh, were trained on wider-band data. And of course we can't do that or - Wha- what's T_I-digits? I thought t- It's wide-band, yeah. It's - in - in fact, we looked it up and it was actually twenty kilohertz sampling. It is wide-band. O_K. Oh, that's right. I - I did look that up. I couldn't remember whether that was T_I-digits or one of the other digit tasks. Mm-hmm. Yeah. Right. But - but, I would - Yeah. It's - it's easy enough to try, just run it on - Mm-hmm. So, Morgan, you're getting a little breath noise. You might wanna move the mike down a little bit. Yeah. See w- Now, eh, does - one - one issue - one issue with - with that is that um, the system has this, uh, notion of a speaker to - which is used in adaptation, variance norm- uh, you know, both in, uh, mean and variance normalization and also in Mm-hmm. the V_T_L estimation. So - Yeah, I noticed the script that extracted it. Do y- ? Is - ? So does - so th- so does - does, um, the T_I-digits database have speakers that are known? Yep. Yep. And is there - is there enough data or a comparable - comparable amount of data to - to what we have in our recordings here? That I don't know. I don't know. I don't know how many speakers there are, O_K. Yeah. and - and how many speakers per utterance. Well, the other thing would be to do it without the adaptation and compare to these numbers without the adaptation. That would - Right. Uh, but I'm not so much worried about the adaptation, actually, than - than the, um, um - the, uh, V_T_L estimation. If you have only one utterance per speaker you might actually screw up on estimating the - Right. the warping, uh, factor. So, um - I strongly suspect that they have more speakers than we do. Right. But it's not the amount of speakers, it's the num- it's the amount of data per speaker. So, uh - Right. So we - we could probably do an extraction that was roughly equivalent. Right. Right. So - Um. So, although I - I sort of know how to run it, there are a little - a f- few details here and there that I'll have to dig out. O_K. The key - So th- the system actually extracts the speaker I_D from the waveform names. Right. I saw that. And there's a - there's a script - and that is actually all in one script. So there's this one script that parses waveform names and extracts things like the, um, speaker, uh, I_D or something that can stand in as a speaker I_D. So, we might have to modify that script to recognize the, um, speakers, Right. um, in the - in the, uh, um, T_I-digits database. Right. And that, uh - Or you can fake - you can fake names for these waveforms that resemble the names that we use here for the - for the meetings. Right. That would be the, sort of - probably the safest way to do - I might have to do that anyway to - to do - because we may have to do an extract to get the amount of data per speaker about right. Uh-huh. The other thing is, isn't T_I-digits isolated digits? Right. Or is that another one? I'm - I looked through a bunch of the digits t- corp- corpora, and now they're all blurring. Mm-hmm. Cuz one of them was literally people reading a single digit. And then others were connected digits. Yeah. Most of T_I-digits is connected digits, I think. The - I mean, we had a Bellcore corpus that we were using. It was - O_K. Maybe it's the Bell Gram . that's - that was isolated digits. Bell Digits. Alright. Um. By the way, I think we can improve these numbers if we care to compr- improve them by, um, not starting with the Switchboard models but by taking the Switchboard models and doing supervised adaptation on a small amount of digit data collected Yep. in this setting. Because that would adapt your models to the room acoustics and f- for the far-field microphones, you know, to the noise. And that should really improve things, um, further. And then you use those adapted models, which are not speaker adapted but sort of acous- you know, channel adapted - Channel adapted. use that as the starting models for your speaker adaptation. Yeah. But the thing is, uh - I mean, w- when you - it depends whether you're ju- were just using this as a - a starter task for - you know, to get things going for conversational or if we're really interested i- in connected digits. And I - I think the answer is both. Well, I don't know. And for - for connected digits over the telephone you don't actually want to put a whole lot of effort into adaptation because somebody gets on the phone and says a number and then you just want it. This is - this - that one's better. Mm-hmm. You don't - don't, uh - Right. Um, but, you know, I - uh, my impression was that you were actually interested in the far-field microphone, uh, problem, I mean. So, you want to - you want to - That's the obvious thing to try. Right? Then, eh - because Oh. Oh. Right. you - you don't have any - That's where the most m- acoustic mismatch is between the currently used models and the - the r- the set up here. So. Yeah. Right. Yeah. So that'd be anoth- another interesting data point. I mean, I - I guess I'm saying I don't know if we'd want to do that as the - as - Mm-hmm. Other way. Liz - Other way. Now you're all watching me. Alright. This way. It f- it clips over your ears. There you go. If you have a strong fe- if you have a strong preference, you could use this. It's just we - we think it has some spikes. So, uh, we - we didn't use that one. You're all watching. This is terrible. I'll get it. But you could if you want. I don't know. And Andre- Andreas, your - your microphone's a little bit low. Yeah. At any rate, I don't know if w- It is? Uh. Yeah. I don't know if we wanna use that as the - Yeah. Uh, it pivots. So if you see the picture and then you have to scr- It - it - like this. I- I - I - I already adjusted this a number of times. I - I Eh. Yeah, I think these mikes are not working as well as I would like. can't quite seem to - Yeah, I think this contraption around your head is not working so well. Too many adju- too many adjustments. Yeah. Anyway, what I was saying is that I - I think I probably wouldn't want to see that as sort of like the norm, that we compared all things to. That looks good. Yeah. To, uh, the - to have - have all this ad- all this, uh, adaptation. But I think it's an important data point, if you're - if - Yeah. Right. Um. The other thing that - that, uh - of course, what Barry was looking at was - was just that, the near versus far. And, yeah, the adaptation would get Mm-hmm. th- some of that. But, I think even - even if there was, uh, only a factor of two or something, like I was saying in the email, I think that's - that's a big factor. So - Mm-hmm. N- Liz, you could also just use the other mike if you're having problems with that one. Well. O_K. Yeah. This would be O_K. We - we - we think that this has spikes on it, so it's not as good acoustically, but - It's this thing's - This is too big for my head. Yeah, basically your ears are too big. I mean, mine are too. No, my - my - But this is too big for my head. So, I mean, E- th- everybody's ears are too big for these things. Uh - it doesn't - you know, it's sit- Well, if you'd rather have this one then it's - O_K. Yeah. Oh, well. It's great. So the - To get that, uh, pivoted this way, it pivots like this. No this way. Yeah. Yeah. There you go. And there's a screw that you can tighten. And then it - Right. Right. I already tried to get it close. Good. So if it doesn't bounce around too much, that's actually good placement. O_K. That looks good. But it looks like it's gonna bounce a lot. So, where were we? Uh - Yeah. Yeah. Digits. Adaptation. Uh, adaptation, non-adaptation, um, factor of two, um - What k- u- By the way, wh- what factor of two did you - ? I mean - Oh, yeah. I know what I was go- w- Oh, no, no. It's tha- that - that we were saying, you know, well is - how much worse is far than near, you know. And I mean it depends on which one you're looking at, but for the everybody, it's Oh, th- O_K. That factor of two. Mm-hmm. little under a factor or two. Yeah. I - I know what I was thinking was that maybe, uh, i- i- we could actually t- t- try at least looking at, uh, some of the - the large vocabulary speech from a far microphone, at least from the good one. Mm-hmm. I mean, before I thought we'd get, you know, a hundred and fifty percent error or something, but if - Mm-hmm. if, uh - if we're getting thirty-five, forty percent or something, Actually if you run, u- um - though, on a close-talking mike over the whole meeting, during all those silences, you get, like, four hundred percent word error. Mm-hmm. Right. I understand. But doing the same kind of limited thing - Or - or some high number. Yeah, sure. Get all these insertions. But I'm saying if you do the same kind of limited thing Yeah. as people have done in Switchboard evaluations or as - a- Where you know who the speaker is and there's no overlap? Yeah. And you do just the far-field for those regions? Yeah. The same sort of numbers that we got those graphs from. Right? Could we do exactly the same thing that we're doing now, but do it with a far-field mike? Yeah, do it with one of - on- Cuz we extract the times from the near-field mike, but you use the acoustics from the far-field mike. Right. I understand that. I just meant that - so you have three choices. There's, um - You can use times where that person is talking only from the transcripts but the segmentations were - were synchronized. Or you can do a forced alignment on the close-talking to determine that, the- you know, within this segment, these really were the times that this person was talking and elsewhere in the segment other people are overlapping and just front-end those pieces. Or you can run it on the whole data, which is - But - but - but how did we get the - how did we determine which is, you know, a - the links, uh, that we're testing on in the stuff we reported? In the H_L_ T paper we took segments that are channel - time-aligned, which is now h- being changed in the transcription process, which is good, and we took cases where the transcribers said there was only one person talking here, because no one else had time - any words in that segment and called that "non-overlap". And tha- And that's what we were getting those numbers from. Right. Yes. Tho- good - the good numbers. The bad numbers were from the segments where there was overlap. Well, we could start with the good ones. But anyway - so I think that we should try it once with Yeah. the same conditions that were used to create those, and in those same segments just use one of the P_Z_Ms. Right. So we - we can do that. Yeah. And then, you know, I mean, the thing is if we were getting, uh - what, thirty-five, forty percent, something like that on - on that particular set, uh, does it go to seventy or eighty? Or, does it use up so much memory we can't decode it? Right. It might also depend on which speaker th- it is and how close they are to the P_Z_M? I don't know how different they are from each other. Uh - You want to probably choose the P_Z_M channel that is closest to the speaker. To be best - For this particular digit ones, I just picked that one. Yeah. f- Well - O_K. So we would then use that one, too, or - ? Oh, O_K. So - This is kind of central. You know, it's - so i- but I would - I'd pick that one. It'll be less good for some people than for other, but I - I'd like to see it on the same - exact same data set that - that we did the other thing on. Right? Actually - I sh- actually should've picked a different one, because that could be why the P_D_A is worse. Because it's further away from most of the people reading digits. It's further away. Yeah. Yeah. That's probably one of the reasons. Hmm. Mm-hmm. Well, yeah. You could look at, I guess, that P_Z_M or something. Yep. But the other is, it's very, uh - I mean, even though there's - I'm sure the f- f- the - the S_R_I, uh, front-end has some kind of pre-emphasis, it's - it's, uh - Mm-hmm. still, th- it's picking up lots of low-frequency energy. So, even discriminating against it, I'm sure some of it's getting through. Um. But, yeah, you're right. Prob- a part of it is just the distance. And aren't these pretty bad microphones? Well, they're bad. Yep. I mean - But, I mean, if you listen to it, it sounds O_K. You know? Yeah. When you listen to it, u- Yeah. uh, the P_Z_M and the P_D_A - Yeah, th- the P_D_A has higher sound floor but not by a lot. It's really pretty - uh, pretty much the same. I just remember you saying you got them to be cheap on purpose. Cheap in terms of their quality. So. Well, they're twenty-five cents or so. Yeah. Th- we wanted them to be - to be typical of what would be in a P_D_A. So they are - Mm-hmm. they're not the P_Z_M three hundred dollar type. They're the twenty-five cent, Yeah. buy them in packs of thousand type. I see. But, I mean, the thing is people use those little mikes for everything because they're really not bad. Everything. I mean, if you're not Mm-hmm. doing something ridiculous like feeding it to a speech recognizer, they - they - they - you know, you can hear the sou- hear the sounds just fine. You know, it's - Right. They - I mean, i- it's more or less the same principles as these other mikes are built under, it's just that there's less quality control. They just, you know, churn them out and don't check them. Um. So. So that was - Yeah. So that was i- interesting result. So like I said, the front-end guys are very much interested in - in this is as - as well and So - so, but where is this now? I mean, what's - where do we go from here? I mean, Yeah. That was gonna be my question. we - so we have a - we have a - a system that works pretty well but it's not, you know, the system that people here are used to using - to working with. So what - what do we do now? Well, I think what we wanna do is we want to - eh, and we've talked about this in other contexts - we want to have the ability to feed it different features. Mm-hmm. O_K. And then, um, from the point of view of the front-end research, it would be s- uh, substituting for H_T_K. O_K. I think that's the key thing. And then if we can feed it different features, then we can try all the different things that we're trying there. O_K. Alright. And then, um, uh, also Dave is - is thinking about using the data in different ways, uh, to Mm-hmm. um, uh, explicitly work on reverberation starting with some techniques that some other people have found somewhat useful, and - Yeah. O_K. So - so the key thing that's missing here is basically the ability to feed, you know, other features i- into the recognizer and also then to train the system. Right. Right. O_K. And, uh, es- I don't know when Chuck will be back but that's exactly what he - he's gonna - H- h- He's - he's sort of back, but he drove for fourteen hours an- and wasn't gonna make it in today. Oh, O_K. So, I think that's one of the things that he said he would be working on. Um. Just sort of t- to make sure that we can do that and - Yeah. Yeah. Um. Right. It's - uh, I mean, the - the front-end is f- i- tha- that's in the S_R_I recognizer is very nice in that it does a lot of things on the fly but it unfortunately is not designed and, um - like the, uh, ICSI system is, where you can feed it from a pipeline of - of the command. So, the - what that means probably for the foreseeable future is that you have to, uh, dump out, um - you know, if you want to use some new features, you have to dump them into individual files and give those files to the recognizer. We do - we tend to do that anyway. O_K. Oh. So, although you - you can pipe it as well, we tend to do it that way because that way you can concentrate on one block and not keep re-doing it over and over. Oh, O_K. Alright. Yeah. Yeah. So I've - I - So tha- that's exactly what the P_file is for. Yeah. Yeah, the - the - the cumbersome thing is - is, um - is that you actually have to dump out little - little files. So for each segment that you want to recognize you have to dump out a separate file. Uh - Uh-huh. Just like i- th- like th- as if there were these waveform segments, but instead you have sort of feature file segments. But, you know - So. Cool. O_K. So the s- the - the next thing we had on the agenda was something about alignments? Oh. Yes, we have - I don't know, did you wanna talk about it, or - ? I can give a - I was just telling this to Jane and - and - W- we - we were able to get some definite improvement on the forced alignments by looking at them first and then realizing the kinds of errors that were occurring and um, some of the errors occurring very frequently are just things like the first word being moved to as early as possible in the recognition, which is a um, I think was both a - a pruning problem and possibly a problem with needing constraints on word locations. And so we tried both of these st- things. We tried saying - I don't know, I got this whacky idea that - just from looking at the data, that when people talk their words are usually chunked together. It's not that they say one word and then there's a bunch of words together. They're might say one word and then another word far away if they were doing just backchannels? But in general, if there's, like, five or six words and one word's far away from it, that's probably wrong on average. So, um - And then also, ca- the pruning, of course, was too - too severe. So that's actually interesting. The pruning was the same value that we used for recognition. And we had lowered that - we had used tighter pruning after Liz ran some experiments showing that, you know, it runs slower and there's no real difference in - No gain. Actually it was better with - slightly better or about th- it was the same with tighter pruning. Right. So for free recognition, this - the lower pruning value is better. You - It's probably cuz the recognition's just bad en- at a point where it's bad enough that - that you don't lose anything. Correct. Right. Um, but it turned out for - for - to get accurate alignments it was really important to open up the pruning Right. significantly. Hmm. Um because otherwise it would sort of do greedy alignment, um, in regions where there was no real speech yet from the foreground speaker. Mm-hmm. Um, so that was one big factor that helped improve things and then the other thing was that, you know, as Liz said the - we f- enforce the fact that, uh, the foreground speech has to be continuous. It cannot be - you cannot have a background speech hypothesis in the middle of the foreground speech. You can only have background speech at the beginning and the end. Yeah. I mean, yeah, it isn't always true, and I think what we really want is some clever way to do this, where, um, you know, from the data or from maybe some hand-corrected alignments from transcribers that things like words that do occur just by themselves a- alone, like backchannels or something that we did allow to have background speech around it - Yeah. those would be able to do that, but the rest would be constrained. Sorry. So, I think we have a version that's pretty good for the native speakers. I don't know yet about the non-native speakers. And, um, we basically also made noise models for the different - sort of grouped some of the mouth noises together. Um, so, and then there's a background speech model. And we also - There was some neat - or, interesting cases, like there's one meeting where, um, Jose's giving a presentation and he's talking about, um, the word "mixed signal" and someone didn't understand, uh, that you were saying " mixed " - I think, Morgan. Yeah, yeah. And so your speech -ch was s- saying something about mixed signal. And the next turn was a lot of people saying "mixed", like "he means mixed signal" or "I think it's mixed". And the word "mixed" in this segment occurs, like, a bunch of times. Sh- And Chuck's on the lapel here, and he also says "mixed" but it's at the last one, and of course the aligner th- aligns it everywhere else to everybody else's "mixed", Yeah. cuz there's no adaptation yet. So there's - I think there's some issues about - u- We probably want to adapt at least the foreground speaker. But, I guess Andreas tried adapting both the foreground and a background generic speaker, and that's actually a little bit of a f- funky model. Like, it gives you some weird alignments, just because often the background speakers match better to the foreground than the foreground speaker. So there's some things there, especially when you get lots of the same words, Oh - Yeah. Oh. uh, occurring in the - Well, the - I - I think you can do better by uh, cloning - so we have a reject phone. And you - and what we wanted to try with - you know, once we have this paper written and have a little more time, uh, t- cloning that reject model and then one copy of it would be adapted to the foreground speaker to capture the rejects in the foreground, like fragments and stuff, and the other copy would be adapted to the background speaker. Right. I mean, in general we actually - And - Right now the words like partial words are reject models Mm-hmm. and you normally allow those to match to any word. But then the background speech was also a reject model, and so this constraint of not allowing rejects in between - you know, it needs to differentiate between the two. So just sort of working through a bunch of Right. debugging kinds of issues. And another one is turns, like people starting with "well I think" and someone else is "well how about". So the word "well" is in this - in this segment multiple times, and as soon as it occurs usually the aligner will try to align it to the first person who says it. But then that constraint of sort of - uh, proximity constraint will push it over to the person who really said it in general. Is the proximity constraint a hard constraint, or did you do some sort of probabilistic weighting distance, or - ? We - we didn't - No. We - w- Right now it's a kluge. O_K. We - it's straightforward to actually just have a - a penalty that doesn't completely disallows it but discourages it. But, um, we just didn't have time to play with, you know, tuning yet another - yet another parameter. The ve- level. Yeah. Yeah. And really the reason we can't do it is just that we don't have a - we don't have ground truth for these. So, we would need a hand-marked, um, word-level alignments or at least sort of the boundaries of the speech betw- you know, between the speakers. Um, and then use that as a reference and tune the parameters of the - of the model, uh, to op- to get the best performance. Yeah. G- given - I - I mean, I wa- I wa- I was gonna ask you anyway, uh, how you assessed that things were better. Mm-hmm. I looked at them. I spent two days - um, in Waves - O_K. Oh, it was painful because the thing is, you know the alignments share a lot in common, so - And you're - yo- you're looking at these segments where there's a lot of speech. I mean, a lot of them have a lot of words. Yeah. Not by every speaker but by some speaker there's a lot of words. Yeah. Ju- No, not - I mean that if you look at the individual Yeah. segments from just one person you don't see a lot of words, but altogether you'll see a lot of words up there. And so the reject is also mapping and pauses - Mm-hmm. Yeah. Yeah. So I looked at them all in Waves and just lined up all the alignments, and, at first it sort of looked like a mess and then the more I looked at it, I thought "O_K, well it's moving these words leftward and -" You know, it wasn't that bad. It was just doing certain things wrong. So - But, I don't, you know, have time to l- to look at all of them and it would be really useful to have, like, a - a transcriber who could use Waves, um, just mark, like, the beginning and end of the foreground speaker's real words - like, the beginning of the first word, the end of the last word - and then we could, Yeah. I - O_K. I have to ask you something, which is, first of all, um, you know, do some adjustments. is i- does it have to be Waves? Because if we could benefit from what you did, incorporate that into the present transcripts, that would help. No. And then, um, the other thing is, I believe that I did hand- So. One of these transcripts was gone over by a transcriber and then I hand-marked it myself so that we do have, uh, the beginning and ending of individual utterances. Mm-hmm. Um, I didn't do it word level, but - but in terms - So I - so for - for one of the N_S_ A groups. And also I went back to the original one that I first transcribed and - and did it Mm-hmm. w- uh, w- uh, utterance by utterance for that particular one. So I think you do have - if that's a sufficient unit, I think that you do have hand-marking for that. But it'd be wonderful to be able to benefit from your Waves stuff. Mm-hmm. We don't care what - what tool you use. O_K. I used it in Transcriber and it's - it's in the - Yeah. I mean, if - if you can, um - if you wanna - well, Jane and I were - U- uh - just in terms of the tool, talking about this. I guess Sue had had some reactions. You know, interface-wise if you're looking at speech, you wanna be able to know really where the words are. And so, Yeah, that's right. Middle of the word, or - we can give you some examples of sort of what this output looks like, um, and see if you can in- maybe incorporate it into the Transcriber tool some way, or - Well, I th- I'm thinking just ch- e- e- incorporating it into the representation. I mean, if it's - if it's - Um. You mean like - Yeah, word start insights. if you have start points, if you have, like, time tags, which is what I assume. Isn't that what - what you - ? Well, see, Adam would be - Right. Yeah, whatever you use. I mean, we convert it to this format Yeah. that the, um, NIST scoring tool unders- uh, C_T_M. Conversation Time-Marked file. And - and then that's the - that's what the - I think Transcriber, uh, outputs C_T_M. If it - ? O_K. So you would know this more than I would. I think so. Yeah. So, I mean - Right. It seems like she - if she's g- if she's moving time marks around, since our representation in Transcriber uses time marks, it seems like there should be some way of - of using that - benefitting from that. Right. Yeah, it wou- the advantage would just be that when you brought up a bin you would be able - if you were zoomed in enough in Transcriber to see all the words, you would be able to, like, have the words sort of located in time, Mm-hmm. if you wanted to do that. So. So - so if we e- e- even just had a - a - It sounds like w- we - we almost do. Uh, if we - We have two. Yeah. We have two. Just ha- uh, trying out the alignment Mm-hmm. procedure that you have on that you could actually get something, um - uh, uh, get an objective measure. Mm-hmm. Uh - You mean on - on the hand-marked, um - So we - we only r- hav- I only looked at actually alignments from one meeting that we chose, I think M_R four, just randomly, um - Yeah. And - Actually, not randomly. We knew - we knew that it had these insertion errors from - Yeah. Not randomly - It had sort of average recognition performance in a bunch of speakers and Yeah. it was a Meeting Recorder meeting. Um. But, yeah, we should try to use what you have. I did re-run recognition on your new version of Oh, good. Good! M_R one. I - I mean the - the one with Dan Ellis in it and Eric. Uh-huh. Yeah, exactly. Yeah. Yeah. I don't think that was the new version. Um - That - Yeah, actually it wasn't the new new, it was the medium new. But - but we would - we should do the - the latest version. It was the one from last week. O_K. O_K. Yeah. You - did you adjust the - the utterance times, um, for each channel? Yes. Yes, I did. And furthermore, I found that there were a certain number where - not - not a lot, but several times I actually moved an utterance from Adam's channel to Dan's or from Dan's to Adam's. So there was some speaker identif- And the reason was because I transcribed that at a point before - uh, before we had the multiple audio available f- so I couldn't switch between the audio. I - I transcribed it off of the mixed channel entirely, which meant in overlaps, I was at a - at a terrific disadvantage. Right. Right. In addition it was before the channelized, uh, possibility was there. And finally I did it using the speakers of my, um - of - you know, off the C_P_U on my - on my machine cuz I didn't have a headphone. So it @@ , like, I mean - Right. Yeah, I - I mean, i- in retrospect it would've been good to ha- have got- I should've gotten a headphone. But in any case, um, thi- this is - this was transcribed in a - in a, uh, less optimal way than - than the ones that came after it, and I was able to - you know, an- and this meant that there were some speaker identif- identifications which were Well, I know there were some speaker labeling problems, um, after interruptions. Is that what you're referring to? changes. Yeah. Fixed that. Oh, well - I mean, cuz there's this one instance when, for example, you're running down the stairs. I remember this meeting really well. Yeah. Don - Don has had - He knows - he can just read it like a play. Right. It's a - Yeah, I've - I've - I'm very well acquainted with this meeting. Yeah. Yeah, I can s- "And then she said, and then he said." Yeah, I know it by heart. So, um, there's one point when you're running down the stairs. Right? And, like, there's an interruption. Uh-oh. You interrupt somebody, but then there's no line after that. For example, there's no speaker identification after that line. Uh-huh. Is that what you're talking about? Or were there mislabelings as far as, like, the a- Adam was - ? That was fixed, um, before - i- i- i- I think I- I think I understood that pretty - Thank you for mentioning. Yeah, no, tha- that - Yeah. Cuz I thought I let you know about that. Yeah. That I think went away a couple of versions ago, but it's good to know. O_K. But you're actually saying that certain, uh, speakers were mis- mis-identified. Yeah. So, with - under - um, uh, listening to the mixed channel, there were times when, as surprising as that is, I got Adam's voice confused with Dan's and vice versa - not for long utterances, but jus- just a couple of places, O_K. O_K. Yeah. and embedde- embedded in overlaps. The other thing that was w- interesting to me was Mm-hmm. that I picked up a lot of, um, backchannels which were hidden in the mixed signal, which, you know, I mean, you c- not - not too surprising. Right. But the other thing that - I - I hadn't thought about this, but I thou- I wanted to raise this when you were - uh, with respect to also a strategy which might help with the alignments potentially, but that's - When I was looking at these backchannels, they were turning up usually - very often in - w- well, I won't say "usually" - but anyway, very often, I picked them up in a channel w- which was the person who had asked a question. S- so, like, someone says "an- and have you done the so-and-so?" And then there would be backchannels, but it would be the person who asked the question. Other people weren't really doing much backchanneling. And, you know, sometimes you have the - Yeah, uh-huh. Well, that's interesting. I mean, i- it wouldn't be perfect, but - but it does seem more natural to give a backchannel when - Yeah. No, that's really interesting. when you're somehow involved in the topic, and the most natural way is for you to have initiated the topic by asking a question. Mm-hmm. Well, That's interesting. I think - No. I think it's - actually I think what's going on is backchanneling is something that happens in two-party conversations. And if you ask someone a question, you essentially initiating a little two-party conversation. Mm-hmm. Yeah. Well, actu- Yeah, when we looked at this - So then you're - so and then you're expected to backchannel because the person is addressing you directly and not everybody. Exactly. Exactly. Exactly my point. An- and so this is the expectation thing that - uh, uh, just the dyadic - But in addition, Yeah. Yeah. Right. Mm-hmm. Right. H- you know, if someone has done this analysis himself and isn't involved in the dyad, but they might also give backchannels to verify what - what the answer is that this - that the - the answerer's given - I tell you, I say - I say "uh-huh" a lot, Right. It's - There you go. Well, but it's interesting cuz, uh - while people are talking to each other. There you go. Yeah. Yeah. But there are fewer - I think there are fewer "uh-huhs". I mean, just from - We were looking at word frequency lists to try to find the cases that we would allow to have the reject words in between in doing the alignment. You know the ones we wouldn't constrain to be next to the other words. Oh, yeah. And "uh-huh" is not as frequent as it sort of would be in Switchboard, if you looked at just a word frequency list of one-word short utterances. And " yeah " is way up there, but not "uh-huh". And so I was thinking thi- it's not like you're being encouraged by everybody else to keep talking in the meeting. And uh, that's all, I- I'll stop there, cuz I- I think what you say makes a lot of sense. But it was sort of - Well, that's right. And that would - Well, an- And what you say is the - is the re- uh, o- other side of this, which is that, you know, so th- there are lots of channels where you don't have these backchannels, w- when a question has been asked and - and these - Right. There's just probably less backchanneling Mm-hmm. So that's good news, really. in general, even if you consider every other person altogether one person in the meeting, but we'll find out anyway. We were - I guess the other thing we're - we're - I should say is that we're gonna, um try - compare this type of overlap analysis to Switchboard, And CallHome. where - and CallHome, where we have both sides, so that we can try to answer this question of, you know, Mm-hmm. Mm-hmm. is there really more overlap in meetings or is it just because we don't have the other channel in Switchboard and we don't know what people are doing. Try to create a paper out of that. Yeah. I mean, y- y- you folks have probably already told me, but were - were you intending to do a Eurospeech submission, or - ? Um, you mean the one due tomorrow? Yeah. Yeah. Well, we're still, like, writing the scripts for doing the research, and we will - Yes, we're gonna try. Mm-hmm. And I was telling Don, do not take this as an example of how people should work. Do as I say, don't do as I do. Yeah. That's r- So, we will try. It'll probably be a little late, but I'm gonna try it. Well - It is different. In previous years, Eurospeech only had the abstract due by now, not the full paper. Right. Right. And so all our timing was off. I've given up on trying to do digits. I just don't think that what I have so far makes a Eurospeech paper. Well, I'm no- We may be in the same position, and I figured we'll try, because that'll at least get us to the point where we have - We have this really nice database format that Andreas and I were working out that - It - it's not very fancy. It's just a ASCII line by line format, but it does give you information - It's the - it's the spurt format. It - Yeah, we're calling these "spurts" after Chafe. I was trying to find what's a word for a continuous region with pauses around it? Hmm. Yeah. I know that th- the Telecom people use - use "spurt" for that. Yes. Good. They do? Oh! Oh. I would jus- Oh. And that's - I mean, I - I was using that for a while when I was doing the rate of speech stuff, because I - because I looked up in some books and I found - O_K, I wanna find a spurt Ah, right! It's just, like, defined by the acoustics. in which - and - an- because - cuz it's another question about how many pauses they put in between them. But how fast do they Horrible. Right. do the words within the spurt? Yeah. you know " Burst " also? Isn't "burst" is used also? Right. Well, that's what we were calling It's gonna - Burst. spurt, so - Spurt has the horrible name overloading with other - with hardware at ICSI. Here @@ - Here. Just very locally, yeah. But - but that just - Well, well, Chafe had this wor- I think it was Chafe, or somebody had a - the word "spurt" originally, and so I - Actually - But tha- that's good to know. Was thi- it's Chafe? Well, see, I know S- Sue wrote about spurts of development. But, in any case, I think it's a good term, and, uh - So maybe we should talk - Maybe it was Sue - ? Y- Hmm! So we have spurts and we have spurt-ify dot shell and spurt-ify Yeah. And ma- maybe - maybe Chafe did. I know - I know Ch- Chafe dealt with - Yeah. Uh. So s- That's cool. And then it's got all - it's a verb now. W- uh, w- Chafe speaks about intonation units. Yes. Right. But maybe he speaks about spurts as well and I just don't know. Yeah, go ahead. We- So what we're doing - uh, this - this is just - maybe someone has s- some - some ideas about how to do it better, but we - I've heard "burst" also. Mmm. So we're taking these, uh, alignments from the individual channels. We're - from each alignment we're producing, uh, one of these C_T_M files, which essentially has - it's just a linear sequence of words with the begin times for every word and the duration. Great. And - and - and of course - It looks like a Waves label file almost. Right? It's just - Right. But it has - one - the first column has the meeting name, so it could actually contain several meetings. Um. And the second column is the channel. Third column is the, um, start times of the words and the fourth column is the duration of the words. And then we're, um - O_K. Then we have a messy alignment process where we actually insert into the sequence of words the, uh, tags for, like, where - where sentence - ends of sentence, question marks, um, various other things. Uh. Yeah. These are things that we had Don - So, Don sort of, Right. um, propagated the punctuation from the original transcriber - so whether it was, like, question mark or period or, um, you know, comma and things like that, and we kept the - and disfluency dashes - Mm-hmm. uh, kept those in because we sort of wanna know where those are relative to the spurt overlaps - sp- overlaps, or - Right. So - so those are actually sort of retro-fitted into the time alignment. And then we merge all the alignments from the various channels and we sort them by time. And then there's a - then there's a process where you now determine the spurts. That is - Actually, no, you do that before you merge the various channels. So you - you id- identify by some criterion, which is pause length - you identify the beginnings and ends of these spurts, and you put another set of tags in there to keep those straight. Mm-hmm. And then you merge everything in terms of, you know, linearizing the sequence based on the time marks. And then you extract the individual channels again, but this time you know where the other people start and end talking - you know, where their spurts start and end. And so you extract the individual channels, uh, one sp- spurt by spurt as it were. Um, and inside the words or between the words you now have begin and end tags for overlaps. So, you - you basically have everything sort of lined up and in a form where you can look at the individual speakers and how their speech relates to the other speakers' speech. Right. Uh, I mean, I think that's actually really u- useful also because And - even if you weren't studying overlaps, if you wanna get a transcription for the far-field mikes, how are you gonna know which words from which speakers occurred at which times relative to each other? You have to be able to get a transcript like - like this anyway, just for doing far-field recognition. So, you know, Yeah. it's - it's sort of - I thi- it's just an issue we haven't dealt with before, how you time-align things that are overlapping anyway. That's wonderful. So - Well - And - and we - I mean, i- I never thought about it before, but - Y- yes. I mean, s- when I came up with the original data - suggested data format based on the transcription graph, there's capability of doing In - that sort of thing in there. Right. But you can't get it directly from the transcription. Yeah, this is like a poor man's ver- formatting version. But it's, you know - It's clean, it's just not fancy. Mm-hmm. Yeah, that's right. Right. Well, this is - this is just - Well, there's lots of little things. It's like there're twelve different scripts which you run and then at the end you have what you want. But, um, Right. Um. at the very last stage we throw away the actual time information. All we care about is whether - that there's a certain word was overlapped by someone else's word. So you sort of - at that point, you discretize things into just having overlap or no overlap. Because we figure that's about the level of analysis that we want to do for this paper. Mm-hmm. But if you wanted to do a more fine-grained analysis and say, you know, how far into the word is the overlap, you could do that. It's just - it'll just require more - Yeah. Just sort of huge. you know, slightly different - What's interesting is it's exactly what, um, i- in discussing with, um, Sue about this, um, she, um, Yeah. i- i- i- indicated that that - you know, that's very important for overlap analysis. Yeah. It's - it's nice to know, and also I think as a human, like, I don't always hear these in the actual order that they occur. So I can have two foreground speakers, you know, Morgan an- and Right. um, Adam and Jane could all be talking, and I could align each of them to be starting their utterance at the correct time, and then look where they are relative to each other, and that's not really what I heard. And that's another thing she said. This is - This is Bever's - Bever's effect, when - where - Cuz it's just hard to do. Y- Yeah. It's sort of - In psy- ps- psycho-linguistics you have these experiments where people have perceptual biases a- as to what they hear, that - that - Yeah, you sort of move things around until you get to a low information point and yo- then you can bring in the other person. So it's Not the best - actually not even possible, I think, for any person to listen to a mixed signal, even equalize, and make sure that they have all the words in the right order. Mm-hmm. So, I guess, we'll try to write this Eurospeech paper. I mean, we will write it. Whether they accept it late or not, I don't know. Superb. Um, and the good thing is that we have - It's sort of a beginning of what Don can use to link the prosodic features from each file to each other. Yeah. Yeah. That's the good thing about these pape- Hmm? Plus, mayb- So. i- You know, might as well. We- I ju- Otherwise we won't get the work done I don't know, m- I mean, u- u- Jane likes to look at data. Maybe, you know, you could - you could look at this format and see if you find anything interesting. on our deadline. Yeah. I don't know. Yeah. No, it's - that's the good thing about these pape- paper deadlines and, uh, you know, class projects, and - and things like that, because you - you really get g- Yeah. Well, what I'm thinking is - Yeah. Yeah. Yeah. Right. Well, my - Mm-hmm. Well th- th- the other thing that - that - that yo- that you usually don't tell your graduate students is that these deadlines are actually not that, Yeah. Forces you to do the work. Exactly. um, you know, strictly enforced, because the - Strict. @@ Oh, now it's out in the public, this - this - this secret information. Yeah. I think we can ha- Right. So - because - bec- b- Nah - No. No. Nah. i- Because these - the conference organizers actually have an interest in getting lots of submissions. I mean, a - a monetary interest. So - Right. Right. Yeah. Um. Th- that's - that's true. And good ones, good ones, which sometimes means a little extra time. And good submission- Right. Well - That's - That's true. That's another issue, but - By th- by the way, this is totally unfair, you may - you may feel, but the - the, uh - the morning meeting folks actually have an - an extra month or so. Mm-hmm. Yep. Yep. The Aurora - there's a special Aurora - When - Uh - There's a special Aurora session and the Aurora pe- people involved in Aurora have till Ma- uh, early May or something to turn in their paper. Oh. Mmm. Oh. Mmm. Well, then you can just - Oh, well maybe we'll submit to s- Actually - Maybe you can submit the digits paper on e- for the Aurora session. Yeah. Yeah. Yeah. Oh, I could! I could submit that to Aurora. That would be pretty - pretty - I- if it w- Yeah. Well - Yeah. i- it has - Yeah. S- That wouldn't work. It's not Aurora. @@ No, it wouldn't work. It's - it's not the Aurora - I mean, it - it's - it's actually the Aurora task. Maybe they'll get s- Aurora's very specific. It- Well, maybe it won't be after this deadline extension. Maybe they'll - But - but the people - I mean, a - a paper that is not on Aurora would probably be more interesting at that point because everybody's so sick and tired of the Aurora task. Yeah. Oh, I thought you meant this was just the digits section. I didn't know you meant it was Aurora digits. Yeah. Well, no. If you - if you have - it's to - if you discuss some relation to the Aurora task, like if you use the same - This is not the Aurora task. So they just do a little grep for - Do - uh, d- d- Um. Do not - do not - we are not setting a good example. This is not a - Well, a relation other than negation, maybe, um. So. I don't know. Anyway. But the good thing is this does - Well, I- I don't know. I mean, you could - you could do a paper on what's wrong with the Aurora task by comparing it to other ways of doing it. How well does an Aurora system do on - on - you know, on digits collected in a - in this environment? Yeah. @@ Different way. Yeah. Maybe. Maybe. Pretty hokey. I think it's a littl- little far-fetched. Nah, I mean, the thing is Aurora's pretty closed community. I mean, you know, the people who were involved in the - Yep. Mm-hmm. the only people who are allowed to test on that are people who - who made it above a certain threshold in the first round, It's very specific. uh w- in ninety-nine and it's - it's sort of a - it's - not like a - Well, that's maybe why they don't f- know that they have a crummy system. I mean, a crummy back-end. No, I mean - I mean, seriously, if you - if you have a very - No, I'm sorry. Uh, "beep" "bee-" No. I didn't mean I mean, th- anybody - any particular system. I meant this H_T_ K back-end. If they - Oh, you don't like H_T_K? Yeah. I don't h- I don't have any stock in H_T_K or Entropic or anything. No. I mean, this - it- it's the H_T_K that is trained on a very limited amount of data. Yeah. It's d- it's very specific. Right. But so, if you - But maybe you should, you know, consider more - using more data, or - I mean - Oh, yeah. I - I really think that that's true. And they i- i- If yo- if you sort of hermetically stay within one task and don't look left and right, then you're gonna - But they - they had - i- But - They had something very specific in mind when they designed it. Right. Well, u- i- Right? And so - so you can - you can argue about maybe that wasn't the right thing to do, but, you know, they - they - But, one of the reasons I have Chuck's messing around with - with the back-end that you're not supposed to touch - they had something specific. Mm-hmm. I mean, for the evaluations, yes, we'll run a version that hasn't been touched. Mm-hmm. But, uh, one of the reasons I have him messing around with that, because I think it's sort of an open question that we don't know the answer to. People always say very glibly that i- if you s- show improvement on a bad system, that doesn't mean anything, cuz it may not be - show - uh, because, you know, it doesn't tell you anything about the good system. Mm-hmm. And I - I've always sort of felt that that depends. You know, that if some peopl- If you're actually are getting at something that has some conceptual substance to it, it will port. Mm-hmm. And in fact, most methods that people now use were originally tried with something that was not their absolute best system at some level. But of course, sometimes it doesn't, uh, port. So I think that's - that's an interesting question. If we're getting three percent error on, uh, u- uh, English, uh, nati- native speakers, um, using the Aurora system, and we do some improvements and bring it from three to two, do those same improvements bring, uh, th- you know, the S_R_I system from one point three to - you know, to Hmm. Mm-hmm. Zero. point eight? Well. You know, so that's - that's something we can test. Mmm. Right. So. Anyway. O_K. I think we've - we've covered that one up extremely well. Mm-hmm. Whew! O_K. So, um - Yeah. So tha- so we'll - you know, maybe you guys'll have - have one. Uh, you - you and, uh - and Dan have - have a paper that - that's going in. You know, that's - that's pretty solid, on the segmentation stuff. Yeah. Yeah. Yeah. I will send you the - the final version, which is not - Yeah. And the Aurora folks here will - will definitely get something in on Aurora, so. Actually this - this, um - So, there's another paper. It's a Eurospeech paper but not related to meetings. But it's on digits. So, um, uh, a colleague at S_R_I developed a improved version of M_M_I_E training. Uh-huh. And he tested it mostly on digits because it's sort of a - you know, it doesn't take weeks to train it. Right. Um. And got some very impressive results, um, with, you know, discriminative, uh, Gaussian training. Um, you know, like, um, error rates go from - I don't know, in very noisy environment, like from, uh, uh - I for- now I - O_K, now I have the order of magnit- I'm not sure about the order of magnitude. Was it like from ten percent to eight percent or from e- e- you know, point - you know, from one percent to point eight percent? I mean, it's a - H- i- it got - it got better. Yeah, yeah. It got better. That's the important thing. Yeah. But it's - Hey, that's the same percent relative, so - Yeah. Yeah. Yeah. Right. It's, uh, something in - Right. Yeah. Twenty percent relative gain. Yeah. Yeah. Yeah. Um, let's see. I think the only thing we had left was - unless somebody else - Well, there's a couple things. Uh, one is anything that, um, anybody has to say about Saturday? Anything we should do in prep for Saturday? Um - I guess everybody knows about - I mean, u- um, Mari was asking - was trying to come up with something like an agenda and we're sort of fitting around people's times a bit. But, um, clearly when we actually get here we'll move things around this, as we need to, but - so you can't absolutely count on it. But - but, uh - O_K. Yeah. Are we meeting in here probably or - ? O_K. Yeah. That was my thought. I think this is - Are we recording it? Yeah. @@ We won't have enough microphones, but - u- No. I - I hadn't in- intended to. We won- we wanna - I mean, they're - There's no way. O_K. there's gonna be, uh, Jeff, Katrin, Mari and two students. So there's five from there. And Brian. But you know th- And Brian's coming, so that's six. Mm-hmm. And plus all of us. Can use the Oprah mike. Uh - Depends how fast you can throw it. It's just - It seems like too many - too much coming and going. Mm-hmm. Yeah. Well - We don't even have enough channel - Because it would be a different kind of meeting, that's what I'm - Yeah. Well - Yeah. But - I hadn't really thought of it, but - Maybe just - maybe not the whole day but just, you know, maybe some - I mean, part of it? Maybe part of it. Maybe part of it. Make everyone read digits. At the same time. At the same time. At the same time. Please. Yeah. @@ We c- I don't know. Any- That's their initiation into our Into our - our - our cult. w- Yeah, our - Yeah, our - Maybe the sections that are not right afte- you know, after lunch when everybody's still munching and - O_K. Well - So can you send out a schedule once you know it, jus- ? O_K. Yeah. I guess I sent it around a little bit. But - Is - is there a r- ? There's a res- Is it changed now, or - ? I hadn't heard back from Mari after I - I u- u- uh, brought up the point abou- about Andreas's schedule. So, um, maybe when I get back there'll be some - some mail from her. O_K. I'm looking forward to seeing your representation. That'd be, uh - So, I'll make a - And w- we should get the two meetings from y- I mean, I know about the first meeting, um, I'd like to see that. Yeah. but the other one that you did, the N_S_A one, which we hadn't done cuz we weren't running recognition on it, because the non-native speaker - Mm-hmm. there were five non-native speakers. Mm-hmm. I see. Mm-hmm. But, it would be useful for the - to see what we get with that one. So. Which N_S_A meeting was that? Great. O_K. It's, uh, two thousand eleven twenty-one one thousand. Oh, we did - That was the last one I gave you. Yeah, three. Right. So - Mm-hmm. Great. I sent email when I finished the - that one. That was sort of son- N_S_ A three, I think. Yeah. Yeah, that's right. That's right. That's much simpler. I don't know what they said but I know the number. Th- that part's definitely gonna confuse somebody who looks at these later. I mean, this is - we- we're recording secret N_S_A meetings? I mean, it's - Right. I remember the date. So. Um. Not the - Yeah. Uh. The - th- the - Yeah. Not that N_S_A. It's network services and applications. They are hard to understand. They're very, uh, out there. I have no idea what they're talking about. Wait. The - Yeah, they're cryptic. The, um - Yeah. th- the other good thing about the alignments is that, um, it's not always the machine's fault if it doesn't work. So, you can actually find, um, It's the person's fault. It's Morgan's fault. problem - uh, proble- You can find - You can find, uh, problems with - with the transcripts, um, you know, It's always Morgan's fault. Oh. and go back and fix them. But - Yeah. Tha- There are some cases like where the - the wrong speaker - uh, these ca- Not a lot, but where the - the wrong person - the - the speech is addre- attached to the wrong speaker and you can tell that when you run it. Or at least you can get clues to it. So these are from the early transcriptions that people did on the mixed signals, like what you have. Interesting. I guess it does w- Mm-hmm. It also raises the possibility of, um, using that kind of representation - I mean, I don't know, this'd be something we'd wanna check, but maybe using that representation for data entry and then displaying it on the channelized, uh, representation, cuz it - I think that the - I mean, my - my preference in terms of, like, looking at the data is to see it in this kind of musical score format. Mm-hmm. And also, s- you know, Sue's preference as well. Yeah, if you can get it to - And - and - but, I mean, this - if this is a better interface for making these kinds of, uh, you know, lo- clos- local changes, then that'd be fine, too. I don't - I have no idea. I think this is something that would need to be checked. Yeah. O_K. Th- the other thing I had actually was, I - I didn't realize this till today, but, uh, this is, uh, Jose's last day. Is my last - my last day. My - my last meeting about meetings. Yeah. Oh! Oh! Oh! Oh. You're not gonna be here tomorrow? Oh, that's right. Tomorrow - Yeah. Because, eh, I leave, eh, the next Sunday. I will come back to home - to Spain. The last meeting meeting? It's off. Mm-hmm. Oh. Yeah. I d- so I - I jus- Mm-hmm. And I - I would like to - to - to say thank you very much, eh, to all people in the group and at ICSI, because I - I enjoyed @@ very much, uh. Oh. Mm-hmm. Yeah. It was good having you. Mmm. Yeah. Mmm. And I'm sorry by the result of overlapping, because, eh, I haven't good results, eh, yet but, eh, I - I pretend to - to continuing out to Spain, eh, during the - the following months, eh, because I have, eh, another ideas Uh-huh. but, eh, I haven't enough time to - to - with six months it's not enough to - Yep. Yeah. to - to research, eh, and e- i- I mean, if, eh, the topic is, eh, so difficult, uh, in my opinion, there isn't - Yeah. Maybe somebody else will come along and will be, uh, interested in working on it and could start off from where you are also, you know. They'd make use of - of what you've done. Yeah. Yeah. Yeah. But, eh, I - I will try to recommend, eh, at, eh, the Spanish government but, eh, the following @@ scholarship, eh, eh, eh, will be here more time, because eh, i- in my opinion is - is better, eh, for us to - to spend more time here and to work more time i- i- in a topic. No ? Yeah, it's a very short time. Yeah. Yeah. But, uh - Yeah, six months is hard. I think a year Yeah. It is . Yeah. Yeah. It's difficult. is a lot better. You - e- you have, eh - you are lucky, and you - you find a solution in - in - in some few tim- uh, months, eh? O_K. But, eh, I think it's not, eh, common. But, eh, anyway, thank you. Thank you very much. Eh, I - I bring the chocolate, eh, to - Mmm. to tear, uh, with - with you, uh. Oh. Ah. Nice. I - I hope if you need, eh, something, eh, from us in the future, I - I will be at Spain, to you help, uh. Great. Well. Great. Thank you. Thank you, Jose. Right. And, thank you very much. Thank you. Have a good trip. Yeah. Yeah. Thank you. Yeah. Keep in touch. O_K. I guess, uh, unless somebody has something else, we'll read - read our digits and we'll get our - Digits? Uh. Oops. Are we gonna do them simultaneously or - ? get our last bit of, uh, Jose's - Jose - Jose's digit - Uh, I'm sorry? You - eh - Ye- ye- you prefer, eh, to eat, eh, chocolate, eh, at the coffee break, eh, at the - ? Or you prefer now, before after - ? Well, we have a time - No, we prefer to keep it for ourselves. Well, we have a s- a time - time constraint. Yeah. Yeah. During - So keep it away from that end of the table. Yeah, yeah. Yeah. during digits. Yeah. Why is it that I can read your mind? Well, we've gotta wait until after di- after we take the mikes off. No, no. So are we gonna do digits simultaneously or what? Well? Yeah. You - This is our reward if we do our digi- Yeah. O_K. I - I think, eh, it's enough, eh, for more peopl- for more people after. But, eh - Simultaneous digit chocolate task. We're gonna - we're gonna do digits at the same - Oh. I'm going to get a ticket if people don't eat some. Mmm! That's nice. Wow. Mm-hmm. So, I have a vested interest. Um. Well - Oh, thanks, Jose. To Andreas, the idea is - is good. s- To eat here. Mmm. Wow. Very nice. Oh. Tha- that's - that looks great. Alright, so in the interest of getting to the - Oh, wow. Oh, yeah. Th- it doesn't - it won't leave this room. We could do digits while other people eat. So it's background crunching. Yeah. Yeah. Mmm. Yeah. Is, eh, a - another acoustic event. Nice. We don't have background chewing. Background crunch. Yeah. No, we don't have any data with background eating. Mmm. Yeah. I'm serious. You- She's - she's serious. She is serious. It's just the rest of the digits - the rest of the digits are very clean, I am serious. Mmm. Are you - ? Oh, they're clean. Well - ? um, without a lot of background noise, so I'm just not sure - Yeah ! And it - You have to write down, like, while y- what you're - what ch- chocolate you're eating cuz they might make different sounds, like n- nuts - chocolate with nuts, chocolate without nuts. Chocolate adaptation. Oh. Crunchy frogs. Um - Actually - actually kind of careful cuz I have a strong allergy to nuts, so I have to sort of figure out one without th- That w- Oh, yeah, they - they might. It's hard to - hard to say. Maybe those? They're so - I don't know. Um - I don't know. This is - You know, this is a different kind of speech, looking at chocolates, deciding - you know, it's another style. Well - Take - take several. Mmm. Mmm. Yeah. I may - I may hold off. But if I was - eh, but maybe I'll get some later. Mmm. Thanks. Well - well, why don't we - ? He - he's worried about a ticket. Why don't we do a simultaneous one? Yeah. Thank you! O_K. O_K. O_K. Mmm. Simultaneous one? O_K. Remember to read the transcript number, please. And you laughed at me, too, f- the first time I said that. Right. O_K. Oops. Yeah. I have to what? You laughed at me, too, the first time I sa- said - I did, and now I love it so much. You really shouldn't, uh, te- O_K, everyone ready? You have to sort of, um - Jose, if you haven't done this, you have to plug your ears while you're t- talking so that you don't get confused, I guess. Transcript L_ four four. W- wait - wait a minute - wait a minute. W- we want - we want - we want it synchronized. Yeah. Hey, you've done this before. Haven't you? Yeah. Oh, you've done this one before? Together? That's - You've read digits together with us, haven't you - I mean, at the same time? No. Oh, you haven't! Oh, O_K. I'm not - we - we - Oh, and you haven't done this either. I- the first time is traumatic, but - O_K. Oh, yeah. We- Y- Yeah, bu- Oh, and the groupings are important, so yo- you're supposed to pause between the groupings. Mmm. The grouping. Yeah. O_K. So, uh - Yeah. You mean that the - the grouping is supposed to be synchronized? Yeah, sure. No. No. No, no. Synchronized digits. No? Oh, wow. No? That'd be good. We- we'll give everybody the same sheet but they say different - It's like a - like a Greek - like a Greek choir? You know? Like - Yeah. Hey, what a good idea. We could do the same sheet for everyone. Have them all read them at once. Yes. Well, different digits but same groupings. Eh - Or - or just same digits. See if anyone notices. So they would all be - Yeah. Yeah. That'd be good. And then - then we can sing them next time. There's so many possibilities. Uh. O_K, why don't we go? O_K. Uh, one two three - Go! L_ forty four, Transcript L_ twenty-nine. Transcript L_ dash four one. Transcript L_ forty-six. Four, eight eight seven, two six, one one two, one. Transcript L_ forty three. Transcript L_ forty five. Six eight six, nine three two, nine two three. Two, eight six eight, two three, two five three, eight. s- seven nine six seven, eight, four eight five, Four, zero three six, three five, one four three, one. Nine six one, eight five, nine seven seven five. Zero five five three, two, one nine zero. Forty nine, seventy seven, one six, zero three, five zero. Nine three two six, six five, nine three, three four one one. Seven one O_ eight, one nine eight four, five eight seven seven. seven four one nine, two six eight nine, four nine seven one. Zero four eight, nine four three, two three one. Two nine nine, nine four five, three four nine three. Eight two seven, two one six, four three six. Three nine five nine, five nine one eight, five one seven five. I'm starting over. Seven, five six two, six nine, three three four, four. Four nine, seven seven, one six, zero three, five zero, Two eight four three, three six two three, O_ eight five four. Zero six six, five four, two four three zero. Five nine one zero, zero nine six six, eight zero eight eight. Nine three nine, six zero five, four seven seven. three, two five four, six four, one, five six eight, Four one two six, four one five four, three four seven three. Six eight seven, eight five eight, eight one four. Three one three, O_ nine six, nine four nine. Five three, seven three, six seven, seven five zero seven. Five six one, four nine six, three six four six. Seven, seven nine two, six nine, four five zero, four. two zero nine seven, seven nine nine seven, seven eight seven eight. Five, two seven one, six one, zero five eight, one. Three six one, three six nine, Oops. Three seven nine, four O_ seven. five O_ three, three two five, five two zero nine, Six four seven seven, two zero, eight two, four seven. Zero six eight two, eight, six one nine. Two two eight eight, one eight four seven, one one two seven. Nine zero nine, three nine, two five two five. Seven eight three zero, eight five eight one, nine seven two eight. One five five, two seven, three eight nine two. Eight one, nine six, nine O_, three six, three six. Three zero three three, zero four four five, one six zero fi- six. Seven four one, zero five five, nine eight two. Seven one one eight, four one seven five, five five zero one. two seven, two four, two nine, six three, five six, Nine O_ five seven, six eight four five, one five three eight, Eight four eight nine, one, one five six. One four two two, seven four three one, nine nine three three. Seven, three five one, three four, two three one, zero. Six nine, one four, zero five, seven nine, five six, nine two. One seven three, four eight, zero seven nine three. Twenty-s- Oh, sorry. Two one eight, seven one six, five one eight, two nine, eight four, eight two, O_ three, five two. Two seven, five one, zero two, five zero, two two. Eight four seven two, O_ seven two three, O_ one seven O_. Nine seven, three seven, five nine, seven seven, two three. Four eight four three, four seven five one, eight five nine eight. eight two nine zero, six four three one, six eight five five, One nine, one six, five four, eight four, three one. four eh five eh seven, six eight, eh nine zero one two Two nine eight, one two five, two two nine nine. Seven seven five, zero eight three, nine five four seven. t- two two five eight, five two eight nine, five seven seven eight. four six nine, one five six, four seven eight one, Mmm! five four eight seven, three, five five five, one six three one, two, eight three eight, eight zero nine, zero seven four, one six four seven. Did you read it twice or what? And Andreas has the last word. He's try- No, he's trying to get good recognition performance. He had the h- Yeah. Yeah. He had the - the long form. No. And we're off. 