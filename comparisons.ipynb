{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5078412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "from nltk.tag import pos_tag # for proper noun\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from statistics import median\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statistics\n",
    "\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b1ed2b",
   "metadata": {},
   "source": [
    "# AMI Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc06969",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='path_to_ami\\\\ami-transcripts'\n",
    "filelist=os.listdir(path)\n",
    "transcripts={}\n",
    "for file in filelist:\n",
    "    with open(f'{path}\\\\{file}', 'r') as f: \n",
    "        txt=f.read()\n",
    "        transcripts[file.split('.')[0]]=txt\n",
    "\n",
    "path='path_to_ami\\\\ami-summary\\\\extractive'\n",
    "filelist=os.listdir(path)       \n",
    "extractive_summary={}\n",
    "for file in filelist:\n",
    "    with open(f'{path}\\\\{file}', 'r') as f: \n",
    "        txt=f.read()\n",
    "        extractive_summary[file.split('.')[0]]=txt \n",
    "        \n",
    "path='path_to_ami\\\\ami-summary\\\\abstractive'\n",
    "filelist=os.listdir(path)       \n",
    "abstractive_summary={}\n",
    "for file in filelist:\n",
    "    with open(f'{path}\\\\{file}', 'r') as f: \n",
    "        txt=f.read()\n",
    "        abstractive_summary[file.split('.')[0]]=txt\n",
    "\n",
    "        \n",
    "inner_join_transcripts=deepcopy(transcripts)\n",
    "inner_join_extractive_summary=deepcopy(extractive_summary)\n",
    "inner_join_abstractive_summary=deepcopy(abstractive_summary)\n",
    "l=list(transcripts.keys())\n",
    "\n",
    "for i in l:\n",
    "    if i not in inner_join_abstractive_summary:\n",
    "        del inner_join_transcripts[i]\n",
    "        if i in inner_join_extractive_summary:\n",
    "            del inner_join_extractive_summary[i]\n",
    "        continue\n",
    "    if i not in inner_join_extractive_summary:\n",
    "        del inner_join_transcripts[i]\n",
    "        if i in inner_join_abstractive_summary:\n",
    "            del inner_join_abstractive_summary[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6cbd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ami_mean(transcripts, small_threshold , big_threshold ):\n",
    "    stopWords = list(set(stopwords.words(\"english\")))\n",
    "    count_small=0\n",
    "    count_big=0\n",
    "    sum_small=0\n",
    "    sum_big=0\n",
    "    position_small=[]\n",
    "    position_big=[]\n",
    "    total_sent=0\n",
    "    total_words=0\n",
    "    total_refined_words=0\n",
    "    unique={}\n",
    "    number=len(transcripts)\n",
    "    total_unique_refined_words=0\n",
    "    max_len=[]\n",
    "    position_max_len=[]\n",
    "    small_bool=[]\n",
    "    turns=0\n",
    "    for key in transcripts.keys():\n",
    "        txt=nltk.sent_tokenize(transcripts[key])\n",
    "        total_sent+=len(txt)\n",
    "        m=0\n",
    "        p=0\n",
    "        for index,sentence in enumerate(txt):\n",
    "            temp=nltk.word_tokenize(sentence)\n",
    "            sen_len=len(temp)\n",
    "            m=max(m,sen_len)\n",
    "            if m==sen_len:\n",
    "                p=index/len(txt)\n",
    "            temp=[word.lower() for word in temp]\n",
    "            word_tokens_refined=[x for x in temp if x not in stopWords]\n",
    "            for word in word_tokens_refined:\n",
    "                if word not in unique: \n",
    "                    total_unique_refined_words+=1\n",
    "                    unique[word]=1\n",
    "                elif word in unique:\n",
    "                    unique[word]+=1\n",
    "\n",
    "            total_refined_words+=len(word_tokens_refined)\n",
    "            total_words+=sen_len\n",
    "            \n",
    "            if sen_len<small_threshold:\n",
    "                count_small+=1\n",
    "                sum_small+=sen_len\n",
    "                position_small.append(index/len(txt))\n",
    "                small_bool.append(1)\n",
    "            elif sen_len>big_threshold:\n",
    "                count_big+=1\n",
    "                sum_big+=sen_len\n",
    "                position_big.append(index/len(txt))\n",
    "                small_bool.append(0)\n",
    "            else:\n",
    "                small_bool.append(0)\n",
    "                \n",
    "        max_len.append(m)\n",
    "        position_max_len.append(p)\n",
    "\n",
    "    # number of continuous occurances\n",
    "    small_cont=0\n",
    "    check=False\n",
    "    for i in range(0,len(small_bool),4):\n",
    "        if sum(small_bool[i:i+4])>=2:\n",
    "            small_cont+=1\n",
    "            check=False\n",
    "        else:\n",
    "            try: \n",
    "                if sum(small_bool[i:i+4])==1 and small_bool[i+3]==1:\n",
    "                    check=True\n",
    "                elif sum(small_bool[i:i+4])==1 and small_bool[i]==1 and check==True:\n",
    "                    small_cont+=1\n",
    "                    check=False            \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    \n",
    "    \n",
    "    return (count_small, count_big, sum_small, sum_big, position_small, position_big, \n",
    "total_sent, total_words, total_refined_words, total_unique_refined_words, unique, number, max_len,\n",
    "            position_max_len, small_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f261a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "(count_small_transcripts_ami, count_big_transcripts_ami, sum_small_transcripts_ami, \n",
    "sum_big_transcripts_ami, position_small_transcripts_ami, position_big_transcripts_ami, \n",
    "total_sent_transcripts_ami, total_words_transcripts_ami, total_refined_words_transcripts_ami,\n",
    "total_refined_unique_words_transcripts_ami, unique_dict_transcripts_ami, number_transcripts_ami,\n",
    "max_len_transcripts_ami, position_max_len_transcripts_ami, small_cont_transcripts_ami) = ami_mean(transcripts, 5 , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa194e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4014.497076023392"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_refined_words_transcripts_ami/number_transcripts_ami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70f30c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(count_small_esum_ami, count_big_esum_ami, sum_small_esum_ami, \n",
    "sum_big_esum_ami, position_small_esum_ami, position_big_esum_ami, \n",
    "total_sent_esum_ami, total_words_esum_ami, total_refined_words_esum_ami,\n",
    "total_refined_unique_words_esum_ami, unique_dict_esum_ami,number_esum_ami,\n",
    "max_len_esum_ami, position_max_len_esum_ami,small_cont_esum_ami)  = ami_mean(extractive_summary, 5 , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "126afdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "(count_small_asum_ami, count_big_asum_ami, sum_small_asum_ami, \n",
    "sum_big_asum_ami, position_small_asum_ami, position_big_asum_ami, \n",
    "total_sent_asum_ami, total_words_asum_ami, total_refined_words_asum_ami,\n",
    "total_refined_unique_words_asum_ami, unique_dict_asum_ami,number_asum_ami,\n",
    "max_len_asum_ami, position_max_len_asum_ami,small_cont_asum_ami) = ami_mean(abstractive_summary, 5 , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "312c9839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179.2887323943662"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words_asum_ami/len(abstractive_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43df1340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.471830985915492"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sent_asum_ami/number_asum_ami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e24277ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of hours:\n",
    "ami_hours=100  #from their website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4a6132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='parth_to_ami\\\\ami-transcripts-speaker'\n",
    "filelist=os.listdir(path)\n",
    "ami_turns=[]\n",
    "for key in transcripts.keys():\n",
    "    speakers=[0]*len(transcripts[key].split('.'))\n",
    "    for file in filelist:\n",
    "        if file.split('.')[0]==key:\n",
    "            name=file.split('.')[1]\n",
    "            with open(f'{path}\\\\{file}', 'r') as f: \n",
    "                txt=f.read()\n",
    "            for index,line in enumerate(transcripts[key].split('.')):\n",
    "                if line in txt:\n",
    "                    speakers[index]=name\n",
    "    t=0\n",
    "    for index in range(1,len(speakers)):\n",
    "        if speakers[index-1]!=speakers[index]:\n",
    "            t+=1\n",
    "    ami_turns.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4123754",
   "metadata": {},
   "source": [
    "# ICSI Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc3c888d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-7f80e797a5c7>:31: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  participants_count_list.append(len(root[0][index].getchildren()))\n"
     ]
    }
   ],
   "source": [
    "path='path_to_icsi\\\\transcripts'\n",
    "temp=os.listdir(path)\n",
    "root_list=[]\n",
    "meeting_name_list=[]\n",
    "text_list=[]\n",
    "time_list=[]\n",
    "participants_count_list=[]\n",
    "turns=[]\n",
    "\n",
    "path1='path_to_icsi\\\\Summarization\\\\abstractive'\n",
    "absumfilelist=os.listdir(path1)\n",
    "\n",
    "filelist=[]\n",
    "for file in absumfilelist:\n",
    "    if f'{file[:-12]}.mrt' in temp:\n",
    "        filelist.append(f'{file[:-12]}.mrt')\n",
    "\n",
    "\n",
    "for file in filelist:\n",
    "    if file[-3:]=='mrt':\n",
    "        tree = ET.parse(f'{path}/{file}')\n",
    "        root = tree.getroot()\n",
    "        root_list.append(root)\n",
    "        meeting_name=root.attrib['Session']\n",
    "        meeting_name_list.append(meeting_name)\n",
    "        time = float(root[1].attrib['EndTime'])-float(root[1].attrib['StartTime'])\n",
    "        time_list.append(time)\n",
    "        # number of speakers\n",
    "        for index,child in enumerate(root[0]):\n",
    "            if child.tag=='Participants':\n",
    "                participants_count_list.append(len(root[0][index].getchildren()))\n",
    "        \n",
    "        txt=[]\n",
    "        t=0\n",
    "\n",
    "        for child in root[1]:\n",
    "            try:\n",
    "                current_speaker=child.attrib['Participant']\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        for child in root[1]:\n",
    "            try:\n",
    "                new_speaker=child.attrib['Participant']\n",
    "            except:\n",
    "                pass\n",
    "            if current_speaker!=new_speaker:\n",
    "                t+=1\n",
    "                current_speaker=new_speaker\n",
    "            txt.append(child.text)\n",
    "        text_list.append(txt)\n",
    "        turns.append(t)\n",
    "        \n",
    "ab_sumroot_list=[]\n",
    "ab_sumtext_list=[]\n",
    "for f in absumfilelist:\n",
    "    tree = ET.parse(f'{path1}/{f}')\n",
    "    root = tree.getroot()\n",
    "    ab_sumroot_list.append(root)\n",
    "    txt=[]\n",
    "    s=[]\n",
    "    for child in ab_sumroot_list[0]:\n",
    "        for i in child:\n",
    "            txt.append(i.text)\n",
    "    ab_sumtext_list.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(turns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0830282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "icsi_hours=sum(time_list)/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0abfcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def icsi_mean(text_list,small_threshold, big_threshold):\n",
    "    stopWords = list(set(stopwords.words(\"english\")))\n",
    "    count_small=0\n",
    "    count_big=0\n",
    "    sum_small=0\n",
    "    sum_big=0\n",
    "    position_small=[]\n",
    "    position_big=[]\n",
    "    total_sent=0\n",
    "    total_words=0\n",
    "    total_refined_words=0\n",
    "    unique={}\n",
    "    total_unique_refined_words=0\n",
    "    number=len(text_list)\n",
    "    \n",
    "    max_len=[]\n",
    "    position_max_len=[]\n",
    "    \n",
    "    small_bool=[]\n",
    "    for i in range(len(text_list)):\n",
    "        # index of one document \n",
    "        \n",
    "        total_sent+=len(text_list[i])\n",
    "        m=0\n",
    "        p=0\n",
    "        #m=max(m,len(text_list[i]))\n",
    "        #if m==len(text_list[i]):\n",
    "        #    p=i/\n",
    "        for index,text in enumerate(text_list[i]):\n",
    "            \n",
    "            a=nltk.word_tokenize(text)\n",
    "            m=max(m,len(a))\n",
    "            if m==len(a):\n",
    "                p=index/len(text_list[i])\n",
    "            a=[word.lower() for word in a]\n",
    "            word_tokens_refined=[x for x in a if x not in stopWords]\n",
    "            for word in word_tokens_refined:\n",
    "                if word not in unique: \n",
    "                    total_unique_refined_words+=1\n",
    "                    unique[word]=1\n",
    "                elif word in unique:\n",
    "                    unique[word]+=1\n",
    "            total_refined_words+=len(word_tokens_refined)\n",
    "            total_words+=len(a)\n",
    "            \n",
    "            if len(a)<small_threshold:\n",
    "                count_small+=1\n",
    "                sum_small+=len(a)\n",
    "                position_small.append(index/len(text_list[i]))\n",
    "                small_bool.append(1)\n",
    "            elif len(a)>big_threshold:\n",
    "                count_big+=1\n",
    "                sum_big+=len(a)\n",
    "                position_big.append(index/len(text_list[i]))\n",
    "                small_bool.append(0)\n",
    "            else:\n",
    "                small_bool.append(0)\n",
    "            max_len.append(m)\n",
    "            position_max_len.append(p)\n",
    "     \n",
    "    small_cont=0\n",
    "    check=False\n",
    "    for i in range(0,len(small_bool),4):\n",
    "        if sum(small_bool[i:i+4])>=2:\n",
    "            small_cont+=1\n",
    "            check=False\n",
    "        else:\n",
    "            try:\n",
    "                if sum(small_bool[i:i+4])==1 and small_bool[i+3]==1:\n",
    "                    check=True\n",
    "                elif sum(small_bool[i:i+4])==1 and small_bool[i]==1 and check==True:\n",
    "                    small_cont+=1\n",
    "                    check=False  \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "                \n",
    "    return (count_small, count_big, sum_small, sum_big, position_small, position_big, total_sent,total_words,\n",
    "total_refined_words, total_unique_refined_words, unique, number, max_len, position_max_len, small_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93d84cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "(count_small_transcripts_icsi, count_big_transcripts_icsi, sum_small_transcripts_icsi,\n",
    "sum_big_transcripts_icsi, position_small_transcripts_icsi, position_big_transcripts_icsi,\n",
    "total_sent_transcripts_icsi,total_words_transcripts_icsi, total_refined_words_transcripts_icsi, \n",
    "total_refined_unique_words_transcripts_icsi, unique_dict_transcripts_icsi, number_transcripts_icsi, \n",
    "max_len_transcripts_icsi, position_max_len_transcripts_icsi, small_cont_transcripts_icsi)=icsi_mean(text_list,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9332ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_refined_words_transcripts_icsi/number_transcripts_icsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60bcfd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "(count_small_asum_icsi, count_big_asum_icsi, sum_small_asum_icsi,\n",
    "sum_big_asum_icsi, position_small_asum_icsi, position_big_asum_icsi,\n",
    "total_sent_asum_icsi,total_words_asum_icsi, total_refined_words_asum_icsi, \n",
    "total_refined_unique_words_asum_icsi, unique_dict_asum_icsi, number_asum_icsi,\n",
    "max_len_asum_icsi, position_max_len_asum_icsi, small_count_asum_icsi)=icsi_mean(ab_sumtext_list,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba977152",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_refined_words_asum_icsi/number_asum_icsi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c91a7c",
   "metadata": {},
   "source": [
    "# AutoMin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bdebb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='path_to_automn\\\\deidentified-meetings'\n",
    "filelist=os.listdir(path)\n",
    "\n",
    "automin_transcripts={}\n",
    "automin_summary={}\n",
    "participants={}\n",
    "for file in filelist:\n",
    "    summs={}\n",
    "    #if file[-2:]=='en':\n",
    "    f=os.listdir(f'{path}\\\\{file}')\n",
    "    for ts in f:\n",
    "        if ts[-4:]!='.txt':\n",
    "            continue\n",
    "        #print(ts[:7])\n",
    "\n",
    "        if ts[:10]=='transcript':\n",
    "            name=list(ts.split('_'))[2][:8]\n",
    "            with open(f'{path}\\\\{file}\\\\{ts}', 'r', encoding='utf8') as f1:\n",
    "                txt=f1.read()\n",
    "                automin_transcripts[file]=txt\n",
    "\n",
    "        if ts[:7]=='minutes':\n",
    "            #print(ts[:7])\n",
    "            name=''.join(list(ts.split('.'))[-3])\n",
    "            with open(f'{path}\\\\{file}\\\\{ts}', 'r', encoding='utf8') as f2:\n",
    "                txt=f2.read()\n",
    "                summs[name]=txt\n",
    "    automin_summary[file]=summs\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c12d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(automin_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92ecbce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(count_small_transcripts_automin, count_big_transcripts_automin, sum_small_transcripts_automin, \n",
    "sum_big_transcripts_automin, position_small_transcripts_automin, position_big_transcripts_automin, \n",
    "total_sent_transcripts_automin, total_words_transcripts_automin, total_refined_words_transcripts_automin,\n",
    "total_refined_unique_words_transcripts_automin, unique_dict_transcripts_automin, number_transcripts_automin,\n",
    "max_len_transcripts_automin, position_max_len_transcripts_automin, small_cont_transcripts_automin) = ami_mean(automin_transcripts, 5 , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c546ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_small_asum_automin=[]\n",
    "count_big_asum_automin=[]\n",
    "sum_small_asum_automin=[]\n",
    "sum_big_asum_automin=[]\n",
    "position_small_asum_automin=[]\n",
    "position_big_asum_automin=[]\n",
    "total_sent_asum_automin=[]\n",
    "total_words_asum_automin=[]\n",
    "total_refined_words_asum_automin=[]\n",
    "total_refined_unique_words_asum_automin=[]\n",
    "unique_dict_asum_automin=[]\n",
    "number_asum_automin=[]\n",
    "max_len_asum_automin=[]\n",
    "position_max_len_asum_automin=[]\n",
    "small_cont_asum_automin=[]\n",
    "\n",
    "ll=[count_small_asum_automin, count_big_asum_automin, sum_small_asum_automin, \n",
    "sum_big_asum_automin, position_small_asum_automin, position_big_asum_automin, \n",
    "total_sent_asum_automin, total_words_asum_automin, total_refined_words_asum_automin,\n",
    "total_refined_unique_words_asum_automin, unique_dict_asum_automin, number_asum_automin,\n",
    "max_len_asum_automin, position_max_len_asum_automin, small_cont_asum_automin]\n",
    "\n",
    "for key in automin_summary.keys():\n",
    "    temp=ami_mean(automin_summary[key], 5,10)\n",
    "    for index in range(len(temp)):\n",
    "        ll[index].append(temp[index])\n",
    "    \n",
    "count_small_asum_automin=sum(count_small_asum_automin)\n",
    "count_big_asum_automin=sum(count_big_asum_automin)\n",
    "sum_small_asum_automin=sum(sum_small_asum_automin)\n",
    "sum_big_asum_automin=sum(sum_big_asum_automin)\n",
    "position_small_asum_automin=sum(position_small_asum_automin,[])\n",
    "position_big_asum_automin=sum(position_big_asum_automin,[])\n",
    "total_sent_asum_automin=sum(total_sent_asum_automin)\n",
    "total_words_asum_automin=sum(total_words_asum_automin)\n",
    "total_refined_words_asum_automin=sum(total_refined_words_asum_automin)\n",
    "total_refined_unique_words_asum_automin=sum(total_refined_unique_words_asum_automin)\n",
    "number_asum_automin=sum(number_asum_automin)\n",
    "max_len_asum_automin=sum(max_len_asum_automin,[])\n",
    "position_max_len_asum_automin=sum(position_max_len_asum_automin,[])\n",
    "small_cont_asum_automin=sum(small_cont_asum_automin)\n",
    "\n",
    "\n",
    "\n",
    "# (count_small_asum_automin, count_big_asum_automin, sum_small_asum_automin, \n",
    "# sum_big_asum_automin, position_small_asum_automin, position_big_asum_automin, \n",
    "# total_sent_asum_automin, total_words_asum_automin, total_refined_words_asum_automin,\n",
    "# total_refined_unique_words_asum_automin, unique_dict_asum_automin, number_asum_automin,\n",
    "# max_len_asum_automin, position_max_len_asum_automin, small_cont_asum_automin) = ami_mean(automin_summary, 5 , 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b926127",
   "metadata": {},
   "source": [
    "# AMI Turns and number of speakers (using change in speaker) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0db1ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "turns_list=[]\n",
    "speaker_list=[]\n",
    "\n",
    "for key in automin_transcripts.keys():\n",
    "    s=[]\n",
    "    check=False\n",
    "    turns=0\n",
    "    speaker=[]\n",
    "    for index,i in enumerate(automin_transcripts[key]):\n",
    "        if check == True:\n",
    "            s.append(i)\n",
    "        if i=='(' and automin_transcripts[key][index+1:index+7]=='PERSON':\n",
    "            turns+=1\n",
    "            check=True\n",
    "        if i==')' and check == True:\n",
    "            check=False\n",
    "            speaker.append(''.join(s[:-1]))\n",
    "            s=[]\n",
    "    turns_list.append(turns)\n",
    "    speaker_list.append(list(set(speaker)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b832e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of speakers\n",
    "\n",
    "len(sum(speaker_list, []))/ len(speaker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4ff687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns\n",
    "\n",
    "sum(turns_list)/len(turns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5805d",
   "metadata": {},
   "source": [
    "# Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d83c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['count_small', 'count_big','sum_small','sum_big','position_small','position_big',\n",
    "    'total_sent','total_words','total_refined_words','total_unique_refined_words','number','unique','max_len',\n",
    "                         'position_max_len','small_cont_percentage','dataset', 'label'],\n",
    "                index=['ami_transcripts', 'ami_esum', 'ami_asum','icsi_transcripts', 'icsi_asum','automin_transcripts', 'automin_asum'])\n",
    "df.count_small=[count_small_transcripts_ami, count_small_esum_ami, count_small_asum_ami,count_small_transcripts_icsi,count_small_asum_icsi, count_small_transcripts_automin, count_small_asum_automin]\n",
    "df.count_big=[count_big_transcripts_ami, count_big_esum_ami, count_big_asum_ami,count_big_transcripts_icsi, count_big_asum_icsi, count_big_transcripts_automin, count_big_asum_automin]\n",
    "df.sum_small=[sum_small_transcripts_ami, sum_small_esum_ami, sum_small_asum_ami, sum_small_transcripts_icsi,sum_small_asum_icsi, sum_small_transcripts_automin, sum_small_asum_automin]\n",
    "df.sum_big=[sum_big_transcripts_ami,sum_big_esum_ami, sum_big_asum_ami, sum_big_transcripts_icsi, sum_big_asum_icsi, sum_big_transcripts_automin, sum_big_asum_automin]\n",
    "df.position_small=[position_small_transcripts_ami,position_small_esum_ami, position_small_asum_ami, position_small_transcripts_icsi, position_small_asum_icsi, position_small_transcripts_automin, position_small_asum_automin]\n",
    "df.position_big=[position_big_transcripts_ami,position_big_esum_ami, position_big_asum_ami, position_big_transcripts_icsi, position_big_asum_icsi, position_big_transcripts_automin, position_big_asum_automin]\n",
    "df.total_sent=[total_sent_transcripts_ami, total_sent_esum_ami, total_sent_asum_ami, total_sent_transcripts_icsi, total_sent_asum_icsi, total_sent_transcripts_automin, total_sent_asum_automin]\n",
    "df.total_words=[total_words_transcripts_ami,total_words_esum_ami,total_words_asum_ami, total_words_transcripts_icsi, total_words_asum_icsi, total_words_transcripts_automin, total_words_asum_automin]\n",
    "df.total_refined_words=[total_refined_words_transcripts_ami, total_refined_words_esum_ami, total_refined_words_asum_ami, total_refined_words_transcripts_icsi, total_refined_words_asum_icsi, total_refined_words_transcripts_automin, total_refined_words_asum_automin]\n",
    "df.total_unique_refined_words=[total_refined_unique_words_transcripts_ami, total_refined_unique_words_esum_ami, total_refined_unique_words_asum_ami, total_refined_unique_words_transcripts_icsi, total_refined_unique_words_asum_icsi, total_refined_unique_words_transcripts_ami, total_refined_unique_words_asum_automin]\n",
    "df.number=[number_transcripts_ami,number_esum_ami, number_asum_ami, number_transcripts_icsi, number_asum_icsi, number_transcripts_automin, number_asum_automin]\n",
    "df.unique=[unique_dict_transcripts_ami, unique_dict_esum_ami, unique_dict_asum_ami, unique_dict_transcripts_icsi, unique_dict_asum_icsi, unique_dict_transcripts_automin, unique_dict_asum_automin]\n",
    "df.max_len=[max_len_transcripts_ami, max_len_esum_ami, max_len_asum_ami,max_len_transcripts_icsi,max_len_asum_icsi, max_len_transcripts_automin, max_len_asum_automin]\n",
    "df.position_max_len=[position_max_len_transcripts_ami, position_max_len_esum_ami, position_max_len_asum_ami,position_max_len_transcripts_icsi,position_max_len_asum_icsi, position_max_len_transcripts_automin, position_max_len_asum_automin]\n",
    "df.small_cont_percentage=[small_cont_transcripts_ami/total_sent_transcripts_ami, small_cont_esum_ami/total_sent_esum_ami, small_cont_asum_ami/total_sent_asum_ami,small_cont_transcripts_icsi/total_sent_transcripts_icsi, small_count_asum_icsi/total_sent_asum_icsi, small_cont_transcripts_automin/total_sent_transcripts_automin, small_cont_asum_automin/total_sent_asum_automin]\n",
    "\n",
    "df.dataset=['ami', 'ami','ami', 'icsi', 'icsi','automin', 'automin']\n",
    "df.label=['transcript', 'extractive_summary', 'abstractive_summary','transcript', 'abstractive_summary','transcript', 'abstractive_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f39a1b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-ae695f6ac165>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['unique']['automin_asum']=t\n"
     ]
    }
   ],
   "source": [
    "# convert list of automin unique words to dict\n",
    "original_automin_vocabs=deepcopy(df['unique']['automin_asum'])\n",
    "t={}\n",
    "for dic in df['unique']['automin_asum']:\n",
    "    for key in dic:\n",
    "        if key in t:\n",
    "            t[key]+=dic[key]\n",
    "        else:\n",
    "            t[key]=dic[key]\n",
    "df['unique']['automin_asum']=t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2275d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131814dc",
   "metadata": {},
   "source": [
    "### Novel summary words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59870851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def novel_summ_word(df, dataset):\n",
    "    t=0\n",
    "    for i in df['unique'][f'{dataset}_asum'].keys():\n",
    "        if i not in df['unique'][f'{dataset}_transcripts'].keys():\n",
    "            t+=1\n",
    "            \n",
    "    return t, t/len(df.unique[f'{dataset}_asum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937fc125",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.unique.ami_asum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f5f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_summ_word(df, 'ami')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16917420",
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_summ_word(df, 'icsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f1342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_summ_word(df,'automin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e450af",
   "metadata": {},
   "source": [
    "### Distribution of small sentences in datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1265157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df.position_small.loc['ami_transcripts'])\n",
    "sns.kdeplot(df.position_small.loc['icsi_transcripts'])\n",
    "sns.kdeplot(df.position_small.loc['automin_transcripts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5137e05",
   "metadata": {},
   "source": [
    "### Distribution of big sentences in datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df.position_big.loc['ami_transcripts'])\n",
    "sns.kdeplot(df.position_big.loc['icsi_transcripts'])\n",
    "sns.kdeplot(df.position_big.loc['automin_transcripts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fedfd47",
   "metadata": {},
   "source": [
    "### Total sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc2d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df.label,y=df.total_sent, hue=df.dataset, palette='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f314f8da",
   "metadata": {},
   "source": [
    "### Total words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee5c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df.label,y=df.total_words, hue=df.dataset, palette='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41288def",
   "metadata": {},
   "source": [
    "### Total refined words  (occurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db8c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df.label,y=df.total_refined_words, hue=df.dataset, palette='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ccae75",
   "metadata": {},
   "source": [
    "### Total unique refined words  (Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df.label,y=df.total_unique_refined_words, hue=df.dataset, palette='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d45c61",
   "metadata": {},
   "source": [
    "### Frequency of small talk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df.label,y=df.count_small, hue=df.dataset, palette='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17edb5c5",
   "metadata": {},
   "source": [
    "### Frequency of big talk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df.label,y=df.count_big, hue=df.dataset, palette='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df7b826",
   "metadata": {},
   "source": [
    "### Percentage of small talk (sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9ebb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df.label,y=(df.count_small/df.total_sent)*100, hue=df.dataset, palette='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count_small/df.total_sent*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6167c9",
   "metadata": {},
   "source": [
    "### Percentage of big sentences  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d48706",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df.label,y=(df.count_big/df.total_sent)*100, hue=df.dataset, palette='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911ac28",
   "metadata": {},
   "source": [
    "### Average length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bccfe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df.label,y=(df.total_sent/df.number), hue=df.dataset, palette='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95a484",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.total_sent/df.number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91020b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.total_words/df.number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb8f978",
   "metadata": {},
   "source": [
    "### Max sentence length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d3dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.max_len.apply(sum)/df.max_len.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f54e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.position_small.automin_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d331ef",
   "metadata": {},
   "source": [
    "### T-test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d342cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca86d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(df.position_big.icsi_transcripts,df.position_big.automin_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b7584",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(df.position_big.ami_transcripts,df.position_big.automin_transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b65e3c",
   "metadata": {},
   "source": [
    "# LDA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52dc22d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\umang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\umang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "#     tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "344f0cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TS3012d .......................  : done\n",
      "15.421052631578947\n"
     ]
    }
   ],
   "source": [
    "#ami \n",
    "scores=0\n",
    "for key in transcripts.keys():\n",
    "    clear_output(wait=True)\n",
    "    print(key,\".......................\", end=\" \")\n",
    "    text_data = []\n",
    "    for line in nltk.sent_tokenize(transcripts[key]):\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if len(tokens)>=1:\n",
    "            text_data.append(tokens)\n",
    "    \n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=100, id2word=dictionary, passes=15)\n",
    "    topics = ldamodel.print_topics(num_words=20)\n",
    "    a=[]\n",
    "    for topic in topics:\n",
    "\n",
    "        s=0\n",
    "        for i in topic[1].split('+ '):\n",
    "            s+=float(i[:5])\n",
    "        a.append(s)\n",
    "    maxa=max(a)\n",
    "    a=[i/maxa for i in a if i/maxa>0.5]\n",
    "    scores+=len(a)\n",
    "    print(' : done')\n",
    "print(scores/len(transcripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15c5a799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.278688524590162\n"
     ]
    }
   ],
   "source": [
    "#icsi\n",
    "\n",
    "scores=0\n",
    "for index in range(len(text_list)):\n",
    "    text_data = []\n",
    "    for line in nltk.sent_tokenize(''.join(text_list[index])):\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        # print(tokens)\n",
    "        if len(tokens)>=1:\n",
    "            #print(tokens)\n",
    "            text_data.append(tokens)\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    #pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "    #dictionary.save('dictionary.gensim')\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=100, id2word=dictionary, passes=15)\n",
    "    topics = ldamodel.print_topics(num_words=20)\n",
    "    a=[]\n",
    "    for topic in topics:\n",
    "        s=0\n",
    "        for i in topic[1].split('+ '):\n",
    "            s+=float(i[:5])\n",
    "        a.append(s)\n",
    "    maxa=max(a)\n",
    "    a=[i/maxa for i in a if i/maxa>0.5]\n",
    "    scores+=len(a)\n",
    "\n",
    "    \n",
    "print(scores/len(text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb7bb92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.75438596491228\n"
     ]
    }
   ],
   "source": [
    "# automin\n",
    "\n",
    "scores=0\n",
    "for key in automin_transcripts.keys():\n",
    "    text_data = []\n",
    "    for line in nltk.sent_tokenize(automin_transcripts[key]):\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        # print(tokens)\n",
    "        if len(tokens)>=1:\n",
    "            #print(tokens)\n",
    "            text_data.append(tokens)\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    #pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "    #dictionary.save('dictionary.gensim')\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=100, id2word=dictionary, passes=15)\n",
    "    topics = ldamodel.print_topics(num_words=20)\n",
    "    a=[]\n",
    "    for topic in topics:\n",
    "        s=0\n",
    "        for i in topic[1].split('+ '):\n",
    "            s+=float(i[:5])\n",
    "        a.append(s)\n",
    "    maxa=max(a)\n",
    "    a=[i/maxa for i in a if i/maxa>0.5]\n",
    "    scores+=len(a)\n",
    "    \n",
    "print(scores/len(transcripts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f48a82c",
   "metadata": {},
   "source": [
    "### Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb7e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08234c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(dataset):\n",
    "    #normalize score\n",
    "    t_vocab={}\n",
    "    for key in df['unique'][f'{dataset}_transcripts'].keys():\n",
    "        t_vocab[key]=df['unique'][f'{dataset}_transcripts'][key]/len(df['unique'][f'{dataset}_transcripts'])\n",
    "    print(t_vocab)\n",
    "    s_vocab={}\n",
    "    for key in df['unique'][f'{dataset}_asum'].keys():\n",
    "        s_vocab[key]=df['unique'][f'{dataset}_asum'][key]/len(df['unique'][f'{dataset}_asum'])\n",
    "\n",
    "    corr=[]\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for key in t_vocab:\n",
    "        if key in s_vocab:\n",
    "            x.append(t_vocab[key])\n",
    "            y.append(s_vocab[key])\n",
    "\n",
    "    return pearsonr(x,y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d55eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['unique'][f'ami_transcripts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c947b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation('ami')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19340903",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation('icsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77522f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation('automin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59465d8",
   "metadata": {},
   "source": [
    "# Perplexity and Entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of words spoken by each speaker\n",
    "# ami\n",
    "path='path_to_ami\\\\ami-transcripts-speaker'\n",
    "filelist=os.listdir(path)\n",
    "entropy=[]\n",
    "perplexity=[]\n",
    "for key in transcripts.keys():\n",
    "    speaker_words=[]\n",
    "    for file in filelist:\n",
    "        if file[:7]==key:\n",
    "            with open(f'{path}\\\\{file}','r', encoding='utf8') as f:\n",
    "                txt=f.read()\n",
    "                speaker_words.append(len(txt.split(' ')))\n",
    "        elif file[:6]==key:\n",
    "            with open(f'{path}\\\\{file}','r', encoding='utf8') as f:\n",
    "                txt=f.read()\n",
    "                speaker_words.append(len(txt.split(' ')))\n",
    "        else:\n",
    "            pass\n",
    "    tot=sum(speaker_words)\n",
    "    speaker_words=[i/tot for i in speaker_words]\n",
    "    e=0\n",
    "    for p in speaker_words:\n",
    "        e+=(np.log2(p))\n",
    "    e=e/len(speaker_words)\n",
    "    entropy.append(e)\n",
    "    perplexity.append(pow(2,e))\n",
    "print('AVG Perplexity: ',statistics.mean(perplexity))\n",
    "print('AVG Entropy: ',statistics.mean(entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e9c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# icsi\n",
    "\n",
    "speakers={}\n",
    "entropy=[]\n",
    "perplexity=[]\n",
    "for root in root_list:\n",
    "    \n",
    "    #getting number of words spoken by speakers\n",
    "    \n",
    "    for child in root[1]:\n",
    "        if 'Participant' in child.attrib:\n",
    "                #print(len(child.text.split(' ')))\n",
    "                if child.attrib['Participant'] in speakers:\n",
    "                    speakers[child.attrib['Participant']]+=len(child.text.split(' '))\n",
    "                else:\n",
    "                    speakers[child.attrib['Participant']]=len(child.text.split(' '))\n",
    "        #print(child)\n",
    "    \n",
    "    tot=sum(speakers.values())\n",
    "    speaker=[i/tot for i in speakers.values()]\n",
    "    e=0\n",
    "    for p in speaker:\n",
    "        e+=(np.log2(p))\n",
    "    e=e/len(speaker)\n",
    "    entropy.append(e)\n",
    "    perplexity.append(pow(2,e))\n",
    "\n",
    "print('AVG Perplexity: ',statistics.mean(perplexity))\n",
    "print('AVG Entropy: ',statistics.mean(entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automin\n",
    "\n",
    "entropy=[]\n",
    "perplexity=[]\n",
    "\n",
    "for key in automin_transcripts.keys():\n",
    "    turns=0\n",
    "    speaker={}\n",
    "    text=automin_transcripts[key]#.split(' ')\n",
    "    # get speaker words\n",
    "    for index,i in enumerate(text):\n",
    "        \n",
    "        \n",
    "        if i=='(' and text[index+1:index+7]=='PERSON':\n",
    "            name=text[index+1:index+8]\n",
    "            t=[]\n",
    "            for index_temp,temp in enumerate(text[(index+10):]):\n",
    "                if temp=='(':\n",
    "                    break\n",
    "                t.append(temp)\n",
    "            t=len((''.join(t)).split(' '))\n",
    "            if name in speaker:\n",
    "                speaker[name]+=t\n",
    "            else:\n",
    "                speaker[name]=t\n",
    "    try:\n",
    "        tot=sum(speaker.values())\n",
    "        speaker=[i/tot for i in speaker.values()]\n",
    "        e=0\n",
    "        for p in speaker:\n",
    "            e+=(np.log2(p))\n",
    "        e=e/len(speaker)\n",
    "        entropy.append(e)\n",
    "        perplexity.append(pow(2,e))\n",
    "    except:\n",
    "        pass\n",
    "        #print(key)\n",
    "                    \n",
    "\n",
    "print('AVG Perplexity: ',statistics.mean(perplexity))\n",
    "print('AVG Entropy: ',statistics.mean(entropy))\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cad839",
   "metadata": {},
   "source": [
    "# Inter Annotator Agreement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9c5ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare\n",
    "chisq=[]\n",
    "standard_deviations=[]\n",
    "for key1 in automin_summary.keys():\n",
    "    scores=0\n",
    "    # For each transcripts' summary\n",
    "    og=[]\n",
    "    full_dioc={}    # master dictionary Contains all the keywords with their occurance over all the summaries (for each transcript) \n",
    "    dioc=[]         # list of individual dictionaries with occurance score for every keyword in ONE summary. \n",
    "    for key in automin_summary[key1].keys():\n",
    "        text_data = []\n",
    "        a=[]\n",
    "        for line in nltk.sent_tokenize(automin_summary[key1][key]):\n",
    "            tokens = prepare_text_for_lda(line)\n",
    "            # print(tokens)\n",
    "            if len(tokens)>=1:\n",
    "                #print(tokens)\n",
    "                text_data.append(tokens)\n",
    "        dictionary = corpora.Dictionary(text_data)\n",
    "        corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "        #pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "        #dictionary.save('dictionary.gensim')\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
    "        topics = ldamodel.print_topics(num_words=10)\n",
    "\n",
    "        # For each summary.\n",
    "        topic_list=[]  # keywords\n",
    "        topc_dic={}    # keywords with occurance score for every summary. \n",
    "        for topic in topics:\n",
    "            #print(topic[1].split('+'))\n",
    "            for i in topic[1].split('+'):\n",
    "                a.append(i[7:-1])\n",
    "                name=i[7:-1]\n",
    "                if name in dioc:\n",
    "                    full_dioc[name]+=1\n",
    "                else:\n",
    "                    full_dioc[name]=1\n",
    "                if name in topc_dic:\n",
    "                    topc_dic[name]+=1\n",
    "                else:\n",
    "                    topc_dic[name]=1\n",
    "\n",
    "            \n",
    "            topic_list.append(a)\n",
    "        dioc.append(topc_dic)\n",
    "        og.append(topic_list)\n",
    "        \n",
    "        \n",
    "    vals=[]\n",
    "    for i in range(len(dioc)):\n",
    "        #print(i)\n",
    "        x_obs=list(dioc[i].values())\n",
    "        #x_obs=[1]*len(dioc[i])\n",
    "        y_exp=[]\n",
    "        for key in dioc[i]:\n",
    "            if key in full_dioc:\n",
    "                y_exp.append(full_dioc[key])\n",
    "            else:\n",
    "                y_exp.append(0)\n",
    "        vals.append(chisquare(x_obs,y_exp)[1])\n",
    "    chisq.append(statistics.median(vals))\n",
    "    if len(vals)>1:\n",
    "        standard_deviations.append(statistics.stdev(vals))\n",
    "    else:\n",
    "        standard_deviations.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cf22e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd2436f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umang\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:3508: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    }
   ],
   "source": [
    "# correlation for inter annoator agreement, test  ---------------------------------------------------------------------\n",
    "\n",
    "chisq=[]\n",
    "standard_deviations=[]\n",
    "for key1 in automin_summary.keys():\n",
    "    scores=0\n",
    "    # For each transcripts' summary\n",
    "    og=[]\n",
    "    full_dioc={}    # master dictionary Contains all the keywords with their occurance over all the summaries (for each transcript) \n",
    "    dioc=[]         # list of individual dictionaries with occurance score for every keyword in ONE summary. \n",
    "    for key in automin_summary[key1].keys():\n",
    "        text_data = []\n",
    "        a=[]\n",
    "        for line in nltk.sent_tokenize(automin_summary[key1][key]):\n",
    "            tokens = prepare_text_for_lda(line)\n",
    "            # print(tokens)\n",
    "            if len(tokens)>=1:\n",
    "                #print(tokens)\n",
    "                text_data.append(tokens)\n",
    "        dictionary = corpora.Dictionary(text_data)\n",
    "        corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "        #pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "        #dictionary.save('dictionary.gensim')\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
    "        topics = ldamodel.print_topics(num_words=10)\n",
    "\n",
    "        # For each summary.\n",
    "        topic_list=[]  # keywords\n",
    "        topc_dic={}    # keywords with occurance score for every summary. \n",
    "        for topic in topics:\n",
    "            #print(topic[1].split('+'))\n",
    "            for i in topic[1].split('+'):\n",
    "                a.append(i[7:-1])\n",
    "                name=i[7:-1]\n",
    "                if name in full_dioc:\n",
    "                    full_dioc[name]+=1\n",
    "                else:\n",
    "                    full_dioc[name]=1\n",
    "                if name in topc_dic:\n",
    "                    topc_dic[name]+=1\n",
    "                else:\n",
    "                    topc_dic[name]=1\n",
    "\n",
    "            \n",
    "            topic_list.append(a)\n",
    "        dioc.append(topc_dic)\n",
    "        og.append(topic_list)\n",
    "        \n",
    "        \n",
    "    vals=[]\n",
    "    for i in range(len(dioc)):\n",
    "        #print(i)\n",
    "        x_obs=list(dioc[i].values())\n",
    "        #x_obs=[1]*len(dioc[i])\n",
    "        y_exp=[]\n",
    "        for key in dioc[i]:\n",
    "            if key in full_dioc:\n",
    "                y_exp.append(full_dioc[key])\n",
    "            else:\n",
    "                y_exp.append(0)\n",
    "        vals.append(pearsonr(x_obs,y_exp)[0])\n",
    "    chisq.append(statistics.median(vals))\n",
    "    if len(vals)>1:\n",
    "        standard_deviations.append(statistics.stdev(vals))\n",
    "    else:\n",
    "        standard_deviations.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25025306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7811294912904847"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.median(chisq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca6daf2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x15dedb44eb0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.kdeplot(chisq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0891f9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person1\"': 1, '\"meeting\"': 1, '\"person9\"': 1, '\"attendee\"': 1, '\"person2\"': 1, '\"preparation\"': 1, '\"purpose\"': 1, '\"2020/05/04\"': 1, '\"location1\"': 1, '\"session': 1, 'already\"': 1, '\"prepare\"': 1, '\"consecutevely\"': 1, '\"czech\"': 1, '\"translate\"': 1, '\"video\"': 1, '\"youtube\"': 1, '\"person1\"': 1, '\"organization2': 1, 'check\"': 1, '\"platform\"': 1, '\"person5\"': 1, '\"light\"': 1, '\"conferencing\"': 1, '\"submit\"': 1, '\"minutes\"': 1, '\"annotator2\"': 1, '\"location1': 1, '\"domain\"': 1, '\"organization2\"': 1, '\"summary\"': 1, '\"person\"': 1, '\"worry\"': 1, '\"present\"': 1, '\"romanian\"': 1, '\"location': 1, 'session\"': 1, '\"input\"': 1, '\"person8\"': 1, '\"german\"': 1, '\"doodle': 1, 'person8\"': 1, '\"organization5\"': 1, '\"choose\"': 1, '\"chosen\"': 1, '\"need\"': 1, '\"session\"': 1, 'input\"': 1, '\"english\"': 1, '\"arrange\"': 1, '\"doodle\"': 1, '\"person9': 1, '\"check': 1, '\"discus\"': 1, '\"german': 1, 'location1\"': 1, 'project5\"': 1, '\"project1\"': 1, '\"subtitle\"': 1, '\"submit': 1, 'choose\"': 1, '\"cooperation\"': 1, '\"language\"': 1, '\"audio\"': 1, '\"preparation': 1, 'language\"': 1, '\"agenda\"': 1, '\"youtube': 1, '\"subtitle': 1, 'preparation\"': 1, '\"04/05/2020\"': 1, '\"-plans\"': 1, '\"-workflow\"': 1, '\"test\"': 1, '\"-purpose\"': 1, '\"language': 1, '-workflow\"': 1, '\"running\"': 1, '\"everybody': 1, 'minutes\"': 1, '\"annotator1\"': 1, '\"romanian': 1, 'chosen\"': 1, '\"suitable\"': 1, '\"available\"': 1, '\"search\"': 1, '\"organization10\"': 1, '\"channel\"': 1, '\"probably\"': 1, '\"organization1\"': 1, '\"problem\"': 1, '\"connection': 1, 'english\"': 1, '\"translation\"': 1, '\"machine\"': 1, '\"meeting': 1, 'romanian\"': 1, '\"target\"': 1, '\"standard\"': 1, '\"slack\"': 1, '\"-plans': 1, '\"speaker\"': 1, '\"running': 1, 'someone\"': 1, 'video\"': 1, '\"check\"': 1, '\"enough\"': 1, '\"audio': 1, '\"everybody\"': 1, '\"agree\"': 1, 'discuss\"': 1, '\"email\"': 1, '\"speaker': 1, 'project3\"': 1, '\"discuss\"': 1, '\"people\"': 1, '\"person11\"': 1, '\"person1': 1, '\"prapairing\"': 1, '\"project5\"': 1, '\"people': 1, '\"email': 1, '\"person8': 1, 'participant\"': 1, '\"require\"': 1, '\"sessions\"': 1, '\"arrangement\"': 1, '\"review\"': 1, '\"former\"': 1, '\"correction\"': 1, '\"shortcoming': 1, 'presentation\"': 1, '\"select\"': 1, '\"highly\"': 1, '\"likely\"': 1, '\"organization4\"': 1, '\"congress': 1, 'organization2\"': 1, '\"avoid\"': 1, '\"performance\"': 1, '\"colleague\"': 1, '\"participate\"': 1, '\"competition\"': 1, '\"direct\"': 1, '\"result\"': 1, '\"significant': 1, 'meeting\"': 1, '\"presentation\"': 1, '\"technical\"': 1, '\"decide\"': 1, '\"formation\"': 1, '\"parameter\"': 1, '\"structure\"': 1, '\"topic': 1, '\"source\"': 1, '\"and/or\"': 1, '\"option\"': 1, '\"entire\"': 1, '\"arrange': 1, '\"complete\"': 1, '\"device\"': 1, '\"pipeline\"': 1, '\"capacity': 1, 'subject\"': 1, '\"perform\"': 1, '\"remote\"': 1, '\"channels\"': 1, '\"restriction\"': 1, '\"impose\"': 1, '\"easing\"': 1, '\"government\"': 1, '\"presentation': 1, 'strength\"': 1, '\"ability\"': 1, '\"support\"': 1, '\"action\"': 1, '\"machine': 1, '\"confirm\"': 1, '\"still\"': 1, '\"select': 1, 'german\"': 1, '\"site\"': 1, '\"respective\"': 1, '\"austrian\"': 1, '\"input': 1, 'interest\"': 1, '\"using\"': 1, '\"metrics\"': 1, '\"person10\"': 1, '\"waiting': 1, 'person6\"': 1, '\"person14\"': 1, '\"try\"': 1, '\"stop\"': 1, '\"leaflet\"': 1, '\"organizational\"': 1, '\"coming\"': 1, '\"desirable': 1, '\"person6\"': 1, '\"translation': 1, 'metrics\"': 1, '\"special\"': 1, '\"kind\"': 1, '\"first\"': 1, '\"different\"': 1, '\"useful\"': 1, '\"semantic': 1, 'discus\"': 1, '\"interest\"': 1, '\"stop': 1, '\"situation': 1, '\"waiting\"': 1, '\"person4\"': 1, '\"project6\"': 1, '\"---at': 1, '\"server\"': 1, '\"situation\"': 1, '\"least\"': 1, 'result\"': 1, '\"try': 1, '\"double\"': 1, '\"inform\"': 1, '\"suggest\"': 1, '\"alexandr': 1, 'person5\"': 1, '\"another\"': 1, '\"helping\"': 1, '\"plenary\"': 1, '\"person12\"': 1, '\"weekend\"': 1, '\"remote': 1, '\"connect\"': 1, '\"remotely': 1, '\"belief\"': 1, '\"comparing\"': 1, '\"better\"': 1, '\"match\"': 1, '\"google': 1, '\"lower\"': 1, '\"start\"': 1, '\"organization5': 1, 'person9\"': 1, '\"document\"': 1, '\"planning\"': 1, '\"pool\"': 1, '\"share\"': 1, '\"person3\"': 1, '\"congress\"': 1, '\"cause\"': 1, '\"something\"': 1, '\"domain': 1, '\"testing\"': 1, '\"already': 1, 'organization5\"': 1, '\"talks\"': 1, '\"resource\"': 1, 'doodle\"': 1, '\"internal\"': 1, '\"schedule\"': 1, '\"starting\"': 1, '\"propose\"': 1, '\"person7': 1, 'generate\"': 1, '\"right\"': 1, '\"offline\"': 1, '\"might\"': 1, '\"automatic\"': 1, '\"excellent': 1, 'audio\"': 1, '\"define\"': 1, '\"office': 1, '\"organization8\"': 1, '\"organization7\"': 1, '\"ready\"': 1, '\"directly': 1, 'translation\"': 1, '\"pivot\"': 1, '\"understandability': 1, 'conferencing\"': 1, '\"party\"': 1, '\"need': 1, 'scheduling\"': 1, '\"involve\"': 1, '\"backup\"': 1, '\"internally': 1, 'probably\"': 1, '\"organization7': 1, '\"google\"': 1, '\"possible\"': 1, '\"already\"': 1, '\"concern\"': 1, '\"challenge\"': 1, '\"think\"': 1}\n"
     ]
    }
   ],
   "source": [
    "print(full_dioc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2cb00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(standard_deviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd974fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics.median(chisq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9159bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics.mean(chisq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a7219e",
   "metadata": {},
   "source": [
    "# Standard Deviation between length of vocabularies of two summaries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362846a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = list(set(stopwords.words(\"english\")))\n",
    "standard_devs=[]\n",
    "for key1 in automin_summary.keys():\n",
    "    lens=[]\n",
    "    for key in automin_summary[key1].keys():\n",
    "        text_data = []\n",
    "        a=[]\n",
    "        vocab={}\n",
    "        for line in nltk.sent_tokenize(automin_summary[key1][key]):\n",
    "            words = nltk.word_tokenize(line)\n",
    "            words=[word.lower() for word in words]\n",
    "            refined_words=[word for word in words if word not in stopWords]\n",
    "            for word in refined_words:\n",
    "                if word in vocab:\n",
    "                    vocab[word]+=1\n",
    "                else:\n",
    "                    vocab[word]=1\n",
    "            \n",
    "        lens.append(len(vocab.keys()))\n",
    "    if len(lens)==1:\n",
    "        standard_devs.append(0)\n",
    "    else:\n",
    "        standard_devs.append(statistics.stdev(lens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f958d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics.mean(standard_devs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da4d91",
   "metadata": {},
   "source": [
    "# Similarity between topics in Transcript and summaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b980dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jaccard_sim(l1, l2): \n",
    "    a = set(l1) \n",
    "    b = set(l2)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee61265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ami \n",
    "similarity=[]\n",
    "for key in extractive_summary.keys():\n",
    "    clear_output(wait=True)\n",
    "    print(key,\".......................\", end=\" \")\n",
    "    text_data = []\n",
    "    for line in nltk.sent_tokenize(transcripts[key]):\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "\n",
    "        if len(tokens)>=1:\n",
    "            text_data.append(tokens)\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
    "    topics = ldamodel.print_topics(num_words=10)\n",
    "    \n",
    "    \n",
    "    t_topic=[]\n",
    "    for topic in topics:\n",
    "        temp=[]\n",
    "        for index in range(1,len(topic[1].split('\"')),2):\n",
    "            temp.append(topic[1].split('\"')[index])\n",
    "        t_topic.append(temp)\n",
    "    \n",
    "    text_data=[]\n",
    "    for line in nltk.sent_tokenize(extractive_summary[key]):\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if len(tokens)>=1:\n",
    "            text_data.append(tokens)\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
    "    topics = ldamodel.print_topics(num_words=10)\n",
    "    \n",
    "    \n",
    "    s_topic=[]\n",
    "    for topic in topics:\n",
    "        temp=[]\n",
    "        for index in range(1,len(topic[1].split('\"')),2):\n",
    "            temp.append(topic[1].split('\"')[index])\n",
    "        s_topic.append(temp)\n",
    "    sim=[]\n",
    "    for index_t in range(len(t_topic)):\n",
    "        max_sim=0\n",
    "        for index_s in range(len(s_topic)):\n",
    "            max_sim=max(max_sim,get_jaccard_sim(t_topic[index_t], s_topic[index_s]))\n",
    "        sim.append(max_sim)\n",
    "    similarity.append(statistics.mean(sim))\n",
    "    print(' : done')\n",
    "print('Jaccard similarity between transcript and summary topics: ', statistics.mean(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d497cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#icsi\n",
    "similarity=[]\n",
    "for index in range(len(text_list)):\n",
    "    clear_output(wait=True)\n",
    "    print((index/len(text_list))*100,\".......................\", end=\" \")\n",
    "    text_data = []\n",
    "    for line in nltk.sent_tokenize(''.join(text_list[index])):\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if len(tokens)>=1:\n",
    "            text_data.append(tokens)\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
    "    topics = ldamodel.print_topics(num_words=10)\n",
    "    t_topic=[]\n",
    "    for topic in topics:\n",
    "        temp=[]\n",
    "        for index in range(1,len(topic[1].split('\"')),2):\n",
    "            temp.append(topic[1].split('\"')[index])\n",
    "        t_topic.append(temp)\n",
    "    \n",
    "    \n",
    "    text_data=[]\n",
    "    for line in nltk.sent_tokenize(''.join(ab_sumtext_list[index])):\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if len(tokens)>=1:\n",
    "            text_data.append(tokens)\n",
    "    \n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
    "    topics = ldamodel.print_topics(num_words=10)\n",
    "    s_topic=[]\n",
    "    for topic in topics:\n",
    "        temp=[]\n",
    "        for index in range(1,len(topic[1].split('\"')),2):\n",
    "            temp.append(topic[1].split('\"')[index])\n",
    "        s_topic.append(temp)\n",
    "    print(len(t_topic), len(s_topic))\n",
    "    sim=[]\n",
    "    for index_t in range(len(t_topic)):\n",
    "        max_sim=0\n",
    "        #sim.append(get_jaccard_sim(t_topic[index_t], s_topic[index_t]))\n",
    "        for index_s in range(len(s_topic)):\n",
    "            max_sim=max(max_sim,get_jaccard_sim(t_topic[index_t], s_topic[index_s]))\n",
    "        sim.append(max_sim)\n",
    "    similarity.append(statistics.mean(sim))\n",
    "    print(' : done')\n",
    "print('Jaccard similarity between transcript and summary topics: ', statistics.mean(similarity))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d8e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#automin\n",
    "\n",
    "similarity=[]\n",
    "for key in automin_transcripts.keys():\n",
    "    clear_output(wait=True)\n",
    "    print(key,\".......................\", end=\" \")\n",
    "    text_data = []\n",
    "    for line in nltk.sent_tokenize(automin_transcripts[key]):\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if len(tokens)>=1:\n",
    "            text_data.append(tokens)\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
    "    topics = ldamodel.print_topics(num_words=10)\n",
    "    t_topic=[]\n",
    "    for topic in topics:\n",
    "        temp=[]\n",
    "        for index in range(1,len(topic[1].split('\"')),2):\n",
    "            temp.append(topic[1].split('\"')[index])\n",
    "        t_topic.append(temp)\n",
    "    \n",
    "    text_data=[]\n",
    "    for key1 in automin_summary[key]:\n",
    "        for line in nltk.sent_tokenize(automin_summary[key][key1]):\n",
    "            tokens = prepare_text_for_lda(line)\n",
    "            if len(tokens)>=1:\n",
    "                text_data.append(tokens)\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
    "    topics = ldamodel.print_topics(num_words=10)\n",
    "    \n",
    "    s_topic=[]\n",
    "    for topic in topics:\n",
    "        temp=[]\n",
    "        for index in range(1,len(topic[1].split('\"')),2):\n",
    "            temp.append(topic[1].split('\"')[index])\n",
    "        s_topic.append(temp)\n",
    "        sim=[]\n",
    "    for index_t in range(len(t_topic)):\n",
    "        max_sim=0\n",
    "        sim.append(get_jaccard_sim(t_topic[index_t], s_topic[index_t]))\n",
    "#         for index_s in range(len(s_topic)):\n",
    "#             max_sim=max(max_sim,get_jaccard_sim(t_topic[index_t], s_topic[index_s]))\n",
    "#         sim.append(max_sim)\n",
    "    similarity.append(statistics.mean(sim))\n",
    "    print(' : done')\n",
    "print('Jaccard similarity between transcript and summary topics: ', statistics.mean(similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec818c",
   "metadata": {},
   "source": [
    "# Position bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ami\n",
    "\n",
    "final_scores=[0]*100\n",
    "for key in extractive_summary.keys():\n",
    "    unique={}\n",
    "    # creating vocab\n",
    "    txt=nltk.sent_tokenize(extractive_summary[key])\n",
    "    for index,sentence in enumerate(txt):\n",
    "        temp=nltk.word_tokenize(sentence)\n",
    "        temp=[word.lower() for word in temp]\n",
    "        word_tokens_refined=[x for x in temp if x not in stopWords]\n",
    "        for word in word_tokens_refined:\n",
    "            if word not in unique: \n",
    "                unique[word]=1\n",
    "            elif word in unique:\n",
    "                unique[word]+=1\n",
    "    sentence_scores=[]\n",
    "    txt=nltk.sent_tokenize(transcripts[key])\n",
    "    \n",
    "    # scoring each sentence in transcript\n",
    "    for index,sentence in enumerate(txt):\n",
    "        score=0\n",
    "        temp=nltk.word_tokenize(sentence)\n",
    "        temp=[word.lower() for word in temp]\n",
    "        word_tokens_refined=[x for x in temp if x not in stopWords]\n",
    "        for word in word_tokens_refined:\n",
    "            if word in unique: \n",
    "                score+=1\n",
    "        sentence_scores.append(score)\n",
    "    # creating 100 bins\n",
    "    for index,i in enumerate(range(0,len(sentence_scores),(len(sentence_scores)//100)+1)):\n",
    "        try:\n",
    "            final_scores[index]+=sum(sentence_scores[i:i+len(sentence_scores)//100])\n",
    "        except:\n",
    "            final_scores[index]+=sum(sentence_scores[i:])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869bb0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=list(range(100)),y=final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2c2ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# icsi\n",
    "\n",
    "final_scores=[0]*100\n",
    "for key in range(len(text_list)):\n",
    "    unique={}\n",
    "    # creating vocab\n",
    "    txt=nltk.sent_tokenize(''.join(ab_sumtext_list[key]))\n",
    "    for index,sentence in enumerate(txt):\n",
    "        temp=nltk.word_tokenize(sentence)\n",
    "        temp=[word.lower() for word in temp]\n",
    "        word_tokens_refined=[x for x in temp if x not in stopWords]\n",
    "        for word in word_tokens_refined:\n",
    "            if word not in unique: \n",
    "                unique[word]=1\n",
    "            elif word in unique:\n",
    "                unique[word]+=1\n",
    "    \n",
    "    sentence_scores=[]\n",
    "    txt=nltk.sent_tokenize(''.join(text_list[key]))\n",
    "    \n",
    "    # scoring each sentence in transcript\n",
    "    for index,sentence in enumerate(txt):\n",
    "        score=0\n",
    "        temp=nltk.word_tokenize(sentence)\n",
    "        temp=[word.lower() for word in temp]\n",
    "        word_tokens_refined=[x for x in temp if x not in stopWords]\n",
    "        for word in word_tokens_refined:\n",
    "            if word in unique: \n",
    "                score+=1\n",
    "        sentence_scores.append(score)\n",
    "    # creating 100 bins\n",
    "    for index,i in enumerate(range(0,len(sentence_scores),(len(sentence_scores)//100)+1)):\n",
    "        try:\n",
    "            final_scores[index]+=sum(sentence_scores[i:i+len(sentence_scores)//100])\n",
    "        except:\n",
    "            final_scores[index]+=sum(sentence_scores[i:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edec634",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=list(range(100)),y=final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cca446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automin\n",
    "\n",
    "\n",
    "final_scores=[0]*100\n",
    "for key in automin_summary.keys():\n",
    "    unique={}\n",
    "    # creating vocab\n",
    "    for key1 in automin_summary[key].keys():\n",
    "        txt=nltk.sent_tokenize(automin_summary[key][key1])\n",
    "        for index,sentence in enumerate(txt):\n",
    "            temp=nltk.word_tokenize(sentence)\n",
    "            temp=[word.lower() for word in temp]\n",
    "            word_tokens_refined=[x for x in temp if x not in stopWords]\n",
    "            for word in word_tokens_refined:\n",
    "                if word not in unique: \n",
    "                    unique[word]=1\n",
    "                elif word in unique:\n",
    "                    unique[word]+=1\n",
    "    \n",
    "    sentence_scores=[]\n",
    "    try:\n",
    "        txt=nltk.sent_tokenize(automin_transcripts[key])\n",
    "        # scoring each sentence in transcript\n",
    "        for index,sentence in enumerate(txt):\n",
    "            score=0\n",
    "            temp=nltk.word_tokenize(sentence)\n",
    "            temp=[word.lower() for word in temp]\n",
    "            word_tokens_refined=[x for x in temp if x not in stopWords]\n",
    "            for word in word_tokens_refined:\n",
    "                if word in unique: \n",
    "                    score+=1\n",
    "            sentence_scores.append(score)\n",
    "    except:\n",
    "        pass\n",
    "    # creating 100 bins\n",
    "    for index,i in enumerate(range(0,len(sentence_scores),(len(sentence_scores)//100)+1)):\n",
    "        try:\n",
    "            final_scores[index]+=sum(sentence_scores[i:i+len(sentence_scores)//100])\n",
    "        except:\n",
    "            final_scores[index]+=sum(sentence_scores[i:])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd13fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=list(range(100)),y=final_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870eb4d",
   "metadata": {},
   "source": [
    "# Occurance of important sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa70f49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ami \n",
    "stopWords = list(set(stopwords.words(\"english\")))\n",
    "important=[]\n",
    "for key in extractive_summary.keys():\n",
    "    text_data = []\n",
    "    for line in nltk.sent_tokenize(transcripts[key]):\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        # print(tokens)\n",
    "        if len(tokens)>=1:\n",
    "            #print(tokens)\n",
    "            text_data.append(tokens)\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
    "    topics = ldamodel.print_topics(num_words=10)\n",
    "    \n",
    "    \n",
    "    t_topic=[]\n",
    "    for topic in topics:\n",
    "        for index in range(1,len(topic[1].split('\"')),2):\n",
    "            t_topic.append(topic[1].split('\"')[index])\n",
    "    \n",
    "    t_topic=[word for word in t_topic if word not in stopWords]\n",
    "    scores=[]\n",
    "    for index,line in enumerate(nltk.sent_tokenize(transcripts[key])):\n",
    "        temp=nltk.word_tokenize(line)\n",
    "        temp=[word.lower() for word in temp]\n",
    "        word_tokens_refined=[x for x in temp if x not in stopWords]\n",
    "        score=0\n",
    "        for word in word_tokens_refined:\n",
    "            if word in t_topic:\n",
    "                score+=1\n",
    "        scores.append(score)\n",
    "    max_score=max(scores)\n",
    "    scores=[i/max_score for i in scores]\n",
    "    scores=[i for i in scores if i>0.7]\n",
    "    \n",
    "    important.append(len(scores))\n",
    "statistics.mean(important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47047cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# icsi \n",
    "\n",
    "important=[]\n",
    "for key in range(len(text_list)):\n",
    "    text_data = []\n",
    "    for line in nltk.sent_tokenize(''.join(text_list[key])):\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        # print(tokens)\n",
    "        if len(tokens)>=1:\n",
    "            #print(tokens)\n",
    "            text_data.append(tokens)\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    #pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "    #dictionary.save('dictionary.gensim')\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
    "    topics = ldamodel.print_topics(num_words=10)\n",
    "    \n",
    "    \n",
    "    t_topic=[]\n",
    "    for topic in topics:\n",
    "        for index in range(1,len(topic[1].split('\"')),2):\n",
    "            t_topic.append(topic[1].split('\"')[index])\n",
    "    \n",
    "    t_topic=[word for word in t_topic if word not in stopWords]\n",
    "    scores=[]\n",
    "    for index,line in enumerate(nltk.sent_tokenize(''.join(text_list[key]))):\n",
    "        temp=nltk.word_tokenize(line)\n",
    "        temp=[word.lower() for word in temp]\n",
    "        word_tokens_refined=[x for x in temp if x not in stopWords]\n",
    "        score=0\n",
    "        for word in word_tokens_refined:\n",
    "            if word in t_topic:\n",
    "                score+=1\n",
    "        scores.append(score)\n",
    "    max_score=max(scores)\n",
    "    scores=[i/max_score for i in scores if i/max_score>0.7]\n",
    "    \n",
    "    important.append(len(scores))\n",
    "statistics.mean(important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967625d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#automin\n",
    "\n",
    "important=[]\n",
    "for key in automin_transcripts.keys():\n",
    "    text_data = []\n",
    "    for line in nltk.sent_tokenize(automin_transcripts[key]):\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        # print(tokens)\n",
    "        if len(tokens)>=1:\n",
    "            #print(tokens)\n",
    "            text_data.append(tokens)\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    #pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "    #dictionary.save('dictionary.gensim')\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=10, id2word=dictionary, passes=15)\n",
    "    topics = ldamodel.print_topics(num_words=10)\n",
    "    \n",
    "    \n",
    "    t_topic=[]\n",
    "    for topic in topics:\n",
    "        for index in range(1,len(topic[1].split('\"')),2):\n",
    "            t_topic.append(topic[1].split('\"')[index])\n",
    "    \n",
    "    t_topic=[word for word in t_topic if word not in stopWords]\n",
    "    scores=[]\n",
    "    for index,line in enumerate(nltk.sent_tokenize(automin_transcripts[key])):\n",
    "        temp=nltk.word_tokenize(line)\n",
    "        temp=[word.lower() for word in temp]\n",
    "        word_tokens_refined=[x for x in temp if x not in stopWords]\n",
    "        score=0\n",
    "        for word in word_tokens_refined:\n",
    "            if word in t_topic:\n",
    "                score+=1\n",
    "        scores.append(score)\n",
    "    max_score=max(scores)\n",
    "    scores=[i/max_score for i in scores if i/max_score>0.7]\n",
    "    \n",
    "    important.append(len(scores))\n",
    "statistics.mean(important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9563acd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
